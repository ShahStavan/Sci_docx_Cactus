{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber\n",
        "!pip install pylatexenc\n",
        "!pip install deepdoctection\n",
        "!pip install pdf2image\n",
        "!pip install pytesseract\n",
        "!apt-get install poppler-utils\n",
        "!apt-get install tesseract-ocr\n",
        "!pip install pymupdf\n",
        "!pip install hotpdf\n",
        "!pip install dehyphen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMREo96g2YZC",
        "outputId": "b51283f9-7ad1-4696-dbe2-c3d95faa4815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.10.4)\n",
            "Collecting pdfminer.six==20221105 (from pdfplumber)\n",
            "  Using cached pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (10.2.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.27.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (42.0.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (2.21)\n",
            "Installing collected packages: pdfminer.six\n",
            "  Attempting uninstall: pdfminer.six\n",
            "    Found existing installation: pdfminer.six 20231228\n",
            "    Uninstalling pdfminer.six-20231228:\n",
            "      Successfully uninstalled pdfminer.six-20231228\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "hotpdf 0.5.2 requires pdfminer.six>=20231228, but you have pdfminer-six 20221105 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pdfminer.six-20221105\n",
            "Requirement already satisfied: pylatexenc in /usr/local/lib/python3.10/dist-packages (2.10)\n",
            "Requirement already satisfied: deepdoctection in /usr/local/lib/python3.10/dist-packages (0.30)\n",
            "Requirement already satisfied: catalogue==2.0.7 in /usr/local/lib/python3.10/dist-packages (from deepdoctection) (2.0.7)\n",
            "Requirement already satisfied: huggingface-hub>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from deepdoctection) (0.20.3)\n",
            "Requirement already satisfied: importlib-metadata>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from deepdoctection) (7.0.1)\n",
            "Requirement already satisfied: jsonlines==3.1.0 in /usr/local/lib/python3.10/dist-packages (from deepdoctection) (3.1.0)\n",
            "Requirement already satisfied: mock==4.0.3 in /usr/local/lib/python3.10/dist-packages (from deepdoctection) (4.0.3)\n",
            "Requirement already satisfied: networkx>=2.7.1 in /usr/local/lib/python3.10/dist-packages (from deepdoctection) (3.2.1)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from deepdoctection) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepdoctection) (23.2)\n",
            "Requirement already satisfied: Pillow>=10.0.0 in /usr/local/lib/python3.10/dist-packages (from deepdoctection) (10.2.0)\n",
            "Requirement already satisfied: pypdf>=3.16.0 in /usr/local/lib/python3.10/dist-packages (from deepdoctection) (4.0.2)\n",
            "Requirement already satisfied: pyyaml==6.0 in /usr/local/lib/python3.10/dist-packages (from deepdoctection) (6.0)\n",
            "Requirement already satisfied: pyzmq>=16 in /usr/local/lib/python3.10/dist-packages (from deepdoctection) (23.2.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from deepdoctection) (2.4.0)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from deepdoctection) (0.9.0)\n",
            "Requirement already satisfied: tqdm==4.64.0 in /usr/local/lib/python3.10/dist-packages (from deepdoctection) (4.64.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines==3.1.0->deepdoctection) (23.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.12.0->deepdoctection) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.12.0->deepdoctection) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.12.0->deepdoctection) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.12.0->deepdoctection) (4.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.11.2->deepdoctection) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.12.0->deepdoctection) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.12.0->deepdoctection) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.12.0->deepdoctection) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.12.0->deepdoctection) (2024.2.2)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (1.17.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (10.2.0)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (10.2.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.10/dist-packages (1.23.25)\n",
            "Requirement already satisfied: PyMuPDFb==1.23.22 in /usr/local/lib/python3.10/dist-packages (from pymupdf) (1.23.22)\n",
            "Requirement already satisfied: hotpdf in /usr/local/lib/python3.10/dist-packages (0.5.2)\n",
            "Collecting pdfminer.six>=20231228 (from hotpdf)\n",
            "  Using cached pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six>=20231228->hotpdf) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six>=20231228->hotpdf) (42.0.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six>=20231228->hotpdf) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six>=20231228->hotpdf) (2.21)\n",
            "Installing collected packages: pdfminer.six\n",
            "  Attempting uninstall: pdfminer.six\n",
            "    Found existing installation: pdfminer.six 20221105\n",
            "    Uninstalling pdfminer.six-20221105:\n",
            "      Successfully uninstalled pdfminer.six-20221105\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pdfplumber 0.10.4 requires pdfminer.six==20221105, but you have pdfminer-six 20231228 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pdfminer.six-20231228\n",
            "Collecting dehyphen\n",
            "  Downloading dehyphen-0.3.4-py3-none-any.whl (18 kB)\n",
            "Collecting clean-text[gpl] (from dehyphen)\n",
            "  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
            "Collecting pd3f-flair (from dehyphen)\n",
            "  Downloading pd3f_flair-0.6.0.1-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting emoji<2.0.0,>=1.0.0 (from clean-text[gpl]->dehyphen)\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy<7.0,>=6.0 (from clean-text[gpl]->dehyphen)\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unidecode<2.0.0,>=1.1.1 (from clean-text[gpl]->dehyphen)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deprecated (from pd3f-flair->dehyphen)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pd3f-flair->dehyphen) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pd3f-flair->dehyphen) (2.31.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from pd3f-flair->dehyphen) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pd3f-flair->dehyphen) (4.64.0)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy<7.0,>=6.0->clean-text[gpl]->dehyphen) (0.2.13)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->pd3f-flair->dehyphen) (1.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pd3f-flair->dehyphen) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pd3f-flair->dehyphen) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pd3f-flair->dehyphen) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pd3f-flair->dehyphen) (2024.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->pd3f-flair->dehyphen) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->pd3f-flair->dehyphen) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->pd3f-flair->dehyphen) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->pd3f-flair->dehyphen) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->pd3f-flair->dehyphen) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->pd3f-flair->dehyphen) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->pd3f-flair->dehyphen) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->pd3f-flair->dehyphen) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->pd3f-flair->dehyphen) (1.3.0)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171033 sha256=0009693bd5b84eb8aaa1a389d00e2e741cb92bce22d86e8c59aad71638656bf9\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji, unidecode, ftfy, deprecated, clean-text, pd3f-flair, dehyphen\n",
            "Successfully installed clean-text-0.6.0 dehyphen-0.3.4 deprecated-1.2.14 emoji-1.7.0 ftfy-6.1.3 pd3f-flair-0.6.0.1 unidecode-1.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytesserach and Pdf2image library"
      ],
      "metadata": {
        "id": "KjBEr6ycHCiu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qT4TrxBO13c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e397099-5b34-4380-d096-ba71498c0dab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1409.3215v3 [cs.CL] 14 Dec 2014\n",
            "\n",
            "1V\n",
            "\n",
            "arxX\n",
            "\n",
            " \n",
            "\n",
            "Sequence to Sequence Learning\n",
            "with Neural Networks\n",
            "\n",
            "Ilya Sutskever Oriol Vinyals Quoc V. Le\n",
            "Google Google Google\n",
            "ilyasu@google.com vinyals@google.com qvl@google.com\n",
            "Abstract\n",
            "\n",
            "Deep Neural Networks (DNNs) are powerful models that have achieved excel-\n",
            "lent performance on difficult learning tasks. Although DNNs work well whenever\n",
            "large labeled training sets are available, they cannot be used to map sequences to\n",
            "sequences. In this paper, we present a general end-to-end approach to sequence\n",
            "learning that makes minimal assumptions on the sequence structure. Our method\n",
            "uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence\n",
            "to a vector of a fixed dimensionality, and then another deep LSTM to decode the\n",
            "target sequence from the vector. Our main result is that on an English to French\n",
            "translation task from the WMT’ 14 dataset, the translations produced by the LSTM\n",
            "achieve a BLEU score of 34.8 on the entire test set, where the LSTM’s BLEU\n",
            "score was penalized on out-of-vocabulary words. Additionally, the LSTM did not\n",
            "have difficulty on long sentences. For comparison, a phrase-based SMT system\n",
            "achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM\n",
            "to rerank the 1000 hypotheses produced by the aforementioned SMT system, its\n",
            "BLEU score increases to 36.5, which is close to the previous best result on this\n",
            "task. The LSTM also learned sensible phrase and sentence representations that\n",
            "are sensitive to word order and are relatively invariant to the active and the pas-\n",
            "sive voice. Finally, we found that reversing the order of the words in all source\n",
            "sentences (but not target sentences) improved the LSTM’s performance markedly,\n",
            "because doing so introduced many short term dependencies between the source\n",
            "and the target sentence which made the optimization problem easier.\n",
            "\n",
            "1 Introduction\n",
            "\n",
            "Deep Neural Networks (DNNs) are extremely powerful machine learning models that achieve ex-\n",
            "cellent performance on difficult problems such as speech recognition [13, 7] and visual object recog-\n",
            "nition [19, 6, 21, 20]. DNNs are powerful because they can perform arbitrary parallel computation\n",
            "for a modest number of steps. A surprising example of the power of DNNs is their ability to sort\n",
            "N WN-bit numbers using only 2 hidden layers of quadratic size [27]. So, while neural networks are\n",
            "related to conventional statistical models, they learn an intricate computation. Furthermore, large\n",
            "DNNs can be trained with supervised backpropagation whenever the labeled training set has enough\n",
            "information to specify the network’s parameters. Thus, if there exists a parameter setting of a large\n",
            "DNN that achieves good results (for example, because humans can solve the task very rapidly),\n",
            "supervised backpropagation will find these parameters and solve the problem.\n",
            "\n",
            "Despite their flexibility and power, DNNs can only be applied to problems whose inputs and targets\n",
            "can be sensibly encoded with vectors of fixed dimensionality. It is a significant limitation, since\n",
            "many important problems are best expressed with sequences whose lengths are not known a-priori.\n",
            "For example, speech recognition and machine translation are sequential problems. Likewise, ques-\n",
            "tion answering can also be seen as mapping a sequence of words representing the question to a\n",
            "\fsequence of words representing the answer. It is therefore clear that a domain-independent method\n",
            "that learns to map sequences to sequences would be useful.\n",
            "\n",
            "Sequences pose a challenge for DNNs because they require that the dimensionality of the inputs and\n",
            "outputs is known and fixed. In this paper, we show that a straightforward application of the Long\n",
            "Short-Term Memory (LSTM) architecture [16] can solve general sequence to sequence problems.\n",
            "The idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large fixed-\n",
            "dimensional vector representation, and then to use another LSTM to extract the output sequence\n",
            "from that vector (fig. 1). The second LSTM is essentially a recurrent neural network language model\n",
            "[28, 23, 30] except that it is conditioned on the input sequence. The LSTM’s ability to successfully\n",
            "learn on data with long range temporal dependencies makes it a natural choice for this application\n",
            "due to the considerable time lag between the inputs and their corresponding outputs (fig. 1).\n",
            "\n",
            "There have been a number of related attempts to address the general sequence to sequence learning\n",
            "problem with neural networks. Our approach is closely related to Kalchbrenner and Blunsom [18]\n",
            "who were the first to map the entire input sentence to vector, and is related to Cho et al. [5] although\n",
            "the latter was used only for rescoring hypotheses produced by a phrase-based system. Graves [10]\n",
            "introduced a novel differentiable attention mechanism that allows neural networks to focus on dif-\n",
            "ferent parts of their input, and an elegant variant of this idea was successfully applied to machine\n",
            "translation by Bahdanau et al. [2]. The Connectionist Sequence Classification is another popular\n",
            "technique for mapping sequences to sequences with neural networks, but it assumes a monotonic\n",
            "alignment between the inputs and the outputs [11].\n",
            "\n",
            "W X Y Z <EOS>\n",
            "\n",
            "A B C <EOS> W X Y Z\n",
            "\n",
            "Figure 1: Our model reads an input sentence “ABC” and produces “WXYZ” as the output sentence. The\n",
            "model stops making predictions after outputting the end-of-sentence token. Note that the LSTM reads the\n",
            "input sentence in reverse, because doing so introduces many short term dependencies in the data that make the\n",
            "optimization problem much easier.\n",
            "\n",
            "The main result of this work is the following. On the WMT’ 14 English to French translation task,\n",
            "we obtained a BLEU score of 34.81 by directly extracting translations from an ensemble of 5 deep\n",
            "LSTMs (with 384M parameters and 8,000 dimensional state each) using a simple left-to-right beam-\n",
            "search decoder. This is by far the best result achieved by direct translation with large neural net-\n",
            "works. For comparison, the BLEU score of an SMT baseline on this dataset is 33.30 [29]. The 34.81\n",
            "BLEU score was achieved by an LSTM with a vocabulary of 80k words, so the score was penalized\n",
            "whenever the reference translation contained a word not covered by these 80k. This result shows\n",
            "that a relatively unoptimized small-vocabulary neural network architecture which has much room\n",
            "for improvement outperforms a phrase-based SMT system.\n",
            "\n",
            "Finally, we used the LSTM to rescore the publicly available 1000-best lists of the SMT baseline on\n",
            "the same task [29]. By doing so, we obtained a BLEU score of 36.5, which improves the baseline by\n",
            "3.2 BLEU points and is close to the previous best published result on this task (which is 37.0 [9]).\n",
            "\n",
            "Surprisingly, the LSTM did not suffer on very long sentences, despite the recent experience of other\n",
            "researchers with related architectures [26]. We were able to do well on long sentences because we\n",
            "reversed the order of words in the source sentence but not the target sentences in the training and test\n",
            "set. By doing so, we introduced many short term dependencies that made the optimization problem\n",
            "much simpler (see sec. 2 and 3.3). As a result, SGD could learn LSTMs that had no trouble with\n",
            "long sentences. The simple trick of reversing the words in the source sentence is one of the key\n",
            "technical contributions of this work.\n",
            "\n",
            "A useful property of the LSTM is that it learns to map an input sentence of variable length into\n",
            "a fixed-dimensional vector representation. Given that translations tend to be paraphrases of the\n",
            "source sentences, the translation objective encourages the LSTM to find sentence representations\n",
            "that capture their meaning, as sentences with similar meanings are close to each other while different\n",
            "\fsentences meanings will be far. A qualitative evaluation supports this claim, showing that our model\n",
            "is aware of word order and is fairly invariant to the active and passive voice.\n",
            "\n",
            "2 The model\n",
            "\n",
            "The Recurrent Neural Network (RNN) [31, 28] is a natural generalization of feedforward neural\n",
            "networks to sequences. Given a sequence of inputs (71,...,27), a standard RNN computes a\n",
            "sequence of outputs (y1,..., yr) by iterating the following equation:\n",
            "\n",
            "h, = sigm (Wa, + W\"\" he-1)\n",
            "Ye = Whe\n",
            "\n",
            "The RNN can easily map sequences to sequences whenever the alignment between the inputs the\n",
            "outputs is known ahead of time. However, it is not clear how to apply an RNN to problems whose\n",
            "input and the output sequences have different lengths with complicated and non-monotonic relation-\n",
            "ships.\n",
            "\n",
            "The simplest strategy for general sequence learning is to map the input sequence to a fixed-sized\n",
            "vector using one RNN, and then to map the vector to the target sequence with another RNN (this\n",
            "approach has also been taken by Cho et al. [5]). While it could work in principle since the RNN is\n",
            "provided with all the relevant information, it would be difficult to train the RNNs due to the resulting\n",
            "long term dependencies (figure 1) [14, 4, 16, 15]. However, the Long Short-Term Memory (LSTM)\n",
            "[16] is known to learn problems with long range temporal dependencies, so an LSTM may succeed\n",
            "in this setting.\n",
            "\n",
            "The goal of the LSTM is to estimate the conditional probability p(y1,...,y7|1,..., 7) where\n",
            "(v1,...,%7) is an input sequence and y},..., yr’ is its corresponding output sequence whose length\n",
            "T’ may differ from 7’. The LSTM computes this conditional probability by first obtaining the fixed-\n",
            "dimensional representation v of the input sequence (21,..., 27) given by the last hidden state of the\n",
            "LSTM, and then computing the probability of y1,...,y7- with a standard LSTM-LM formulation\n",
            "whose initial hidden state is set to the representation v of 71,..., 27:\n",
            "\n",
            "T’\n",
            "Py,---.yr/|ei,.-.,er) = |] viyelos yi... yea) (1)\n",
            "t=1\n",
            "\n",
            "In this equation, each p(yz|v, y1,---, Yz—1) distribution is represented with a softmax over all the\n",
            "words in the vocabulary. We use the LSTM formulation from Graves [10]. Note that we require that\n",
            "each sentence ends with a special end-of-sentence symbol “<EOS>”, which enables the model to\n",
            "define a distribution over sequences of all possible lengths. The overall scheme is outlined in figure\n",
            "1, where the shown LSTM computes the representation of “A”, “B”, “C”, “<EOS>” and then uses\n",
            "this representation to compute the probability of “W”, “X”, “Y”, “Z”, “<EOS>”.\n",
            "\n",
            "Our actual models differ from the above description in three important ways. First, we used two\n",
            "different LSTMs: one for the input sequence and another for the output sequence, because doing\n",
            "so increases the number model parameters at negligible computational cost and makes it natural to\n",
            "train the LSTM on multiple language pairs simultaneously [18]. Second, we found that deep LSTMs\n",
            "significantly outperformed shallow LSTMs, so we chose an LSTM with four layers. Third, we found\n",
            "it extremely valuable to reverse the order of the words of the input sentence. So for example, instead\n",
            "of mapping the sentence a, b,c to the sentence a, {,y, the LSTM is asked to map c, b, a to a, 6,7,\n",
            "where a, (3, ¥ is the translation of a, b, c. This way, a is in close proximity to a, b is fairly close to /,\n",
            "and so on, a fact that makes it easy for SGD to “establish communication” between the input and the\n",
            "output. We found this simple data transformation to greatly improve the performance of the LSTM.\n",
            "\n",
            "3 Experiments\n",
            "\n",
            "We applied our method to the WMT’14 English to French MT task in two ways. We used it to\n",
            "directly translate the input sentence without using a reference SMT system and we it to rescore the\n",
            "n-best lists of an SMT baseline. We report the accuracy of these translation methods, present sample\n",
            "translations, and visualize the resulting sentence representation.\n",
            "\f3.1 Dataset details\n",
            "\n",
            "We used the WMT’ 14 English to French dataset. We trained our models on a subset of 12M sen-\n",
            "tences consisting of 348M French words and 304M English words, which is a clean “selected”\n",
            "subset from [29]. We chose this translation task and this specific training set subset because of the\n",
            "public availability of a tokenized training and test set together with 1000-best lists from the baseline\n",
            "SMT [29].\n",
            "\n",
            "As typical neural language models rely on a vector representation for each word, we used a fixed\n",
            "vocabulary for both languages. We used 160,000 of the most frequent words for the source language\n",
            "and 80,000 of the most frequent words for the target language. Every out-of-vocabulary word was\n",
            "replaced with a special “UNK” token.\n",
            "\n",
            "3.2 Decoding and Rescoring\n",
            "\n",
            "The core of our experiments involved training a large deep LSTM on many sentence pairs. We\n",
            "trained it by maximizing the log probability of a correct translation T’ given the source sentence S,\n",
            "so the training objective is\n",
            "\n",
            "1/|S| S— logp(T|S)\n",
            "\n",
            "(T,S)ES\n",
            "\n",
            "where S is the training set. Once training is complete, we produce translations by finding the most\n",
            "likely translation according to the LSTM:\n",
            "\n",
            "A\n",
            "\n",
            "T = arg max p(T|S) (2)\n",
            "\n",
            "We search for the most likely translation using a simple left-to-right beam search decoder which\n",
            "maintains a small number B of partial hypotheses, where a partial hypothesis is a prefix of some\n",
            "translation. At each timestep we extend each partial hypothesis in the beam with every possible\n",
            "word in the vocabulary. This greatly increases the number of the hypotheses so we discard all but\n",
            "the B most likely hypotheses according to the model’s log probability. As soon as the “<EOS>”\n",
            "symbol is appended to a hypothesis, it is removed from the beam and is added to the set of complete\n",
            "hypotheses. While this decoder is approximate, it is simple to implement. Interestingly, our system\n",
            "performs well even with a beam size of 1, and a beam of size 2 provides most of the benefits of beam\n",
            "search (Table 1).\n",
            "\n",
            "We also used the LSTM to rescore the 1000-best lists produced by the baseline system [29]. To\n",
            "rescore an n-best list, we computed the log probability of every hypothesis with our LSTM and took\n",
            "an even average with their score and the LSTM’s score.\n",
            "\n",
            "3.3 Reversing the Source Sentences\n",
            "\n",
            "While the LSTM is capable of solving problems with long term dependencies, we discovered that\n",
            "the LSTM learns much better when the source sentences are reversed (the target sentences are not\n",
            "reversed). By doing so, the LSTM’s test perplexity dropped from 5.8 to 4.7, and the test BLEU\n",
            "scores of its decoded translations increased from 25.9 to 30.6.\n",
            "\n",
            "While we do not have a complete explanation to this phenomenon, we believe that it is caused by\n",
            "the introduction of many short term dependencies to the dataset. Normally, when we concatenate a\n",
            "source sentence with a target sentence, each word in the source sentence is far from its corresponding\n",
            "word in the target sentence. As a result, the problem has a large “minimal time lag” [17]. By\n",
            "reversing the words in the source sentence, the average distance between corresponding words in\n",
            "the source and target language is unchanged. However, the first few words in the source language\n",
            "are now very close to the first few words in the target language, so the problem’s minimal time lag is\n",
            "greatly reduced. Thus, backpropagation has an easier time “establishing communication” between\n",
            "the source sentence and the target sentence, which in turn results in substantially improved overall\n",
            "performance.\n",
            "\n",
            "Initially, we believed that reversing the input sentences would only lead to more confident predic-\n",
            "tions in the early parts of the target sentence and to less confident predictions in the later parts. How-\n",
            "ever, LSTMs trained on reversed source sentences did much better on long sentences than LSTMs\n",
            "\ftrained on the raw source sentences (see sec. 3.7), which suggests that reversing the input sentences\n",
            "results in LSTMs with better memory utilization.\n",
            "\n",
            "3.4 Training details\n",
            "\n",
            "We found that the LSTM models are fairly easy to train. We used deep LSTMs with 4 layers,\n",
            "with 1000 cells at each layer and 1000 dimensional word embeddings, with an input vocabulary\n",
            "of 160,000 and an output vocabulary of 80,000. Thus the deep LSTM uses 8000 real numbers to\n",
            "represent a sentence. We found deep LSTMs to significantly outperform shallow LSTMs, where\n",
            "each additional layer reduced perplexity by nearly 10%, possibly due to their much larger hidden\n",
            "state. We used a naive softmax over 80,000 words at each output. The resulting LSTM has 384M\n",
            "parameters of which 64M are pure recurrent connections (32M for the “encoder” LSTM and 32M\n",
            "for the “decoder” LSTM). The complete training details are given below:\n",
            "\n",
            "e We initialized all of the LSTM’s parameters with the uniform distribution between -0.08\n",
            "and 0.08\n",
            "\n",
            "e We used stochastic gradient descent without momentum, with a fixed learning rate of 0.7.\n",
            "After 5 epochs, we begun halving the learning rate every half epoch. We trained our models\n",
            "for a total of 7.5 epochs.\n",
            "\n",
            "e We used batches of 128 sequences for the gradient and divided it the size of the batch\n",
            "(namely, 128).\n",
            "\n",
            "e Although LSTMs tend to not suffer from the vanishing gradient problem, they can have\n",
            "exploding gradients. Thus we enforced a hard constraint on the norm of the gradient [10,\n",
            "25] by scaling it when its norm exceeded a threshold. For each training batch, we compute\n",
            "$ = ||g||,, where g is the gradient divided by 128. If s > 5, we set g = 22.\n",
            "\n",
            "e Different sentences have different lengths. Most sentences are short (e.g., length 20-30)\n",
            "but some sentences are long (e.g., length > 100), so a minibatch of 128 randomly chosen\n",
            "training sentences will have many short sentences and few long sentences, and as a result,\n",
            "much of the computation in the minibatch is wasted. To address this problem, we made sure\n",
            "that all sentences in a minibatch are roughly of the same length, yielding a 2x speedup.\n",
            "\n",
            "3.5 Parallelization\n",
            "\n",
            "A C++ implementation of deep LSTM with the configuration from the previous section on a sin-\n",
            "gle GPU processes a speed of approximately 1,700 words per second. This was too slow for our\n",
            "purposes, so we parallelized our model using an 8-GPU machine. Each layer of the LSTM was\n",
            "executed on a different GPU and communicated its activations to the next GPU / layer as soon as\n",
            "they were computed. Our models have 4 layers of LSTMs, each of which resides on a separate\n",
            "GPU. The remaining 4 GPUs were used to parallelize the softmax, so each GPU was responsible\n",
            "for multiplying by a 1000 x 20000 matrix. The resulting implementation achieved a speed of 6,300\n",
            "(both English and French) words per second with a minibatch size of 128. Training took about a ten\n",
            "days with this implementation.\n",
            "\n",
            "3.6 Experimental Results\n",
            "\n",
            "We used the cased BLEU score [24] to evaluate the quality of our translations. We computed our\n",
            "BLEU scores using multi-bleu.p1' on the tokenized predictions and ground truth. This way\n",
            "of evaluating the BELU score is consistent with [5] and [2], and reproduces the 33.3 score of [29].\n",
            "However, if we evaluate the best WMT’ 14 system [9] (whose predictions can be downloaded from\n",
            "statmt.org\\matrix) in this manner, we get 37.0, which is greater than the 35.8 reported by\n",
            "statmt.org\\matrix.\n",
            "\n",
            "The results are presented in tables 1 and 2. Our best results are obtained with an ensemble of LSTMs\n",
            "that differ in their random initializations and in the random order of minibatches. While the decoded\n",
            "translations of the LSTM ensemble do not outperform the best WMT’ 14 system, it is the first time\n",
            "that a pure neural translation system outperforms a phrase-based SMT baseline on a large scale MT\n",
            "\n",
            "‘There several variants of the BLEU score, and each variant is defined with a perl script.\n",
            "\ftest BLEU score (ntsti4)\n",
            "Bahdanau et al. [2] 28.45\n",
            "Baseline System [29] 33.30\n",
            "\n",
            "Single forward LSTM, beam size 12 26.17\n",
            "\n",
            "Table 1: The performance of the LSTM on WMT’ 14 English to French test set (ntst14). Note that\n",
            "an ensemble of 5 LSTMs with a beam of size 2 is cheaper than of a single LSTM with a beam of\n",
            "size 12.\n",
            "\n",
            " \n",
            "\n",
            "test BLEU score (ntsti4)\n",
            "Baseline System [29] 33.30\n",
            "\n",
            "Cho et al. [5] 34.54\n",
            "Best WMT14 result [9\n",
            "\n",
            "Rescoring the baseline 1000-best with a single forward LSTM 35.61\n",
            "Rescoring the baseline 1000-best with a single reversed LSTM 35.85\n",
            "Rescoring the baseline 1000-best with an ensemble of 5 reversed LSTMs\n",
            "\n",
            "Oracle Rescoring of the Baseline 1000-best lists\n",
            "\n",
            " \n",
            "\n",
            "Table 2: Methods that use neural networks together with an SMT system on the WMT’ 14 English\n",
            "to French test set (ntst14).\n",
            "\n",
            "task by a sizeable margin, despite its inability to handle out-of-vocabulary words. The LSTM is\n",
            "within 0.5 BLEU points of the best WMT’ 14 result if it is used to rescore the 1000-best list of the\n",
            "baseline system.\n",
            "\n",
            "3.7 Performance on long sentences\n",
            "\n",
            "We were surprised to discover that the LSTM did well on long sentences, which is shown quantita-\n",
            "tively in figure 3. Table 3 presents several examples of long sentences and their translations.\n",
            "\n",
            "3.8 Model Analysis\n",
            "\n",
            "al O | was given a card by her in the garden\n",
            "\n",
            "3F OMary admires John 10; O Inthe garden , she gave me a card\n",
            "O She gave me a card in the garden\n",
            "27 OMary is in love with John\n",
            "\n",
            "OMary respects John\n",
            "aL OJohn admires Mary\n",
            "-55 O She was given a card by me in the garden\n",
            "\n",
            "-2+ OJohn is in love with Mar\n",
            "y © In the garden , | gave her a card\n",
            "\n",
            "-5- | OJohn respects Mary © I gave her a card in the garden\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "1 1 1 1 L L L L J 0 L L L L L 1 J\n",
            "-8 -6 -4 -2 0 2 4 6 8 10 -15 -10 -5 0 5 10 15 20\n",
            "\n",
            "Figure 2: The figure shows a 2-dimensional PCA projection of the LSTM hidden states that are obtained\n",
            "after processing the phrases in the figures. The phrases are clustered by meaning, which in these examples is\n",
            "primarily a function of word order, which would be difficult to capture with a bag-of-words model. Notice that\n",
            "both clusters have similar internal structure.\n",
            "\n",
            "One of the attractive features of our model is its ability to turn a sequence of words into a vector\n",
            "of fixed dimensionality. Figure 2 visualizes some of the learned representations. The figure clearly\n",
            "shows that the representations are sensitive to the order of words, while being fairly insensitive to the\n",
            "\fOur model | Ulrich UNK , membre du conseil d’ administration du constructeur automobile Audi ,\n",
            "affirme qu’ ils’ agit d’ une pratique courante depuis des années pour que les teléphones\n",
            "portables puissent étre collectés avant les reunions du conseil d’ administration afin qu’ ils\n",
            "ne soient pas utilises comme appareils d’ écoute a distance .\n",
            "\n",
            "Truth Ulrich Hackenberg , membre du conseil d’ administration du constructeur automobile Audi ,\n",
            "déclare que la collecte des téléphones portables avant les reunions du conseil , afin qu’ ils\n",
            "ne puissent pas étre utilisés comme appareils d’ écoute a distance , est une pratique courante\n",
            "depuis des années .\n",
            "\n",
            "Our model | “ Les telephones cellulaires , qui sont vraiment une question , non seulement parce qu’ ils\n",
            "pourraient potentiellement causer des interférences avec les appareils de navigation , mais\n",
            "nous savons , selon la FCC , qu’ ils pourraient interférer avec les tours de téléphone cellulaire\n",
            "lorsqu’ ils sont dans I’ air” , dit UNK .\n",
            "\n",
            "Truth “ Les telephones portables sont véritablement un probleme , non seulement parce qu’ ils\n",
            "pourraient éventuellement créer des interférences avec les instruments de navigation , mais\n",
            "parce que nous savons , d’ apres la FCC , qu’ ils pourraient perturber les antennes-relais de\n",
            "telephonie mobile s’ ils sont utilisés a bord ” , a declaré Rosenker .\n",
            "\n",
            "Our model | Avec la crémation , il y a un “ sentiment de violence contre le corps d’ un étre cher ” ,\n",
            "\n",
            "qui sera “ réduit a une pile de cendres ” en trés peu de temps au lieu d’ un processus de\n",
            "decomposition “ qui accompagnera les étapes du deuil ” .\n",
            "\n",
            "Truth Il y a, avec la cremation , “ une violence faite au corps aimé ” ,\n",
            "\n",
            "a qui va étre “ reduit a un tas de cendres ” en trés peu de temps , et non aprés un processus de\n",
            "\n",
            "decomposition , qui “ accompagnerait les phases du deuil ” .\n",
            "\n",
            " \n",
            "\n",
            "Table 3: A few examples of long translations produced by the LSTM alongside the ground truth\n",
            "translations. The reader can verify that the translations are sensible using Google translate.\n",
            "\n",
            "   \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            ">— LSTM (34.8) <+—~ LSTM (34.8)\n",
            "40 e—e baseline (33.3) 40 e—e baseline (33.3)\n",
            "35 35\n",
            "Y Y\n",
            "° °\n",
            "U U\n",
            "wn wn\n",
            "D> D>\n",
            "4 30 47 30\n",
            "a a\n",
            "25 25\n",
            "20 1 1 1 1 1 1 1 20 1 1 1 1 1 1\n",
            "478 12 17 22 28 35 79 0 500 1000 1500 2000 2500 3000 3500\n",
            "test sentences sorted by their length test sentences sorted by average word frequency rank\n",
            "\n",
            "Figure 3: The left plot shows the performance of our system as a function of sentence length, where the\n",
            "x-axis corresponds to the test sentences sorted by their length and is marked by the actual sequence lengths.\n",
            "There is no degradation on sentences with less than 35 words, there is only a minor degradation on the longest\n",
            "sentences. The right plot shows the LSTM’s performance on sentences with progressively more rare words,\n",
            "where the x-axis corresponds to the test sentences sorted by their “average word frequency rank”.\n",
            "\n",
            "replacement of an active voice with a passive voice. The two-dimensional projections are obtained\n",
            "using PCA.\n",
            "\n",
            "4 Related work\n",
            "\n",
            "There is a large body of work on applications of neural networks to machine translation. So far,\n",
            "the simplest and most effective way of applying an RNN-Language Model (RNNLM) [23] or a\n",
            "\fFeedforward Neural Network Language Model (NNLM) [3] to an MT task is by rescoring the n-\n",
            "best lists of a strong MT baseline [22], which reliably improves translation quality.\n",
            "\n",
            "More recently, researchers have begun to look into ways of including information about the source\n",
            "language into the NNLM. Examples of this work include Auli et al. [1], who combine an NNLM\n",
            "with a topic model of the input sentence, which improves rescoring performance. Devlin et al. [8]\n",
            "followed a similar approach, but they incorporated their NNLM into the decoder of an MT system\n",
            "and used the decoder’s alignment information to provide the NNLM with the most useful words in\n",
            "the input sentence. Their approach was highly successful and it achieved large improvements over\n",
            "their baseline.\n",
            "\n",
            "Our work is closely related to Kalchbrenner and Blunsom [18], who were the first to map the input\n",
            "sentence into a vector and then back to a sentence, although they map sentences to vectors using\n",
            "convolutional neural networks, which lose the ordering of the words. Similarly to this work, Cho et\n",
            "al. [5] used an LSTM-like RNN architecture to map sentences into vectors and back, although their\n",
            "primary focus was on integrating their neural network into an SMT system. Bahdanau et al. [2] also\n",
            "attempted direct translations with a neural network that used an attention mechanism to overcome\n",
            "the poor performance on long sentences experienced by Cho et al. [5] and achieved encouraging\n",
            "results. Likewise, Pouget-Abadie et al. [26] attempted to address the memory problem of Cho et\n",
            "al. [5] by translating pieces of the source sentence in way that produces smooth translations, which\n",
            "is similar to a phrase-based approach. We suspect that they could achieve similar improvements by\n",
            "simply training their networks on reversed source sentences.\n",
            "\n",
            "End-to-end training is also the focus of Hermann et al. [12], whose model represents the inputs and\n",
            "outputs by feedforward networks, and map them to similar points in space. However, their approach\n",
            "cannot generate translations directly: to get a translation, they need to do a look up for closest vector\n",
            "in the pre-computed database of sentences, or to rescore a sentence.\n",
            "\n",
            "5 Conclusion\n",
            "\n",
            "In this work, we showed that a large deep LSTM, that has a limited vocabulary and that makes\n",
            "almost no assumption about problem structure can outperform a standard SMT-based system whose\n",
            "vocabulary is unlimited on a large-scale MT task. The success of our simple LSTM-based approach\n",
            "on MT suggests that it should do well on many other sequence learning problems, provided they\n",
            "have enough training data.\n",
            "\n",
            "We were surprised by the extent of the improvement obtained by reversing the words in the source\n",
            "sentences. We conclude that it is important to find a problem encoding that has the greatest number\n",
            "of short term dependencies, as they make the learning problem much simpler. In particular, while\n",
            "we were unable to train a standard RNN on the non-reversed translation problem (shown in fig. 1),\n",
            "we believe that a standard RNN should be easily trainable when the source sentences are reversed\n",
            "(although we did not verify it experimentally).\n",
            "\n",
            "We were also surprised by the ability of the LSTM to correctly translate very long sentences. We\n",
            "were initially convinced that the LSTM would fail on long sentences due to its limited memory,\n",
            "and other researchers reported poor performance on long sentences with a model similar to ours\n",
            "[5, 2, 26]. And yet, LSTMs trained on the reversed dataset had little difficulty translating long\n",
            "sentences.\n",
            "\n",
            "Most importantly, we demonstrated that a simple, straightforward and a relatively unoptimized ap-\n",
            "proach can outperform an SMT system, so further work will likely lead to even greater translation\n",
            "accuracies. These results suggest that our approach will likely do well on other challenging sequence\n",
            "to sequence problems.\n",
            "\n",
            "6 Acknowledgments\n",
            "\n",
            "We thank Samy Bengio, Jeff Dean, Matthieu Devin, Geoffrey Hinton, Nal Kalchbrenner, Thang Luong, Wolf-\n",
            "gang Macherey, Rajat Monga, Vincent Vanhoucke, Peng Xu, Wojciech Zaremba, and the Google Brain team\n",
            "for useful comments and discussions.\n",
            "\fReferences\n",
            "\n",
            "[1]\n",
            "[2]\n",
            "[3]\n",
            "[4]\n",
            "[5]\n",
            "\n",
            "[6]\n",
            "[7]\n",
            "\n",
            "[8]\n",
            "[9]\n",
            "[10]\n",
            "[11]\n",
            "[12]\n",
            "\n",
            "[13]\n",
            "\n",
            "[14]\n",
            "[15]\n",
            "[16]\n",
            "[17]\n",
            "[18]\n",
            "[19]\n",
            "[20]\n",
            "[21]\n",
            "[22]\n",
            "[23]\n",
            "[24]\n",
            "[25]\n",
            "\n",
            "[26]\n",
            "\n",
            "[27]\n",
            "[28]\n",
            "[29]\n",
            "[30]\n",
            "\n",
            "[31]\n",
            "\n",
            "M. Auli, M. Galley, C. Quirk, and G. Zweig. Joint language and translation modeling with recurrent\n",
            "neural networks. In EMNLP, 2013.\n",
            "\n",
            "D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate.\n",
            "arXiv preprint arXiv: 1409.0473, 2014.\n",
            "\n",
            "Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural probabilistic language model. In Journal of\n",
            "Machine Learning Research, pages 1137-1155, 2003.\n",
            "\n",
            "Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult.\n",
            "IEEE Transactions on Neural Networks, 5(2):157—-166, 1994.\n",
            "\n",
            "K. Cho, B. Merrienboer, C. Gulcehre, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase represen-\n",
            "tations using RNN encoder-decoder for statistical machine translation. In Arxiv preprint arXiv: 1406.1078,\n",
            "2014.\n",
            "\n",
            "D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classification.\n",
            "In CVPR, 2012.\n",
            "\n",
            "G. E. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent pre-trained deep neural networks for large\n",
            "vocabulary speech recognition. IEEE Transactions on Audio, Speech, and Language Processing - Special\n",
            "Issue on Deep Learning for Speech and Language Processing, 2012.\n",
            "\n",
            "J. Devlin, R. Zbib, Z. Huang, T. Lamar, R. Schwartz, and J. Makhoul. Fast and robust neural network\n",
            "joint models for statistical machine translation. In ACL, 2014.\n",
            "\n",
            "Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Heafield. Edinburgh’s phrase-based machine\n",
            "translation systems for wmt-14. In WMT, 2014.\n",
            "\n",
            "A. Graves. Generating sequences with recurrent neural networks. In Arxiv preprint arXiv: 1308.0850,\n",
            "2013.\n",
            "\n",
            "A. Graves, S. Fernandez, F. Gomez, and J. Schmidhuber. Connectionist temporal classification: labelling\n",
            "unsegmented sequence data with recurrent neural networks. In ICML, 2006.\n",
            "\n",
            "K. M. Hermann and P. Blunsom. Multilingual distributed representations without word alignment. In\n",
            "ICLR, 2014.\n",
            "\n",
            "G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,\n",
            "T. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition. IEEE\n",
            "Signal Processing Magazine, 2012.\n",
            "\n",
            "S. Hochreiter. Untersuchungen zu dynamischen neuronalen netzen. Master’s thesis, Institut fur Infor-\n",
            "matik, Technische Universitat, Munchen, 1991.\n",
            "\n",
            "S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. Gradient flow in recurrent nets: the difficulty\n",
            "of learning long-term dependencies, 2001.\n",
            "\n",
            "S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 1997.\n",
            "\n",
            "S. Hochreiter and J. Schmidhuber. LSTM can solve hard long time lag problems. 1997.\n",
            "\n",
            "N. Kalchbrenner and P. Blunsom. Recurrent continuous translation models. In EMNLP, 2013.\n",
            "\n",
            "A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification with deep convolutional neural\n",
            "networks. In NIPS, 2012.\n",
            "\n",
            "Q.V. Le, M.A. Ranzato, R. Monga, M. Devin, K. Chen, G.S. Corrado, J. Dean, and A.Y. Ng. Building\n",
            "high-level features using large scale unsupervised learning. In ICML, 2012.\n",
            "\n",
            "Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.\n",
            "Proceedings of the IEEE, 1998.\n",
            "\n",
            "T. Mikolov. Statistical Language Models based on Neural Networks. PhD thesis, Brno University of\n",
            "Technology, 2012.\n",
            "\n",
            "T. Mikolov, M. Karafiat, L. Burget, J. Cernocky, and S. Khudanpur. Recurrent neural network based\n",
            "language model. In INTERSPEECH, pages 1045-1048, 2010.\n",
            "\n",
            "K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. BLEU: a method for automatic evaluation of machine\n",
            "translation. In ACL, 2002.\n",
            "\n",
            "R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. arXiv\n",
            "preprint arXiv:1211.5063, 2012.\n",
            "\n",
            "J. Pouget-Abadie, D. Bahdanau, B. van Merrienboer, K. Cho, and Y. Bengio. Overcoming the\n",
            "curse of sentence length for neural machine translation using automatic segmentation. arXiv preprint\n",
            "arXiv:1409.1257, 2014.\n",
            "\n",
            "A. Razborov. On small depth threshold circuits. In Proc. 3rd Scandinavian Workshop on Algorithm\n",
            "Theory, 1992.\n",
            "\n",
            "D. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors.\n",
            "Nature, 323(6088):533—536, 1986.\n",
            "\n",
            "H. Schwenk. University le mans. http://www-lium.univ-lemans.fr/~schwenk/cslm_\n",
            "joint_paper/, 2014. [Online; accessed 03-September-2014].\n",
            "\n",
            "M. Sundermeyer, R. Schluter, and H. Ney. LSTM neural networks for language modeling. In INTER-\n",
            "SPEECH, 2010.\n",
            "\n",
            "P. Werbos. Backpropagation through time: what it does and how to do it. Proceedings of IEEE, 1990.\n",
            "\f\n"
          ]
        }
      ],
      "source": [
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "\n",
        "# Set the path for Poppler\n",
        "poppler_path = '/usr/bin/'\n",
        "\n",
        "# Set the path for Tesseract\n",
        "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n",
        "\n",
        "def pdf_to_text(pdf_path):\n",
        "    images = convert_from_path(pdf_path, 500, poppler_path=poppler_path)\n",
        "    text = ''\n",
        "    for i, image in enumerate(images):\n",
        "        text += pytesseract.image_to_string(image)\n",
        "    return text\n",
        "\n",
        "# Example Usage\n",
        "pdf_path = \"/content/1409.3215v3.pdf\"\n",
        "result_text = pdf_to_text(pdf_path)\n",
        "print(result_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "HOTPDF - Library"
      ],
      "metadata": {
        "id": "biEbOz3KG9vL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hotpdf import HotPdf\n",
        "\n",
        "pdf_file_path = \"/content/1409.3215v3.pdf\"\n",
        "\n",
        "# Load pdf file into memory\n",
        "hotpdf_document = HotPdf(pdf_file_path)\n",
        "\n",
        "# Get number of pages\n",
        "num_pages = len(hotpdf_document.pages)\n",
        "\n",
        "# Extract full page text from all pages\n",
        "full_text = \"\"\n",
        "for page_num in range(num_pages):\n",
        "    full_page_text = hotpdf_document.extract_page_text(page=page_num)\n",
        "    full_text += full_page_text + \"\\n\"  # Add a newline character to separate pages\n",
        "\n",
        "# Print or use the extracted full text\n",
        "print(full_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2GeDIf9GRdx",
        "outputId": "3e7eee24-3c60-45ac-ce62-52e505a0386b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SequencetoSequenceLearning\n",
            "withNeuralNetworks\n",
            "IlyaSutskeverOriolVinyalsQuocV.Le\n",
            "4\n",
            "GoogleGoogleGoogle\n",
            "1\n",
            "ilyasu@google.comvinyals@google.comqvl@google.com\n",
            "0\n",
            "2\n",
            " \n",
            "c\n",
            "e\n",
            "Abstract\n",
            "D\n",
            " \n",
            "DeepNeuralNetworks(DNNs)arepowerfulmodelsthathaveachievedexcel-\n",
            "4\n",
            "lentperformanceondifﬁcultlearningtasks.AlthoughDNNsworkwellwhenever\n",
            "1\n",
            " \n",
            " largelabeledtrainingsetsareavailable,theycannotbeusedtomapsequencesto\n",
            "]\n",
            "sequences.Inthispaper,wepresentageneralend-to-endapproachtosequence\n",
            "L\n",
            "learningthatmakesminimalassumptionsonthesequencestructure.Ourmethod\n",
            "CusesamultilayeredLongShort-TermMemory(LSTM)tomaptheinputsequence\n",
            ".\n",
            "toavectorofaﬁxeddimensionality,andthenanotherdeepLSTMtodecodethe\n",
            "s\n",
            "c\n",
            "targetsequencefromthevector.OurmainresultisthatonanEnglishtoFrench\n",
            "[\n",
            " translationtaskfromtheWMT’14dataset,thetranslationsproducedbytheLSTM\n",
            " \n",
            "achieveaBLEUscoreof34.8ontheentiretestset,wheretheLSTM’sBLEU\n",
            "3\n",
            "scorewaspenalizedonout-of-vocabularywords.Additionally,theLSTMdidnot\n",
            "v\n",
            "havedifﬁcultyonlongsentences.Forcomparison,aphrase-basedSMTsystem\n",
            "5\n",
            "achievesaBLEUscoreof33.3onthesamedataset.WhenweusedtheLSTM\n",
            "1\n",
            "2torerankthe1000hypothesesproducedbytheaforementionedSMTsystem,its\n",
            "3\n",
            "BLEUscoreincreasesto36.5,whichisclosetothepreviousbestresultonthis\n",
            ".\n",
            "task.TheLSTMalsolearnedsensiblephraseandsentencerepresentationsthat\n",
            "9\n",
            "aresensitivetowordorderandarerelativelyinvarianttotheactiveandthepas-\n",
            "0\n",
            "sivevoice.Finally,wefoundthatreversingtheorderofthewordsinallsource\n",
            "4\n",
            "1sentences(butnottargetsentences)improvedtheLSTM’sperformancemarkedly,\n",
            ":\n",
            "becausedoingsointroducedmanyshorttermdependenciesbetweenthesource\n",
            "v\n",
            "i\n",
            "andthetargetsentencewhichmadetheoptimizationproblemeasier.\n",
            "X\n",
            "r\n",
            "a\n",
            "1Introduction\n",
            "DeepNeuralNetworks(DNNs)areextremelypowerfulmachinelearningmodelsthatachieveex-\n",
            "cellentperformanceondifﬁcultproblemssuchasspeechrecognition[13,7]andvisualobjectrecog-\n",
            "nition[19,6,21,20].DNNsarepowerfulbecausetheycanperformarbitraryparallelcomputation\n",
            "foramodestnumberofsteps.AsurprisingexampleofthepowerofDNNsistheirabilitytosort\n",
            "NN-bitnumbersusingonly2hiddenlayersofquadraticsize[27].So,whileneuralnetworksare\n",
            "relatedtoconventionalstatisticalmodels,theylearnanintricatecomputation.Furthermore,large\n",
            "DNNscanbetrainedwithsupervisedbackpropagationwheneverthelabeledtrainingsethasenough\n",
            "informationtospecifythenetwork’sparameters.Thus,ifthereexistsaparametersettingofalarge\n",
            "DNNthatachievesgoodresults(forexample,becausehumanscansolvethetaskveryrapidly),\n",
            "supervisedbackpropagationwillﬁndtheseparametersandsolvetheproblem.\n",
            "Despitetheirﬂexibilityandpower,DNNscanonlybeappliedtoproblemswhoseinputsandtargets\n",
            "canbesensiblyencodedwithvectorsofﬁxeddimensionality.Itisasigniﬁcantlimitation,since\n",
            "manyimportantproblemsarebestexpressedwithsequenceswhoselengthsarenotknowna-priori.\n",
            "Forexample,speechrecognitionandmachinetranslationaresequentialproblems.Likewise,ques-\n",
            "tionansweringcanalsobeseenasmappingasequenceofwordsrepresentingthequestiontoa\n",
            "1\n",
            "\n",
            "sequenceofwordsrepresentingtheanswer.Itisthereforeclearthatadomain-independentmethod\n",
            "thatlearnstomapsequencestosequenceswouldbeuseful.\n",
            "SequencesposeachallengeforDNNsbecausetheyrequirethatthedimensionalityoftheinputsand\n",
            "outputsisknownandﬁxed.Inthispaper,weshowthatastraightforwardapplicationoftheLong\n",
            "Short-TermMemory(LSTM)architecture[16]cansolvegeneralsequencetosequenceproblems\n",
            "TheideaistouseoneLSTMtoreadtheinputsequence,onetimestepatatime,toobtainlargeﬁxed-\n",
            "dimensionalvectorrepresentation,andthentouseanotherLSTMtoextracttheoutputsequence\n",
            "fromthatvector(ﬁg.1).ThesecondLSTMisessentiallyarecurrentneuralnetworklanguagemodel\n",
            "[28,23,30]exceptthatitisconditionedontheinputsequence.TheLSTM’sabilitytosuccessfully\n",
            "learnondatawithlongrangetemporaldependenciesmakesitanaturalchoiceforthisapplication\n",
            "duetotheconsiderabletimelagbetweentheinputsandtheircorrespondingoutputs(ﬁg.1).\n",
            "Therehavebeenanumberofrelatedattemptstoaddressthegeneralsequencetosequencelearning\n",
            "problemwithneuralnetworks.OurapproachiscloselyrelatedtoKalchbrennerandBlunsom[18]\n",
            "whoweretheﬁrsttomaptheentireinputsentencetovector,andisrelatedtoChoetal.[5]although\n",
            "thelatterwasusedonlyforrescoringhypothesesproducedbyaphrase-basedsystem.Graves[10]\n",
            "introducedanoveldifferentiableattentionmechanismthatallowsneuralnetworkstofocusondif-\n",
            "ferentpartsoftheirinput,andanelegantvariantofthisideawassuccessfullyappliedtomachine\n",
            "translationbyBahdanauetal.[2].TheConnectionistSequenceClassiﬁcationisanotherpopular\n",
            "techniqueformappingsequencestosequenceswithneuralnetworks,butitassumesamonotonic\n",
            "alignmentbetweentheinputsandtheoutputs[11].\n",
            "Figure1:Ourmodelreadsaninputsentence“ABC”andproduces“WXYZ”astheoutputsentence.The\n",
            "modelstopsmakingpredictionsafteroutputtingtheend-of-sentencetoken.NotethattheLSTMreadsthe\n",
            "inputsentenceinreverse,becausedoingsointroducesmanyshorttermdependenciesinthedatathatmakethe\n",
            "optimizationproblemmucheasier.\n",
            "Themainresultofthisworkisthefollowing.OntheWMT’14EnglishtoFrenchtranslationtask,\n",
            "weobtainedaBLEUscoreof34.81bydirectlyextractingtranslationsfromanensembleof5deep\n",
            "LSTMs(with384Mparametersand8,000dimensionalstateeach)usingasimpleleft-to-rightbeam-\n",
            "searchdecoder.Thisisbyfarthebestresultachievedbydirecttranslationwithlargeneuralnet-\n",
            "works.Forcomparison,theBLEUscoreofanSMTbaselineonthisdatasetis33.30[29].The34.81\n",
            "BLEUscorewasachievedbyanLSTMwithavocabularyof80kwords,sothescorewaspenalized\n",
            "wheneverthereferencetranslationcontainedawordnotcoveredbythese80k.Thisresultshows\n",
            "thatarelativelyunoptimizedsmall-vocabularyneuralnetworkarchitecturewhichhasmuchroom\n",
            "forimprovementoutperformsaphrase-basedSMTsystem.\n",
            "Finally,weusedtheLSTMtorescorethepubliclyavailable1000-bestlistsoftheSMTbaselineon\n",
            "thesametask[29].Bydoingso,weobtainedaBLEUscoreof36.5,whichimprovesthebaselineby\n",
            "3.2BLEUpointsandisclosetothepreviousbestpublishedresultonthistask(whichis37.0[9]).\n",
            "Surprisingly,theLSTMdidnotsufferonverylongsentences,despitetherecentexperienceofother\n",
            "researcherswithrelatedarchitectures[26].Wewereabletodowellonlongsentencesbecausewe\n",
            "reversedtheorderofwordsinthesourcesentencebutnotthetargetsentencesinthetrainingandtest\n",
            "set.Bydoingso,weintroducedmanyshorttermdependenciesthatmadetheoptimizationproblem\n",
            "muchsimpler(seesec.2and3.3).Asaresult,SGDcouldlearnLSTMsthathadnotroublewith\n",
            "longsentences.Thesimpletrickofreversingthewordsinthesourcesentenceisoneofthekey\n",
            "technicalcontributionsofthiswork.\n",
            "AusefulpropertyoftheLSTMisthatitlearnstomapaninputsentenceofvariablelengthinto\n",
            "aﬁxed-dimensionalvectorrepresentation.Giventhattranslationstendtobeparaphrasesofthe\n",
            "sourcesentences,thetranslationobjectiveencouragestheLSTMtoﬁndsentencerepresentations\n",
            "thatcapturetheirmeaning,assentenceswithsimilarmeaningsareclosetoeachotherwhiledifferent\n",
            "2\n",
            "\n",
            "sentencesmeaningswillbefar.Aqualitativeevaluationsupportsthisclaim,showingthatourmodel\n",
            "isawareofwordorderandisfairlyinvarianttotheactiveandpassivevoice.\n",
            "2Themodel\n",
            "TheRecurrentNeuralNetwork(RNN)[31,28]isanaturalgeneralizationoffeedforwardneural\n",
            "networkstosequences.Givenasequenceofinputs(x1,...,xT),astandardRNNcomputesa\n",
            "sequenceofoutputs(y1,...,yT)byiteratingthefollowingequation:\n",
            "ht=sigm(cid:0)Whxxt+Whhht−1(cid:1)\n",
            "yt=Wyhht\n",
            "TheRNNcaneasilymapsequencestosequenceswheneverthealignmentbetweentheinputsthe\n",
            "outputsisknownaheadoftime.However,itisnotclearhowtoapplyanRNNtoproblemswhose\n",
            "inputandtheoutputsequenceshavedifferentlengthswithcomplicatedandnon-monotonicrelation-\n",
            "ships.\n",
            "Thesimpleststrategyforgeneralsequencelearningistomaptheinputsequencetoaﬁxed-sized\n",
            "vectorusingoneRNN,andthentomapthevectortothetargetsequencewithanotherRNN(this\n",
            "approachhasalsobeentakenbyChoetal.[5]).WhileitcouldworkinprinciplesincetheRNNis\n",
            "providedwithalltherelevantinformation,itwouldbedifﬁculttotraintheRNNsduetotheresulting\n",
            "longtermdependencies(ﬁgure1)[14,4,16,15].However,theLongShort-TermMemory(LSTM)\n",
            "[16]isknowntolearnproblemswithlongrangetemporaldependencies,soanLSTMmaysucceed\n",
            "inthissetting.\n",
            "ThegoaloftheLSTMistoestimatetheconditionalprobabilityp(y1,...,yT′|x1,...,xT)where\n",
            "(x1,...,xT)isaninputsequenceandy1,...,yT′isitscorrespondingoutputsequencewhoselength\n",
            "T′maydifferfromT.TheLSTMcomputesthisconditionalprobabilitybyﬁrstobtainingtheﬁxed-\n",
            "dimensionalrepresentationvoftheinputsequence(x1,...,xT)givenbythelasthiddenstateofthe\n",
            "LSTM,andthencomputingtheprobabilityofy1,...,yT′withastandardLSTM-LMformulation\n",
            "whoseinitialhiddenstateissettotherepresentationvofx1,...,xT:\n",
            "T′\n",
            "(1)\n",
            "p(y1,...,yT′|x1,...,xT)=p(yt|v,y1,...,yt−1)\n",
            "Y\n",
            "t=1\n",
            "Inthisequation,eachp(yt|v,y1,...,yt−1)distributionisrepresentedwithasoftmaxoverallthe\n",
            "wordsinthevocabulary.WeusetheLSTMformulationfromGraves[10].Notethatwerequirethat\n",
            "eachsentenceendswithaspecialend-of-sentencesymbol“<EOS>”,whichenablesthemodelto\n",
            "deﬁneadistributionoversequencesofallpossiblelengths.Theoverallschemeisoutlinedinﬁgure\n",
            "1,wheretheshownLSTMcomputestherepresentationof“A”,“B”,“C”,“<EOS>”andthenuses\n",
            "thisrepresentationtocomputetheprobabilityof“W”,“X”,“Y”,“Z”,“<EOS>”.\n",
            "Ouractualmodelsdifferfromtheabovedescriptioninthreeimportantways.First,weusedtwo\n",
            "differentLSTMs:onefortheinputsequenceandanotherfortheoutputsequence,becausedoing\n",
            "soincreasesthenumbermodelparametersatnegligiblecomputationalcostandmakesitnaturalto\n",
            "traintheLSTMonmultiplelanguagepairssimultaneously[18].Second,wefoundthatdeepLSTMs\n",
            "signiﬁcantlyoutperformedshallowLSTMs,sowechoseanLSTMwithfourlayers.Third,wefound\n",
            "itextremelyvaluabletoreversetheorderofthewordsoftheinputsentence.Soforexample,instead\n",
            "ofmappingthesentencea,b,ctothesentenceα,β,γ,theLSTMisaskedtomapc,b,atoα,β,γ,\n",
            "whereα,β,γisthetranslationofa,b,c.Thisway,aisincloseproximitytoα,bisfairlyclosetoβ,\n",
            "andsoon,afactthatmakesiteasyforSGDto“establishcommunication”betweentheinputandthe\n",
            "output.WefoundthissimpledatatransformationtogreatlyimprovetheperformanceoftheLSTM.\n",
            "3Experiments\n",
            "WeappliedourmethodtotheWMT’14EnglishtoFrenchMTtaskintwoways.Weuseditto\n",
            "directlytranslatetheinputsentencewithoutusingareferenceSMTsystemandweittorescorethe\n",
            "n-bestlistsofanSMTbaseline.Wereporttheaccuracyofthesetranslationmethods,presentsample\n",
            "translations,andvisualizetheresultingsentencerepresentation.\n",
            "3\n",
            "\n",
            "3.1Datasetdetails\n",
            "WeusedtheWMT’14EnglishtoFrenchdataset.Wetrainedourmodelsonasubsetof12Msen-\n",
            "tencesconsistingof348MFrenchwordsand304MEnglishwords,whichisaclean“selected”\n",
            "subsetfrom[29].Wechosethistranslationtaskandthisspeciﬁctrainingsetsubsetbecauseofthe\n",
            "publicavailabilityofatokenizedtrainingandtestsettogetherwith1000-bestlistsfromthebaseline\n",
            "SMT[29].\n",
            "Astypicalneurallanguagemodelsrelyonavectorrepresentationforeachword,weusedaﬁxed\n",
            "vocabularyforbothlanguages.Weused160,000ofthemostfrequentwordsforthesourcelanguage\n",
            "and80,000ofthemostfrequentwordsforthetargetlanguage.Everyout-of-vocabularywordwas\n",
            "replacedwithaspecial“UNK”token.\n",
            "3.2DecodingandRescoring\n",
            "ThecoreofourexperimentsinvolvedtrainingalargedeepLSTMonmanysentencepairs.We\n",
            "traineditbymaximizingthelogprobabilityofacorrecttranslationTgiventhesourcesentenceS,\n",
            "sothetrainingobjectiveis\n",
            "logp(T|S)\n",
            "1/|S|X\n",
            "(T,S)∈S\n",
            "whereSisthetrainingset.Oncetrainingiscomplete,weproducetranslationsbyﬁndingthemost\n",
            "likelytranslationaccordingtotheLSTM:\n",
            "(2)\n",
            "Tˆ=argmaxp(T|S)\n",
            "T\n",
            "Wesearchforthemostlikelytranslationusingasimpleleft-to-rightbeamsearchdecoderwhich\n",
            "maintainsasmallnumberBofpartialhypotheses,whereapartialhypothesisisapreﬁxofsome\n",
            "translation.Ateachtimestepweextendeachpartialhypothesisinthebeamwitheverypossible\n",
            "wordinthevocabulary.Thisgreatlyincreasesthenumberofthehypothesessowediscardallbut\n",
            "theBmostlikelyhypothesesaccordingtothemodel’slogprobability.Assoonasthe“<EOS>”\n",
            "symbolisappendedtoahypothesis,itisremovedfromthebeamandisaddedtothesetofcomplete\n",
            "hypotheses.Whilethisdecoderisapproximate,itissimpletoimplement.Interestingly,oursystem\n",
            "performswellevenwithabeamsizeof1,andabeamofsize2providesmostofthebeneﬁtsofbeam\n",
            "search(Table1).\n",
            "WealsousedtheLSTMtorescorethe1000-bestlistsproducedbythebaselinesystem[29].To\n",
            "rescoreann-bestlist,wecomputedthelogprobabilityofeveryhypothesiswithourLSTMandtook\n",
            "anevenaveragewiththeirscoreandtheLSTM’sscore.\n",
            "3.3ReversingtheSourceSentences\n",
            "WhiletheLSTMiscapableofsolvingproblemswithlongtermdependencies,wediscoveredthat\n",
            "theLSTMlearnsmuchbetterwhenthesourcesentencesarereversed(thetargetsentencesarenot\n",
            "reversed).Bydoingso,theLSTM’stestperplexitydroppedfrom5.8to4.7,andthetestBLEU\n",
            "scoresofitsdecodedtranslationsincreasedfrom25.9to30.6.\n",
            "Whilewedonothaveacompleteexplanationtothisphenomenon,webelievethatitiscausedby\n",
            "theintroductionofmanyshorttermdependenciestothedataset.Normally,whenweconcatenatea\n",
            "sourcesentencewithatargetsentence,eachwordinthesourcesentenceisfarfromitscorresponding\n",
            "wordinthetargetsentence.Asaresult,theproblemhasalarge“minimaltimelag”[17].By\n",
            "reversingthewordsinthesourcesentence,theaveragedistancebetweencorrespondingwordsin\n",
            "thesourceandtargetlanguageisunchanged.However,theﬁrstfewwordsinthesourcelanguage\n",
            "arenowveryclosetotheﬁrstfewwordsinthetargetlanguage,sotheproblem’sminimaltimelagis\n",
            "greatlyreduced.Thus,backpropagationhasaneasiertime“establishingcommunication”between\n",
            "thesourcesentenceandthetargetsentence,whichinturnresultsinsubstantiallyimprovedoverall\n",
            "performance.\n",
            "Initially,webelievedthatreversingtheinputsentenceswouldonlyleadtomoreconﬁdentpredic-\n",
            "tionsintheearlypartsofthetargetsentenceandtolessconﬁdentpredictionsinthelaterparts.How-\n",
            "ever,LSTMstrainedonreversedsourcesentencesdidmuchbetteronlongsentencesthanLSTMs\n",
            "4\n",
            "\n",
            "trainedontherawsourcesentences(seesec.3.7),whichsuggeststhatreversingtheinputsentences\n",
            "resultsinLSTMswithbettermemoryutilization.\n",
            "3.4Trainingdetails\n",
            "WefoundthattheLSTMmodelsarefairlyeasytotrain.WeuseddeepLSTMswith4layers,\n",
            "with1000cellsateachlayerand1000dimensionalwordembeddings,withaninputvocabulary\n",
            "of160,000andanoutputvocabularyof80,000.ThusthedeepLSTMuses8000realnumbersto\n",
            "representasentence.WefounddeepLSTMstosigniﬁcantlyoutperformshallowLSTMs,where\n",
            "eachadditionallayerreducedperplexitybynearly10%,possiblyduetotheirmuchlargerhidden\n",
            "state.Weusedanaivesoftmaxover80,000wordsateachoutput.TheresultingLSTMhas384M\n",
            "parametersofwhich64Marepurerecurrentconnections(32Mforthe“encoder”LSTMand32M\n",
            "forthe“decoder”LSTM).Thecompletetrainingdetailsaregivenbelow:\n",
            "•WeinitializedalloftheLSTM’sparameterswiththeuniformdistributionbetween-0.08\n",
            "and0.08\n",
            "•Weusedstochasticgradientdescentwithoutmomentum,withaﬁxedlearningrateof0.7.\n",
            "After5epochs,webegunhalvingthelearningrateeveryhalfepoch.Wetrainedourmodels\n",
            "foratotalof7.5epochs.\n",
            "•Weusedbatchesof128sequencesforthegradientanddivideditthesizeofthebatch\n",
            "(namely,128).\n",
            "•AlthoughLSTMstendtonotsufferfromthevanishinggradientproblem,theycanhave\n",
            "explodinggradients.Thusweenforcedahardconstraintonthenormofthegradient[10,\n",
            "25]byscalingitwhenitsnormexceededathreshold.Foreachtrainingbatch,wecompute\n",
            "s=kgk2,wheregisthegradientdividedby128.Ifs>5,wesetg=5g\n",
            "s.\n",
            "•Differentsentenceshavedifferentlengths.Mostsentencesareshort(e.g.,length20-30)\n",
            "butsomesentencesarelong(e.g.,length>100),soaminibatchof128randomlychosen\n",
            "trainingsentenceswillhavemanyshortsentencesandfewlongsentences,andasaresult,\n",
            "muchofthecomputationintheminibatchiswasted.Toaddressthisproblem,wemadesure\n",
            "thatallsentencesinaminibatchareroughlyofthesamelength,yieldinga2xspeedup.\n",
            "3.5Parallelization\n",
            "AC++implementationofdeepLSTMwiththeconﬁgurationfromtheprevioussectiononasin-\n",
            "gleGPUprocessesaspeedofapproximately1,700wordspersecond.Thiswastooslowforour\n",
            "purposes,soweparallelizedourmodelusingan8-GPUmachine.EachlayeroftheLSTMwas\n",
            "executedonadifferentGPUandcommunicateditsactivationstothenextGPU/layerassoonas\n",
            "theywerecomputed.Ourmodelshave4layersofLSTMs,eachofwhichresidesonaseparate\n",
            "GPU.Theremaining4GPUswereusedtoparallelizethesoftmax,soeachGPUwasresponsible\n",
            "formultiplyingbya1000×20000matrix.Theresultingimplementationachievedaspeedof6,300\n",
            "(bothEnglishandFrench)wordspersecondwithaminibatchsizeof128.Trainingtookaboutaten\n",
            "dayswiththisimplementation.\n",
            "3.6ExperimentalResults\n",
            "WeusedthecasedBLEUscore[24]toevaluatethequalityofourtranslations.Wecomputedour\n",
            "BLEUscoresusingmulti-bleu.pl1onthetokenizedpredictionsandgroundtruth.Thisway\n",
            "ofevaluatingtheBELUscoreisconsistentwith[5]and[2],andreproducesthe33.3scoreof[29].\n",
            "However,ifweevaluatethebestWMT’14system[9](whosepredictionscanbedownloadedfrom\n",
            "statmt.org\\matrix)inthismanner,weget37.0,whichisgreaterthanthe35.8reportedby\n",
            "statmt.org\\matrix.\n",
            "Theresultsarepresentedintables1and2.OurbestresultsareobtainedwithanensembleofLSTMs\n",
            "thatdifferintheirrandominitializationsandintherandomorderofminibatches.Whilethedecoded\n",
            "translationsoftheLSTMensembledonotoutperformthebestWMT’14system,itistheﬁrsttime\n",
            "thatapureneuraltranslationsystemoutperformsaphrase-basedSMTbaselineonalargescaleMT\n",
            "1ThereseveralvariantsoftheBLEUscore,andeachvariantisdeﬁnedwithaperlscript.\n",
            "5\n",
            "\n",
            "MethodtestBLEUscore(ntst14)\n",
            "Bahdanauetal.[2]28.45\n",
            "BaselineSystem[29]33.30\n",
            "SingleforwardLSTM,beamsize1226.17\n",
            "SinglereversedLSTM,beamsize1230.59\n",
            "Ensembleof5reversedLSTMs,beamsize133.00\n",
            "Ensembleof2reversedLSTMs,beamsize1233.27\n",
            "Ensembleof5reversedLSTMs,beamsize234.50\n",
            "Ensembleof5reversedLSTMs,beamsize1234.81\n",
            "Table1:TheperformanceoftheLSTMonWMT’14EnglishtoFrenchtestset(ntst14).Notetha\n",
            "anensembleof5LSTMswithabeamofsize2ischeaperthanofasingleLSTMwithabeamo\n",
            "size12.\n",
            "MethodtestBLEUscore(ntst14)\n",
            "BaselineSystem[29]33.30\n",
            "Choetal.[5]34.54\n",
            "BestWMT’14result[9]37.0\n",
            "Rescoringthebaseline1000-bestwithasingleforwardLSTM35.61\n",
            "Rescoringthebaseline1000-bestwithasinglereversedLSTM35.85\n",
            "Rescoringthebaseline1000-bestwithanensembleof5reversedLSTMs36.5\n",
            "OracleRescoringoftheBaseline1000-bestlists∼45\n",
            "Table2:MethodsthatuseneuralnetworkstogetherwithanSMTsystemontheWMT’14English\n",
            "toFrenchtestset(ntst14).\n",
            "taskbyasizeablemargin,despiteitsinabilitytohandleout-of-vocabularywords.TheLSTMis\n",
            "within0.5BLEUpointsofthebestWMT’14resultifitisusedtorescorethe1000-bestlistofthe\n",
            "baselinesystem.\n",
            "3.7Performanceonlongsentences\n",
            "WeweresurprisedtodiscoverthattheLSTMdidwellonlongsentences,whichisshownquantita\n",
            "tivelyinﬁgure3.Table3presentsseveralexamplesoflongsentencesandtheirtranslations.\n",
            "3.8ModelAnalysis\n",
            "15\n",
            "I was given a card by her in the garden\n",
            "4\n",
            "310\n",
            "Mary admires JohnIn the garden , she gave me a card\n",
            "She gave me a card in the garden\n",
            "2\n",
            "Mary is in love with John\n",
            "5\n",
            "1\n",
            "00\n",
            "Mary respects John\n",
            "John admires Mary\n",
            "−1\n",
            "−5\n",
            "She was given a card by me in the garden\n",
            "−2\n",
            "John is in love with Mary\n",
            "In the garden , I gave her a card\n",
            "−3−10\n",
            "−4\n",
            "−15\n",
            "I gave her a card in the garden\n",
            "John respects Mary\n",
            "−5\n",
            "−6−20\n",
            "−8−6−4−20246810−15−10−505101520\n",
            "Figure2:Theﬁgureshowsa2-dimensionalPCAprojectionoftheLSTMhiddenstatesthatareobtained\n",
            "afterprocessingthephrasesintheﬁgures.Thephrasesareclusteredbymeaning,whichintheseexamplesi\n",
            "primarilyafunctionofwordorder,whichwouldbedifﬁculttocapturewithabag-of-wordsmodel.Noticetha\n",
            "bothclustershavesimilarinternalstructure.\n",
            "Oneoftheattractivefeaturesofourmodelisitsabilitytoturnasequenceofwordsintoavecto\n",
            "ofﬁxeddimensionality.Figure2visualizessomeofthelearnedrepresentations.Theﬁgureclearly\n",
            "showsthattherepresentationsaresensitivetotheorderofwords,whilebeingfairlyinsensitivetothe\n",
            "6\n",
            "\n",
            "TypeSentence\n",
            "OurmodelUlrichUNK,membreduconseild’administrationduconstructeurautomobileAudi,\n",
            "afﬁrmequ’ils’agitd’unepratiquecourantedepuisdesann´eespourquelest´ele´phones\n",
            "portablespuissenteˆtrecollect´esavantlesre´unionsduconseild’administrationaﬁnqu’ils\n",
            "nesoientpasutilise´scommeappareilsd’´ecoutea`distance.\n",
            "TruthUlrichHackenberg,membreduconseild’administrationduconstructeurautomobileAudi,\n",
            "d´eclarequelacollectedeste´l´ephonesportablesavantlesre´unionsduconseil,aﬁnqu’ils\n",
            "nepuissentpaseˆtreutilis´escommeappareilsd’e´coute`adistance,estunepratiquecourante\n",
            "depuisdesanne´es.\n",
            "Ourmodel“Leste´l´ephonescellulaires,quisontvraimentunequestion,nonseulementparcequ’ils\n",
            "pourraientpotentiellementcauserdesinterfe´rencesaveclesappareilsdenavigation,mais\n",
            "noussavons,selonlaFCC,qu’ilspourraientinterfe´reraveclestoursdet´ele´phonecellulaire\n",
            "lorsqu’ilssontdansl’air”,ditUNK.\n",
            "Truth“Leste´l´ephonesportablessontve´ritablementunproble`me,nonseulementparcequ’ils\n",
            "pourraient´eventuellementcre´erdesinterfe´rencesaveclesinstrumentsdenavigation,mais\n",
            "parcequenoussavons,d’apre`slaFCC,qu’ilspourraientperturberlesantennes-relaisde\n",
            "te´l´ephoniemobiles’ilssontutilis´es`abord”,ad´eclare´Rosenker.\n",
            "OurmodelAveclacr´emation,ilyaun“sentimentdeviolencecontrelecorpsd’uneˆtrecher”,\n",
            "quisera“re´duita`unepiledecendres”entre`speudetempsaulieud’unprocessusde\n",
            "d´ecomposition“quiaccompagnerales´etapesdudeuil”.\n",
            "TruthIlya,aveclacr´emation,“uneviolencefaiteaucorpsaime´”,\n",
            "quivaˆetre“r´eduit`auntasdecendres”entre`speudetemps,etnonapr`esunprocessusde\n",
            "d´ecomposition,qui“accompagneraitlesphasesdudeuil”.\n",
            "Table3:AfewexamplesoflongtranslationsproducedbytheLSTMalongsidethegroundtruth\n",
            "translations.ThereadercanverifythatthetranslationsaresensibleusingGoogletranslate.\n",
            "LSTM  (34.8)LSTM  (34.8)\n",
            "40baseline (33.3)40baseline (33.3)\n",
            "3535\n",
            "ee\n",
            "rr\n",
            "oo\n",
            "cc\n",
            "ss\n",
            "  \n",
            "UU\n",
            "EE\n",
            "L30L30\n",
            "BB\n",
            "2525\n",
            "2020\n",
            "4781217222835790500100015002000250030003500\n",
            "test sentences sorted by their lengthtest sentences sorted by average word frequency rank\n",
            "Figure3:Theleftplotshowstheperformanceofoursystemasafunctionofsentencelength,wherethe\n",
            "x-axiscorrespondstothetestsentencessortedbytheirlengthandismarkedbytheactualsequencelengths.\n",
            "Thereisnodegradationonsentenceswithlessthan35words,thereisonlyaminordegradationonthelongest\n",
            "sentences.TherightplotshowstheLSTM’sperformanceonsentenceswithprogressivelymorerarewords,\n",
            "wherethex-axiscorrespondstothetestsentencessortedbytheir“averagewordfrequencyrank”.\n",
            "replacementofanactivevoicewithapassivevoice.Thetwo-dimensionalprojectionsareobtained\n",
            "usingPCA.\n",
            "4Relatedwork\n",
            "Thereisalargebodyofworkonapplicationsofneuralnetworkstomachinetranslation.Sofar,\n",
            "thesimplestandmosteffectivewayofapplyinganRNN-LanguageModel(RNNLM)[23]ora\n",
            "7\n",
            "\n",
            "FeedforwardNeuralNetworkLanguageModel(NNLM)[3]toanMTtaskisbyrescoringthen-\n",
            "bestlistsofastrongMTbaseline[22],whichreliablyimprovestranslationquality.\n",
            "Morerecently,researchershavebeguntolookintowaysofincludinginformationaboutthesource\n",
            "languageintotheNNLM.ExamplesofthisworkincludeAulietal.[1],whocombineanNNLM\n",
            "withatopicmodeloftheinputsentence,whichimprovesrescoringperformance.Devlinetal.[8]\n",
            "followedasimilarapproach,buttheyincorporatedtheirNNLMintothedecoderofanMTsystem\n",
            "andusedthedecoder’salignmentinformationtoprovidetheNNLMwiththemostusefulwordsin\n",
            "theinputsentence.Theirapproachwashighlysuccessfulanditachievedlargeimprovementsover\n",
            "theirbaseline.\n",
            "OurworkiscloselyrelatedtoKalchbrennerandBlunsom[18],whoweretheﬁrsttomaptheinput\n",
            "sentenceintoavectorandthenbacktoasentence,althoughtheymapsentencestovectorsusing\n",
            "convolutionalneuralnetworks,whichlosetheorderingofthewords.Similarlytothiswork,Choet\n",
            "al.[5]usedanLSTM-likeRNNarchitecturetomapsentencesintovectorsandback,althoughtheir\n",
            "primaryfocuswasonintegratingtheirneuralnetworkintoanSMTsystem.Bahdanauetal.[2]also\n",
            "attempteddirecttranslationswithaneuralnetworkthatusedanattentionmechanismtoovercome\n",
            "thepoorperformanceonlongsentencesexperiencedbyChoetal.[5]andachievedencouraging\n",
            "results.Likewise,Pouget-Abadieetal.[26]attemptedtoaddressthememoryproblemofChoet\n",
            "al.[5]bytranslatingpiecesofthesourcesentenceinwaythatproducessmoothtranslations,which\n",
            "issimilartoaphrase-basedapproach.Wesuspectthattheycouldachievesimilarimprovementsby\n",
            "simplytrainingtheirnetworksonreversedsourcesentences.\n",
            "End-to-endtrainingisalsothefocusofHermannetal.[12],whosemodelrepresentstheinputsand\n",
            "outputsbyfeedforwardnetworks,andmapthemtosimilarpointsinspace.However,theirapproach\n",
            "cannotgeneratetranslationsdirectly:togetatranslation,theyneedtodoalookupforclosestvector\n",
            "inthepre-computeddatabaseofsentences,ortorescoreasentence.\n",
            "5Conclusion\n",
            "Inthiswork,weshowedthatalargedeepLSTM,thathasalimitedvocabularyandthatmakes\n",
            "almostnoassumptionaboutproblemstructurecanoutperformastandardSMT-basedsystemwhose\n",
            "vocabularyisunlimitedonalarge-scaleMTtask.ThesuccessofoursimpleLSTM-basedapproach\n",
            "onMTsuggeststhatitshoulddowellonmanyothersequencelearningproblems,providedthey\n",
            "haveenoughtrainingdata.\n",
            "Weweresurprisedbytheextentoftheimprovementobtainedbyreversingthewordsinthesource\n",
            "sentences.Weconcludethatitisimportanttoﬁndaproblemencodingthathasthegreatestnumber\n",
            "ofshorttermdependencies,astheymakethelearningproblemmuchsimpler.Inparticular,while\n",
            "wewereunabletotrainastandardRNNonthenon-reversedtranslationproblem(showninﬁg.1)\n",
            "webelievethatastandardRNNshouldbeeasilytrainablewhenthesourcesentencesarereversed\n",
            "(althoughwedidnotverifyitexperimentally).\n",
            "WewerealsosurprisedbytheabilityoftheLSTMtocorrectlytranslateverylongsentences.We\n",
            "wereinitiallyconvincedthattheLSTMwouldfailonlongsentencesduetoitslimitedmemory\n",
            "andotherresearchersreportedpoorperformanceonlongsentenceswithamodelsimilartoours\n",
            "[5,2,26].Andyet,LSTMstrainedonthereverseddatasethadlittledifﬁcultytranslatinglong\n",
            "sentences.\n",
            "Mostimportantly,wedemonstratedthatasimple,straightforwardandarelativelyunoptimizedap-\n",
            "proachcanoutperformanSMTsystem,sofurtherworkwilllikelyleadtoevengreatertranslation\n",
            "accuracies.Theseresultssuggestthatourapproachwilllikelydowellonotherchallengingsequence\n",
            "tosequenceproblems.\n",
            "6Acknowledgments\n",
            "WethankSamyBengio,JeffDean,MatthieuDevin,GeoffreyHinton,NalKalchbrenner,ThangLuong,Wolf-\n",
            "gangMacherey,RajatMonga,VincentVanhoucke,PengXu,WojciechZaremba,andtheGoogleBrainteam\n",
            "forusefulcommentsanddiscussions.\n",
            "8\n",
            "\n",
            "References\n",
            "[1]M.Auli,M.Galley,C.Quirk,andG.Zweig.Jointlanguageandtranslationmodelingwithrecurrent\n",
            "neuralnetworks.InEMNLP,2013.\n",
            "[2]D.Bahdanau,K.Cho,andY.Bengio.Neuralmachinetranslationbyjointlylearningtoalignandtranslate\n",
            "arXivpreprintarXiv:1409.0473,2014.\n",
            "[3]Y.Bengio,R.Ducharme,P.Vincent,andC.Jauvin.Aneuralprobabilisticlanguagemodel.InJournalof\n",
            "MachineLearningResearch,pages1137–1155,2003.\n",
            "[4]Y.Bengio,P.Simard,andP.Frasconi.Learninglong-termdependencieswithgradientdescentisdifﬁcult\n",
            "IEEETransactionsonNeuralNetworks,5(2):157–166,1994.\n",
            "[5]K.Cho,B.Merrienboer,C.Gulcehre,F.Bougares,H.Schwenk,andY.Bengio.Learningphraserepresen-\n",
            "tationsusingRNNencoder-decoderforstatisticalmachinetranslation.InArxivpreprintarXiv:1406.1078,\n",
            "2014.\n",
            "[6]D.Ciresan,U.Meier,andJ.Schmidhuber.Multi-columndeepneuralnetworksforimageclassiﬁcation\n",
            "InCVPR,2012.\n",
            "[7]G.E.Dahl,D.Yu,L.Deng,andA.Acero.Context-dependentpre-traineddeepneuralnetworksforlarge\n",
            "vocabularyspeechrecognition.IEEETransactionsonAudio,Speech,andLanguageProcessing-Special\n",
            "IssueonDeepLearningforSpeechandLanguageProcessing,2012.\n",
            "[8]J.Devlin,R.Zbib,Z.Huang,T.Lamar,R.Schwartz,andJ.Makhoul.Fastandrobustneuralnetwork\n",
            "jointmodelsforstatisticalmachinetranslation.InACL,2014.\n",
            "[9]NadirDurrani,BarryHaddow,PhilippKoehn,andKennethHeaﬁeld.Edinburgh’sphrase-basedmachine\n",
            "translationsystemsforwmt-14.InWMT,2014.\n",
            "[10]A.Graves.Generatingsequenceswithrecurrentneuralnetworks.InArxivpreprintarXiv:1308.0850,\n",
            "2013.\n",
            "[11]A.Graves,S.Ferna´ndez,F.Gomez,andJ.Schmidhuber.Connectionisttemporalclassiﬁcation:labelling\n",
            "unsegmentedsequencedatawithrecurrentneuralnetworks.InICML,2006.\n",
            "[12]K.M.HermannandP.Blunsom.Multilingualdistributedrepresentationswithoutwordalignment.In\n",
            "ICLR,2014.\n",
            "[13]G.Hinton,L.Deng,D.Yu,G.Dahl,A.Mohamed,N.Jaitly,A.Senior,V.Vanhoucke,P.Nguyen\n",
            "T.Sainath,andB.Kingsbury.Deepneuralnetworksforacousticmodelinginspeechrecognition.IEEE\n",
            "SignalProcessingMagazine,2012.\n",
            "[14]S.Hochreiter.Untersuchungenzudynamischenneuronalennetzen.Master’sthesis,InstitutfurInfor-\n",
            "matik,TechnischeUniversitat,Munchen,1991.\n",
            "[15]S.Hochreiter,Y.Bengio,P.Frasconi,andJ.Schmidhuber.Gradientﬂowinrecurrentnets:thedifﬁculty\n",
            "oflearninglong-termdependencies,2001.\n",
            "[16]S.HochreiterandJ.Schmidhuber.Longshort-termmemory.NeuralComputation,1997.\n",
            "[17]S.HochreiterandJ.Schmidhuber.LSTMcansolvehardlongtimelagproblems.1997.\n",
            "[18]N.KalchbrennerandP.Blunsom.Recurrentcontinuoustranslationmodels.InEMNLP,2013.\n",
            "[19]A.Krizhevsky,I.Sutskever,andG.E.Hinton.ImageNetclassiﬁcationwithdeepconvolutionalneural\n",
            "networks.InNIPS,2012.\n",
            "[20]Q.V.Le,M.A.Ranzato,R.Monga,M.Devin,K.Chen,G.S.Corrado,J.Dean,andA.Y.Ng.Building\n",
            "high-levelfeaturesusinglargescaleunsupervisedlearning.InICML,2012.\n",
            "[21]Y.LeCun,L.Bottou,Y.Bengio,andP.Haffner.Gradient-basedlearningappliedtodocumentrecognition\n",
            "ProceedingsoftheIEEE,1998.\n",
            "[22]T.Mikolov.StatisticalLanguageModelsbasedonNeuralNetworks.PhDthesis,BrnoUniversityof\n",
            "Technology,2012.\n",
            "[23]T.Mikolov,M.Karaﬁ´at,L.Burget,J.Cernocky`,andS.Khudanpur.Recurrentneuralnetworkbased\n",
            "languagemodel.InINTERSPEECH,pages1045–1048,2010.\n",
            "[24]K.Papineni,S.Roukos,T.Ward,andW.J.Zhu.BLEU:amethodforautomaticevaluationofmachine\n",
            "translation.InACL,2002.\n",
            "[25]R.Pascanu,T.Mikolov,andY.Bengio.Onthedifﬁcultyoftrainingrecurrentneuralnetworks.arXiv\n",
            "preprintarXiv:1211.5063,2012.\n",
            "[26]J.Pouget-Abadie,D.Bahdanau,B.vanMerrienboer,K.Cho,andY.Bengio.Overcomingthe\n",
            "curseofsentencelengthforneuralmachinetranslationusingautomaticsegmentation.arXivpreprint\n",
            "arXiv:1409.1257,2014.\n",
            "[27]A.Razborov.Onsmalldepththresholdcircuits.InProc.3rdScandinavianWorkshoponAlgorithm\n",
            "Theory,1992.\n",
            "[28]D.Rumelhart,G.E.Hinton,andR.J.Williams.Learningrepresentationsbyback-propagatingerrors\n",
            "Nature,323(6088):533–536,1986.\n",
            "[29]H.Schwenk.Universitylemans.http://www-lium.univ-lemans.fr/˜schwenk/cslm_\n",
            "joint_paper/,2014.[Online;accessed03-September-2014].\n",
            "[30]M.Sundermeyer,R.Schluter,andH.Ney.LSTMneuralnetworksforlanguagemodeling.InINTER-\n",
            "SPEECH,2010.\n",
            "[31]P.Werbos.Backpropagationthroughtime:whatitdoesandhowtodoit.ProceedingsofIEEE,1990.\n",
            "9\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PYMUPDF Library"
      ],
      "metadata": {
        "id": "oPApYD5kHgk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "def extract_text_from_all_pages(pdf_path):\n",
        "    # Open the PDF file\n",
        "    pdf_document = fitz.open(pdf_path)\n",
        "\n",
        "    # Get the total number of pages\n",
        "    total_pages = pdf_document.page_count\n",
        "\n",
        "    # Extract text from all pages\n",
        "    full_text_by_page = []\n",
        "\n",
        "    for page_number in range(total_pages):\n",
        "        # Get the specific page\n",
        "        page = pdf_document[page_number]\n",
        "\n",
        "        # Extract text from the page\n",
        "        page_text = page.get_text(\"text\")\n",
        "        full_text_by_page.append(page_text)\n",
        "\n",
        "    # Close the PDF file\n",
        "    pdf_document.close()\n",
        "\n",
        "    return full_text_by_page\n",
        "\n",
        "# Example usage:\n",
        "pdf_file_path = \"/content/1409.3215v3.pdf\"  # Replace with the path to your PDF file\n",
        "all_pages_text = extract_text_from_all_pages(pdf_file_path)\n",
        "\n",
        "# Now, all_pages_text is a list containing the text from each page\n",
        "for page_number, text in enumerate(all_pages_text):\n",
        "    print(f\"Page {page_number + 1}:\\n{text}\\n{'=' * 50}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS2ANGNFHfyl",
        "outputId": "cd83d7d6-9793-4c5d-f52e-10df7a9bbe0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page 1:\n",
            "arXiv:1409.3215v3  [cs.CL]  14 Dec 2014\n",
            "Sequence to Sequence Learning\n",
            "with Neural Networks\n",
            "Ilya Sutskever\n",
            "Google\n",
            "ilyasu@google.com\n",
            "Oriol Vinyals\n",
            "Google\n",
            "vinyals@google.com\n",
            "Quoc V. Le\n",
            "Google\n",
            "qvl@google.com\n",
            "Abstract\n",
            "Deep Neural Networks (DNNs) are powerful models that have achieved excel-\n",
            "lent performance on difﬁcult learning tasks. Although DNNs work well whenever\n",
            "large labeled training sets are available, they cannot be used to map sequences to\n",
            "sequences. In this paper, we present a general end-to-end approach to sequence\n",
            "learning that makes minimal assumptions on the sequence structure. Our method\n",
            "uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence\n",
            "to a vector of a ﬁxed dimensionality, and then another deep LSTM to decode the\n",
            "target sequence from the vector. Our main result is that on an English to French\n",
            "translation task from the WMT’14 dataset, the translations produced by the LSTM\n",
            "achieve a BLEU score of 34.8 on the entire test set, where the LSTM’s BLEU\n",
            "score was penalized on out-of-vocabulary words. Additionally, the LSTM did not\n",
            "have difﬁculty on long sentences. For comparison, a phrase-based SMT system\n",
            "achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM\n",
            "to rerank the 1000 hypotheses produced by the aforementioned SMT system, its\n",
            "BLEU score increases to 36.5, which is close to the previous best result on this\n",
            "task. The LSTM also learned sensible phrase and sentence representations that\n",
            "are sensitive to word order and are relatively invariant to the active and the pas-\n",
            "sive voice. Finally, we found that reversing the order of the words in all source\n",
            "sentences (but not target sentences) improved the LSTM’s performance markedly,\n",
            "because doing so introduced many short term dependencies between the source\n",
            "and the target sentence which made the optimization problem easier.\n",
            "1\n",
            "Introduction\n",
            "Deep Neural Networks (DNNs) are extremely powerful machine learning models that achieve ex-\n",
            "cellent performance on difﬁcult problems such as speech recognition [13, 7] and visual object recog-\n",
            "nition [19, 6, 21, 20]. DNNs are powerful because they can perform arbitrary parallel computation\n",
            "for a modest number of steps. A surprising example of the power of DNNs is their ability to sort\n",
            "N N-bit numbers using only 2 hidden layers of quadratic size [27]. So, while neural networks are\n",
            "related to conventional statistical models, they learn an intricate computation. Furthermore, large\n",
            "DNNs can be trained with supervised backpropagation whenever the labeled training set has enough\n",
            "information to specify the network’s parameters. Thus, if there exists a parameter setting of a large\n",
            "DNN that achieves good results (for example, because humans can solve the task very rapidly),\n",
            "supervised backpropagation will ﬁnd these parameters and solve the problem.\n",
            "Despite their ﬂexibility and power, DNNs can only be applied to problems whose inputs and targets\n",
            "can be sensibly encoded with vectors of ﬁxed dimensionality. It is a signiﬁcant limitation, since\n",
            "many important problems are best expressed with sequences whose lengths are not known a-priori.\n",
            "For example, speech recognition and machine translation are sequential problems. Likewise, ques-\n",
            "tion answering can also be seen as mapping a sequence of words representing the question to a\n",
            "1\n",
            "\n",
            "==================================================\n",
            "\n",
            "Page 2:\n",
            "sequence of words representing the answer. It is therefore clear that a domain-independent method\n",
            "that learns to map sequences to sequences would be useful.\n",
            "Sequences pose a challenge for DNNs because they require that the dimensionality of the inputs and\n",
            "outputs is known and ﬁxed. In this paper, we show that a straightforward application of the Long\n",
            "Short-Term Memory (LSTM) architecture [16] can solve general sequence to sequence problems.\n",
            "The idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large ﬁxed-\n",
            "dimensional vector representation, and then to use another LSTM to extract the output sequence\n",
            "from that vector (ﬁg. 1). The second LSTM is essentially a recurrent neural network language model\n",
            "[28, 23, 30] except that it is conditioned on the input sequence. The LSTM’s ability to successfully\n",
            "learn on data with long range temporal dependencies makes it a natural choice for this application\n",
            "due to the considerable time lag between the inputs and their corresponding outputs (ﬁg. 1).\n",
            "There have been a number of related attempts to address the general sequence to sequence learning\n",
            "problem with neural networks. Our approach is closely related to Kalchbrenner and Blunsom [18]\n",
            "who were the ﬁrst to map the entire input sentence to vector, and is related to Cho et al. [5] although\n",
            "the latter was used only for rescoring hypotheses produced by a phrase-based system. Graves [10]\n",
            "introduced a novel differentiable attention mechanism that allows neural networks to focus on dif-\n",
            "ferent parts of their input, and an elegant variant of this idea was successfully applied to machine\n",
            "translation by Bahdanau et al. [2]. The Connectionist Sequence Classiﬁcation is another popular\n",
            "technique for mapping sequences to sequences with neural networks, but it assumes a monotonic\n",
            "alignment between the inputs and the outputs [11].\n",
            "Figure 1: Our model reads an input sentence “ABC” and produces “WXYZ” as the output sentence. The\n",
            "model stops making predictions after outputting the end-of-sentence token. Note that the LSTM reads the\n",
            "input sentence in reverse, because doing so introduces many short term dependencies in the data that make the\n",
            "optimization problem much easier.\n",
            "The main result of this work is the following. On the WMT’14 English to French translation task,\n",
            "we obtained a BLEU score of 34.81 by directly extracting translations from an ensemble of 5 deep\n",
            "LSTMs (with 384M parameters and 8,000 dimensional state each) using a simple left-to-right beam-\n",
            "search decoder. This is by far the best result achieved by direct translation with large neural net-\n",
            "works. For comparison, the BLEU score of an SMT baseline on this dataset is 33.30 [29]. The 34.81\n",
            "BLEU score was achieved by an LSTM with a vocabulary of 80k words, so the score was penalized\n",
            "whenever the reference translation contained a word not covered by these 80k. This result shows\n",
            "that a relatively unoptimized small-vocabulary neural network architecture which has much room\n",
            "for improvement outperforms a phrase-based SMT system.\n",
            "Finally, we used the LSTM to rescore the publicly available 1000-best lists of the SMT baseline on\n",
            "the same task [29]. By doing so, we obtained a BLEU score of 36.5, which improves the baseline by\n",
            "3.2 BLEU points and is close to the previous best published result on this task (which is 37.0 [9]).\n",
            "Surprisingly, the LSTM did not suffer on very long sentences, despite the recent experience of other\n",
            "researchers with related architectures [26]. We were able to do well on long sentences because we\n",
            "reversed the order of words in the source sentence but not the target sentences in the training and test\n",
            "set. By doing so, we introduced many short term dependencies that made the optimization problem\n",
            "much simpler (see sec. 2 and 3.3). As a result, SGD could learn LSTMs that had no trouble with\n",
            "long sentences. The simple trick of reversing the words in the source sentence is one of the key\n",
            "technical contributions of this work.\n",
            "A useful property of the LSTM is that it learns to map an input sentence of variable length into\n",
            "a ﬁxed-dimensional vector representation. Given that translations tend to be paraphrases of the\n",
            "source sentences, the translation objective encourages the LSTM to ﬁnd sentence representations\n",
            "that capture their meaning, as sentences with similar meanings are close to each other while different\n",
            "2\n",
            "\n",
            "==================================================\n",
            "\n",
            "Page 3:\n",
            "sentences meanings will be far. A qualitative evaluation supports this claim, showing that our model\n",
            "is aware of word order and is fairly invariant to the active and passive voice.\n",
            "2\n",
            "The model\n",
            "The Recurrent Neural Network (RNN) [31, 28] is a natural generalization of feedforward neural\n",
            "networks to sequences. Given a sequence of inputs (x1, . . . , xT ), a standard RNN computes a\n",
            "sequence of outputs (y1, . . . , yT ) by iterating the following equation:\n",
            "ht\n",
            "=\n",
            "sigm\n",
            "\u0000W hxxt + W hhht−1\n",
            "\u0001\n",
            "yt\n",
            "=\n",
            "W yhht\n",
            "The RNN can easily map sequences to sequences whenever the alignment between the inputs the\n",
            "outputs is known ahead of time. However, it is not clear how to apply an RNN to problems whose\n",
            "input and the output sequences have different lengths with complicated and non-monotonic relation-\n",
            "ships.\n",
            "The simplest strategy for general sequence learning is to map the input sequence to a ﬁxed-sized\n",
            "vector using one RNN, and then to map the vector to the target sequence with another RNN (this\n",
            "approach has also been taken by Cho et al. [5]). While it could work in principle since the RNN is\n",
            "provided with all the relevant information, it would be difﬁcult to train the RNNs due to the resulting\n",
            "long term dependencies (ﬁgure 1) [14, 4, 16, 15]. However, the Long Short-Term Memory (LSTM)\n",
            "[16] is known to learn problems with long range temporal dependencies, so an LSTM may succeed\n",
            "in this setting.\n",
            "The goal of the LSTM is to estimate the conditional probability p(y1, . . . , yT ′|x1, . . . , xT ) where\n",
            "(x1, . . . , xT ) is an input sequence and y1, . . . , yT ′ is its corresponding output sequence whose length\n",
            "T ′ may differ from T . The LSTM computes this conditional probability by ﬁrst obtaining the ﬁxed-\n",
            "dimensional representation v of the input sequence (x1, . . . , xT ) given by the last hidden state of the\n",
            "LSTM, and then computing the probability of y1, . . . , yT ′ with a standard LSTM-LM formulation\n",
            "whose initial hidden state is set to the representation v of x1, . . . , xT :\n",
            "p(y1, . . . , yT ′|x1, . . . , xT ) =\n",
            "T ′\n",
            "Y\n",
            "t=1\n",
            "p(yt|v, y1, . . . , yt−1)\n",
            "(1)\n",
            "In this equation, each p(yt|v, y1, . . . , yt−1) distribution is represented with a softmax over all the\n",
            "words in the vocabulary. We use the LSTM formulation from Graves [10]. Note that we require that\n",
            "each sentence ends with a special end-of-sentence symbol “<EOS>”, which enables the model to\n",
            "deﬁne a distribution over sequences of all possible lengths. The overall scheme is outlined in ﬁgure\n",
            "1, where the shown LSTM computes the representation of “A”, “B”, “C”, “<EOS>” and then uses\n",
            "this representation to compute the probability of “W”, “X”, “Y”, “Z”, “<EOS>”.\n",
            "Our actual models differ from the above description in three important ways. First, we used two\n",
            "different LSTMs: one for the input sequence and another for the output sequence, because doing\n",
            "so increases the number model parameters at negligible computational cost and makes it natural to\n",
            "train the LSTM on multiple language pairs simultaneously [18]. Second, we found that deep LSTMs\n",
            "signiﬁcantly outperformed shallow LSTMs, so we chose an LSTM with four layers. Third, we found\n",
            "it extremely valuable to reverse the order of the words of the input sentence. So for example, instead\n",
            "of mapping the sentence a, b, c to the sentence α, β, γ, the LSTM is asked to map c, b, a to α, β, γ,\n",
            "where α, β, γ is the translation of a, b, c. This way, a is in close proximity to α, b is fairly close to β,\n",
            "and so on, a fact that makes it easy for SGD to “establish communication” between the input and the\n",
            "output. We found this simple data transformation to greatly improve the performance of the LSTM.\n",
            "3\n",
            "Experiments\n",
            "We applied our method to the WMT’14 English to French MT task in two ways. We used it to\n",
            "directly translate the input sentence without using a reference SMT system and we it to rescore the\n",
            "n-best lists of an SMT baseline. We report the accuracy of these translation methods, present sample\n",
            "translations, and visualize the resulting sentence representation.\n",
            "3\n",
            "\n",
            "==================================================\n",
            "\n",
            "Page 4:\n",
            "3.1\n",
            "Dataset details\n",
            "We used the WMT’14 English to French dataset. We trained our models on a subset of 12M sen-\n",
            "tences consisting of 348M French words and 304M English words, which is a clean “selected”\n",
            "subset from [29]. We chose this translation task and this speciﬁc training set subset because of the\n",
            "public availability of a tokenized training and test set together with 1000-best lists from the baseline\n",
            "SMT [29].\n",
            "As typical neural language models rely on a vector representation for each word, we used a ﬁxed\n",
            "vocabulary for both languages. We used 160,000 of the most frequent words for the source language\n",
            "and 80,000 of the most frequent words for the target language. Every out-of-vocabulary word was\n",
            "replaced with a special “UNK” token.\n",
            "3.2\n",
            "Decoding and Rescoring\n",
            "The core of our experiments involved training a large deep LSTM on many sentence pairs. We\n",
            "trained it by maximizing the log probability of a correct translation T given the source sentence S,\n",
            "so the training objective is\n",
            "1/|S|\n",
            "X\n",
            "(T,S)∈S\n",
            "log p(T |S)\n",
            "where S is the training set. Once training is complete, we produce translations by ﬁnding the most\n",
            "likely translation according to the LSTM:\n",
            "ˆT = arg max\n",
            "T\n",
            "p(T |S)\n",
            "(2)\n",
            "We search for the most likely translation using a simple left-to-right beam search decoder which\n",
            "maintains a small number B of partial hypotheses, where a partial hypothesis is a preﬁx of some\n",
            "translation. At each timestep we extend each partial hypothesis in the beam with every possible\n",
            "word in the vocabulary. This greatly increases the number of the hypotheses so we discard all but\n",
            "the B most likely hypotheses according to the model’s log probability. As soon as the “<EOS>”\n",
            "symbol is appended to a hypothesis, it is removed from the beam and is added to the set of complete\n",
            "hypotheses. While this decoder is approximate, it is simple to implement. Interestingly, our system\n",
            "performs well even with a beam size of 1, and a beam of size 2 provides most of the beneﬁts of beam\n",
            "search (Table 1).\n",
            "We also used the LSTM to rescore the 1000-best lists produced by the baseline system [29]. To\n",
            "rescore an n-best list, we computed the log probability of every hypothesis with our LSTM and took\n",
            "an even average with their score and the LSTM’s score.\n",
            "3.3\n",
            "Reversing the Source Sentences\n",
            "While the LSTM is capable of solving problems with long term dependencies, we discovered that\n",
            "the LSTM learns much better when the source sentences are reversed (the target sentences are not\n",
            "reversed). By doing so, the LSTM’s test perplexity dropped from 5.8 to 4.7, and the test BLEU\n",
            "scores of its decoded translations increased from 25.9 to 30.6.\n",
            "While we do not have a complete explanation to this phenomenon, we believe that it is caused by\n",
            "the introduction of many short term dependencies to the dataset. Normally, when we concatenate a\n",
            "source sentence with a target sentence, each word in the source sentence is far from its corresponding\n",
            "word in the target sentence. As a result, the problem has a large “minimal time lag” [17]. By\n",
            "reversing the words in the source sentence, the average distance between corresponding words in\n",
            "the source and target language is unchanged. However, the ﬁrst few words in the source language\n",
            "are now very close to the ﬁrst few words in the target language, so the problem’s minimal time lag is\n",
            "greatly reduced. Thus, backpropagation has an easier time “establishing communication” between\n",
            "the source sentence and the target sentence, which in turn results in substantially improved overall\n",
            "performance.\n",
            "Initially, we believed that reversing the input sentences would only lead to more conﬁdent predic-\n",
            "tions in the early parts of the target sentence and to less conﬁdent predictions in the later parts. How-\n",
            "ever, LSTMs trained on reversed source sentences did much better on long sentences than LSTMs\n",
            "4\n",
            "\n",
            "==================================================\n",
            "\n",
            "Page 5:\n",
            "trained on the raw source sentences (see sec. 3.7), which suggests that reversing the input sentences\n",
            "results in LSTMs with better memory utilization.\n",
            "3.4\n",
            "Training details\n",
            "We found that the LSTM models are fairly easy to train. We used deep LSTMs with 4 layers,\n",
            "with 1000 cells at each layer and 1000 dimensional word embeddings, with an input vocabulary\n",
            "of 160,000 and an output vocabulary of 80,000. Thus the deep LSTM uses 8000 real numbers to\n",
            "represent a sentence. We found deep LSTMs to signiﬁcantly outperform shallow LSTMs, where\n",
            "each additional layer reduced perplexity by nearly 10%, possibly due to their much larger hidden\n",
            "state. We used a naive softmax over 80,000 words at each output. The resulting LSTM has 384M\n",
            "parameters of which 64M are pure recurrent connections (32M for the “encoder” LSTM and 32M\n",
            "for the “decoder” LSTM). The complete training details are given below:\n",
            "• We initialized all of the LSTM’s parameters with the uniform distribution between -0.08\n",
            "and 0.08\n",
            "• We used stochastic gradient descent without momentum, with a ﬁxed learning rate of 0.7.\n",
            "After 5 epochs, we begun halving the learning rate every half epoch. We trained our models\n",
            "for a total of 7.5 epochs.\n",
            "• We used batches of 128 sequences for the gradient and divided it the size of the batch\n",
            "(namely, 128).\n",
            "• Although LSTMs tend to not suffer from the vanishing gradient problem, they can have\n",
            "exploding gradients. Thus we enforced a hard constraint on the norm of the gradient [10,\n",
            "25] by scaling it when its norm exceeded a threshold. For each training batch, we compute\n",
            "s = ∥g∥2, where g is the gradient divided by 128. If s > 5, we set g = 5g\n",
            "s .\n",
            "• Different sentences have different lengths. Most sentences are short (e.g., length 20-30)\n",
            "but some sentences are long (e.g., length > 100), so a minibatch of 128 randomly chosen\n",
            "training sentences will have many short sentences and few long sentences, and as a result,\n",
            "much of the computation in the minibatch is wasted. To address this problem, we made sure\n",
            "that all sentences in a minibatch are roughly of the same length, yielding a 2x speedup.\n",
            "3.5\n",
            "Parallelization\n",
            "A C++ implementation of deep LSTM with the conﬁguration from the previous section on a sin-\n",
            "gle GPU processes a speed of approximately 1,700 words per second. This was too slow for our\n",
            "purposes, so we parallelized our model using an 8-GPU machine. Each layer of the LSTM was\n",
            "executed on a different GPU and communicated its activations to the next GPU / layer as soon as\n",
            "they were computed. Our models have 4 layers of LSTMs, each of which resides on a separate\n",
            "GPU. The remaining 4 GPUs were used to parallelize the softmax, so each GPU was responsible\n",
            "for multiplying by a 1000 × 20000 matrix. The resulting implementation achieved a speed of 6,300\n",
            "(both English and French) words per second with a minibatch size of 128. Training took about a ten\n",
            "days with this implementation.\n",
            "3.6\n",
            "Experimental Results\n",
            "We used the cased BLEU score [24] to evaluate the quality of our translations. We computed our\n",
            "BLEU scores using multi-bleu.pl1 on the tokenized predictions and ground truth. This way\n",
            "of evaluating the BELU score is consistent with [5] and [2], and reproduces the 33.3 score of [29].\n",
            "However, if we evaluate the best WMT’14 system [9] (whose predictions can be downloaded from\n",
            "statmt.org\\matrix) in this manner, we get 37.0, which is greater than the 35.8 reported by\n",
            "statmt.org\\matrix.\n",
            "The results are presented in tables 1 and 2. Our best results are obtained with an ensemble of LSTMs\n",
            "that differ in their random initializations and in the random order of minibatches. While the decoded\n",
            "translations of the LSTM ensemble do not outperform the best WMT’14 system, it is the ﬁrst time\n",
            "that a pure neural translation system outperforms a phrase-based SMT baseline on a large scale MT\n",
            "1There several variants of the BLEU score, and each variant is deﬁned with a perl script.\n",
            "5\n",
            "\n",
            "==================================================\n",
            "\n",
            "Page 6:\n",
            "Method\n",
            "test BLEU score (ntst14)\n",
            "Bahdanau et al. [2]\n",
            "28.45\n",
            "Baseline System [29]\n",
            "33.30\n",
            "Single forward LSTM, beam size 12\n",
            "26.17\n",
            "Single reversed LSTM, beam size 12\n",
            "30.59\n",
            "Ensemble of 5 reversed LSTMs, beam size 1\n",
            "33.00\n",
            "Ensemble of 2 reversed LSTMs, beam size 12\n",
            "33.27\n",
            "Ensemble of 5 reversed LSTMs, beam size 2\n",
            "34.50\n",
            "Ensemble of 5 reversed LSTMs, beam size 12\n",
            "34.81\n",
            "Table 1: The performance of the LSTM on WMT’14 English to French test set (ntst14). Note that\n",
            "an ensemble of 5 LSTMs with a beam of size 2 is cheaper than of a single LSTM with a beam of\n",
            "size 12.\n",
            "Method\n",
            "test BLEU score (ntst14)\n",
            "Baseline System [29]\n",
            "33.30\n",
            "Cho et al. [5]\n",
            "34.54\n",
            "Best WMT’14 result [9]\n",
            "37.0\n",
            "Rescoring the baseline 1000-best with a single forward LSTM\n",
            "35.61\n",
            "Rescoring the baseline 1000-best with a single reversed LSTM\n",
            "35.85\n",
            "Rescoring the baseline 1000-best with an ensemble of 5 reversed LSTMs\n",
            "36.5\n",
            "Oracle Rescoring of the Baseline 1000-best lists\n",
            "∼45\n",
            "Table 2: Methods that use neural networks together with an SMT system on the WMT’14 English\n",
            "to French test set (ntst14).\n",
            "task by a sizeable margin, despite its inability to handle out-of-vocabulary words. The LSTM is\n",
            "within 0.5 BLEU points of the best WMT’14 result if it is used to rescore the 1000-best list of the\n",
            "baseline system.\n",
            "3.7\n",
            "Performance on long sentences\n",
            "We were surprised to discover that the LSTM did well on long sentences, which is shown quantita-\n",
            "tively in ﬁgure 3. Table 3 presents several examples of long sentences and their translations.\n",
            "3.8\n",
            "Model Analysis\n",
            "−8\n",
            "−6\n",
            "−4\n",
            "−2\n",
            "0\n",
            "2\n",
            "4\n",
            "6\n",
            "8\n",
            "10\n",
            "−6\n",
            "−5\n",
            "−4\n",
            "−3\n",
            "−2\n",
            "−1\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "John respects Mary\n",
            "Mary respects John\n",
            "John admires Mary\n",
            "Mary admires John\n",
            "Mary is in love with John\n",
            "John is in love with Mary\n",
            "−15\n",
            "−10\n",
            "−5\n",
            "0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "−20\n",
            "−15\n",
            "−10\n",
            "−5\n",
            "0\n",
            "5\n",
            "10\n",
            "15\n",
            "I gave her a card in the garden\n",
            "In the garden , I gave her a card\n",
            "She was given a card by me in the garden\n",
            "She gave me a card in the garden\n",
            "In the garden , she gave me a card\n",
            "I was given a card by her in the garden\n",
            "Figure 2: The ﬁgure shows a 2-dimensional PCA projection of the LSTM hidden states that are obtained\n",
            "after processing the phrases in the ﬁgures. The phrases are clustered by meaning, which in these examples is\n",
            "primarily a function of word order, which would be difﬁcult to capture with a bag-of-words model. Notice that\n",
            "both clusters have similar internal structure.\n",
            "One of the attractive features of our model is its ability to turn a sequence of words into a vector\n",
            "of ﬁxed dimensionality. Figure 2 visualizes some of the learned representations. The ﬁgure clearly\n",
            "shows that the representations are sensitive to the order of words, while being fairly insensitive to the\n",
            "6\n",
            "\n",
            "==================================================\n",
            "\n",
            "Page 7:\n",
            "Type\n",
            "Sentence\n",
            "Our model\n",
            "Ulrich UNK , membre du conseil d’ administration du constructeur automobile Audi ,\n",
            "afﬁrme qu’ il s’ agit d’ une pratique courante depuis des ann´ees pour que les t´el´ephones\n",
            "portables puissent ˆetre collect´es avant les r´eunions du conseil d’ administration aﬁn qu’ ils\n",
            "ne soient pas utilis´es comme appareils d’ ´ecoute `a distance .\n",
            "Truth\n",
            "Ulrich Hackenberg , membre du conseil d’ administration du constructeur automobile Audi ,\n",
            "d´eclare que la collecte des t´el´ephones portables avant les r´eunions du conseil , aﬁn qu’ ils\n",
            "ne puissent pas ˆetre utilis´es comme appareils d’ ´ecoute `a distance , est une pratique courante\n",
            "depuis des ann´ees .\n",
            "Our model\n",
            "“ Les t´el´ephones cellulaires , qui sont vraiment une question , non seulement parce qu’ ils\n",
            "pourraient potentiellement causer des interf´erences avec les appareils de navigation , mais\n",
            "nous savons , selon la FCC , qu’ ils pourraient interf´erer avec les tours de t´el´ephone cellulaire\n",
            "lorsqu’ ils sont dans l’ air ” , dit UNK .\n",
            "Truth\n",
            "“ Les t´el´ephones portables sont v´eritablement un probl`eme , non seulement parce qu’ ils\n",
            "pourraient ´eventuellement cr´eer des interf´erences avec les instruments de navigation , mais\n",
            "parce que nous savons , d’ apr`es la FCC , qu’ ils pourraient perturber les antennes-relais de\n",
            "t´el´ephonie mobile s’ ils sont utilis´es `a bord ” , a d´eclar´e Rosenker .\n",
            "Our model\n",
            "Avec la cr´emation , il y a un “ sentiment de violence contre le corps d’ un ˆetre cher ” ,\n",
            "qui sera “ r´eduit `a une pile de cendres ” en tr`es peu de temps au lieu d’ un processus de\n",
            "d´ecomposition “ qui accompagnera les ´etapes du deuil ” .\n",
            "Truth\n",
            "Il y a , avec la cr´emation , “ une violence faite au corps aim´e ” ,\n",
            "qui va ˆetre “ r´eduit `a un tas de cendres ” en tr`es peu de temps , et non apr`es un processus de\n",
            "d´ecomposition , qui “ accompagnerait les phases du deuil ” .\n",
            "Table 3: A few examples of long translations produced by the LSTM alongside the ground truth\n",
            "translations. The reader can verify that the translations are sensible using Google translate.\n",
            "4 7 8\n",
            "12\n",
            "17\n",
            "22\n",
            "28\n",
            "35\n",
            "79\n",
            "test sentences sorted by their length\n",
            "20\n",
            "25\n",
            "30\n",
            "35\n",
            "40\n",
            "BLEU score\n",
            "LSTM  (34.8)\n",
            "baseline (33.3)\n",
            "0\n",
            "500\n",
            "1000\n",
            "1500\n",
            "2000\n",
            "2500\n",
            "3000\n",
            "3500\n",
            "test sentences sorted by average word frequency rank\n",
            "20\n",
            "25\n",
            "30\n",
            "35\n",
            "40\n",
            "BLEU score\n",
            "LSTM  (34.8)\n",
            "baseline (33.3)\n",
            "Figure 3: The left plot shows the performance of our system as a function of sentence length, where the\n",
            "x-axis corresponds to the test sentences sorted by their length and is marked by the actual sequence lengths.\n",
            "There is no degradation on sentences with less than 35 words, there is only a minor degradation on the longest\n",
            "sentences. The right plot shows the LSTM’s performance on sentences with progressively more rare words,\n",
            "where the x-axis corresponds to the test sentences sorted by their “average word frequency rank”.\n",
            "replacement of an active voice with a passive voice. The two-dimensional projections are obtained\n",
            "using PCA.\n",
            "4\n",
            "Related work\n",
            "There is a large body of work on applications of neural networks to machine translation. So far,\n",
            "the simplest and most effective way of applying an RNN-Language Model (RNNLM) [23] or a\n",
            "7\n",
            "\n",
            "==================================================\n",
            "\n",
            "Page 8:\n",
            "Feedforward Neural Network Language Model (NNLM) [3] to an MT task is by rescoring the n-\n",
            "best lists of a strong MT baseline [22], which reliably improves translation quality.\n",
            "More recently, researchers have begun to look into ways of including information about the source\n",
            "language into the NNLM. Examples of this work include Auli et al. [1], who combine an NNLM\n",
            "with a topic model of the input sentence, which improves rescoring performance. Devlin et al. [8]\n",
            "followed a similar approach, but they incorporated their NNLM into the decoder of an MT system\n",
            "and used the decoder’s alignment information to provide the NNLM with the most useful words in\n",
            "the input sentence. Their approach was highly successful and it achieved large improvements over\n",
            "their baseline.\n",
            "Our work is closely related to Kalchbrenner and Blunsom [18], who were the ﬁrst to map the input\n",
            "sentence into a vector and then back to a sentence, although they map sentences to vectors using\n",
            "convolutional neural networks, which lose the ordering of the words. Similarly to this work, Cho et\n",
            "al. [5] used an LSTM-like RNN architecture to map sentences into vectors and back, although their\n",
            "primary focus was on integrating their neural network into an SMT system. Bahdanau et al. [2] also\n",
            "attempted direct translations with a neural network that used an attention mechanism to overcome\n",
            "the poor performance on long sentences experienced by Cho et al. [5] and achieved encouraging\n",
            "results. Likewise, Pouget-Abadie et al. [26] attempted to address the memory problem of Cho et\n",
            "al. [5] by translating pieces of the source sentence in way that produces smooth translations, which\n",
            "is similar to a phrase-based approach. We suspect that they could achieve similar improvements by\n",
            "simply training their networks on reversed source sentences.\n",
            "End-to-end training is also the focus of Hermann et al. [12], whose model represents the inputs and\n",
            "outputs by feedforward networks, and map them to similar points in space. However, their approach\n",
            "cannot generate translations directly: to get a translation, they need to do a look up for closest vector\n",
            "in the pre-computed database of sentences, or to rescore a sentence.\n",
            "5\n",
            "Conclusion\n",
            "In this work, we showed that a large deep LSTM, that has a limited vocabulary and that makes\n",
            "almost no assumption about problem structure can outperform a standard SMT-based system whose\n",
            "vocabulary is unlimited on a large-scale MT task. The success of our simple LSTM-based approach\n",
            "on MT suggests that it should do well on many other sequence learning problems, provided they\n",
            "have enough training data.\n",
            "We were surprised by the extent of the improvement obtained by reversing the words in the source\n",
            "sentences. We conclude that it is important to ﬁnd a problem encoding that has the greatest number\n",
            "of short term dependencies, as they make the learning problem much simpler. In particular, while\n",
            "we were unable to train a standard RNN on the non-reversed translation problem (shown in ﬁg. 1),\n",
            "we believe that a standard RNN should be easily trainable when the source sentences are reversed\n",
            "(although we did not verify it experimentally).\n",
            "We were also surprised by the ability of the LSTM to correctly translate very long sentences. We\n",
            "were initially convinced that the LSTM would fail on long sentences due to its limited memory,\n",
            "and other researchers reported poor performance on long sentences with a model similar to ours\n",
            "[5, 2, 26]. And yet, LSTMs trained on the reversed dataset had little difﬁculty translating long\n",
            "sentences.\n",
            "Most importantly, we demonstrated that a simple, straightforward and a relatively unoptimized ap-\n",
            "proach can outperform an SMT system, so further work will likely lead to even greater translation\n",
            "accuracies. These results suggest that our approach will likely do well on other challenging sequence\n",
            "to sequence problems.\n",
            "6\n",
            "Acknowledgments\n",
            "We thank Samy Bengio, Jeff Dean, Matthieu Devin, Geoffrey Hinton, Nal Kalchbrenner, Thang Luong, Wolf-\n",
            "gang Macherey, Rajat Monga, Vincent Vanhoucke, Peng Xu, Wojciech Zaremba, and the Google Brain team\n",
            "for useful comments and discussions.\n",
            "8\n",
            "\n",
            "==================================================\n",
            "\n",
            "Page 9:\n",
            "References\n",
            "[1] M. Auli, M. Galley, C. Quirk, and G. Zweig. Joint language and translation modeling with recurrent\n",
            "neural networks. In EMNLP, 2013.\n",
            "[2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate.\n",
            "arXiv preprint arXiv:1409.0473, 2014.\n",
            "[3] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural probabilistic language model. In Journal of\n",
            "Machine Learning Research, pages 1137–1155, 2003.\n",
            "[4] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difﬁcult.\n",
            "IEEE Transactions on Neural Networks, 5(2):157–166, 1994.\n",
            "[5] K. Cho, B. Merrienboer, C. Gulcehre, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase represen-\n",
            "tations using RNN encoder-decoder for statistical machine translation. In Arxiv preprint arXiv:1406.1078,\n",
            "2014.\n",
            "[6] D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classiﬁcation.\n",
            "In CVPR, 2012.\n",
            "[7] G. E. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent pre-trained deep neural networks for large\n",
            "vocabulary speech recognition. IEEE Transactions on Audio, Speech, and Language Processing - Special\n",
            "Issue on Deep Learning for Speech and Language Processing, 2012.\n",
            "[8] J. Devlin, R. Zbib, Z. Huang, T. Lamar, R. Schwartz, and J. Makhoul. Fast and robust neural network\n",
            "joint models for statistical machine translation. In ACL, 2014.\n",
            "[9] Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Heaﬁeld. Edinburgh’s phrase-based machine\n",
            "translation systems for wmt-14. In WMT, 2014.\n",
            "[10] A. Graves. Generating sequences with recurrent neural networks. In Arxiv preprint arXiv:1308.0850,\n",
            "2013.\n",
            "[11] A. Graves, S. Fern´andez, F. Gomez, and J. Schmidhuber. Connectionist temporal classiﬁcation: labelling\n",
            "unsegmented sequence data with recurrent neural networks. In ICML, 2006.\n",
            "[12] K. M. Hermann and P. Blunsom. Multilingual distributed representations without word alignment. In\n",
            "ICLR, 2014.\n",
            "[13] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,\n",
            "T. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition. IEEE\n",
            "Signal Processing Magazine, 2012.\n",
            "[14] S. Hochreiter. Untersuchungen zu dynamischen neuronalen netzen. Master’s thesis, Institut fur Infor-\n",
            "matik, Technische Universitat, Munchen, 1991.\n",
            "[15] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. Gradient ﬂow in recurrent nets: the difﬁculty\n",
            "of learning long-term dependencies, 2001.\n",
            "[16] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 1997.\n",
            "[17] S. Hochreiter and J. Schmidhuber. LSTM can solve hard long time lag problems. 1997.\n",
            "[18] N. Kalchbrenner and P. Blunsom. Recurrent continuous translation models. In EMNLP, 2013.\n",
            "[19] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classiﬁcation with deep convolutional neural\n",
            "networks. In NIPS, 2012.\n",
            "[20] Q.V. Le, M.A. Ranzato, R. Monga, M. Devin, K. Chen, G.S. Corrado, J. Dean, and A.Y. Ng. Building\n",
            "high-level features using large scale unsupervised learning. In ICML, 2012.\n",
            "[21] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.\n",
            "Proceedings of the IEEE, 1998.\n",
            "[22] T. Mikolov. Statistical Language Models based on Neural Networks. PhD thesis, Brno University of\n",
            "Technology, 2012.\n",
            "[23] T. Mikolov, M. Karaﬁ´at, L. Burget, J. Cernock`y, and S. Khudanpur. Recurrent neural network based\n",
            "language model. In INTERSPEECH, pages 1045–1048, 2010.\n",
            "[24] K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. BLEU: a method for automatic evaluation of machine\n",
            "translation. In ACL, 2002.\n",
            "[25] R. Pascanu, T. Mikolov, and Y. Bengio. On the difﬁculty of training recurrent neural networks. arXiv\n",
            "preprint arXiv:1211.5063, 2012.\n",
            "[26] J. Pouget-Abadie, D. Bahdanau, B. van Merrienboer, K. Cho, and Y. Bengio.\n",
            "Overcoming the\n",
            "curse of sentence length for neural machine translation using automatic segmentation. arXiv preprint\n",
            "arXiv:1409.1257, 2014.\n",
            "[27] A. Razborov. On small depth threshold circuits. In Proc. 3rd Scandinavian Workshop on Algorithm\n",
            "Theory, 1992.\n",
            "[28] D. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors.\n",
            "Nature, 323(6088):533–536, 1986.\n",
            "[29] H. Schwenk. University le mans. http://www-lium.univ-lemans.fr/˜schwenk/cslm_\n",
            "joint_paper/, 2014. [Online; accessed 03-September-2014].\n",
            "[30] M. Sundermeyer, R. Schluter, and H. Ney. LSTM neural networks for language modeling. In INTER-\n",
            "SPEECH, 2010.\n",
            "[31] P. Werbos. Backpropagation through time: what it does and how to do it. Proceedings of IEEE, 1990.\n",
            "9\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PYPDF2text"
      ],
      "metadata": {
        "id": "X1m-_y9uk621"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tabula-py\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4N5yVPPlDeR",
        "outputId": "9df5a847-eb2b-40c4-a8a3-62241ecccf7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tabula-py\n",
            "  Downloading tabula_py-2.9.0-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.10/dist-packages (from tabula-py) (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tabula-py) (1.25.2)\n",
            "Requirement already satisfied: distro in /usr/lib/python3/dist-packages (from tabula-py) (1.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.3->tabula-py) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.3->tabula-py) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.25.3->tabula-py) (1.16.0)\n",
            "Installing collected packages: tabula-py\n",
            "Successfully installed tabula-py-2.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tabula\n",
        "\n",
        "def extract_tables_from_pdf(pdf_path):\n",
        "    # Read tables from PDF\n",
        "    tables = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True)\n",
        "\n",
        "    return tables\n",
        "\n",
        "# Example usage:\n",
        "pdf_file_path = \"/content/1409.3215v3.pdf\"  # Replace with the path to your PDF file\n",
        "extracted_tables = extract_tables_from_pdf(pdf_file_path)\n",
        "\n",
        "# Print each table\n",
        "for idx, table in enumerate(extracted_tables):\n",
        "    print(f\"Table {idx + 1}:\\n{table}\\n{'=' * 50}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-QiGC3dk-ZD",
        "outputId": "4e1df39d-0b83-40bb-a1b7-1c3716e8410d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tabula.backend:Error importing jpype dependencies. Fallback to subprocess.\n",
            "WARNING:tabula.backend:No module named 'jpype'\n",
            "WARNING:tabula.backend:Got stderr: Feb 29, 2024 8:39:38 PM org.apache.pdfbox.pdmodel.font.FileSystemFontProvider loadDiskCache\n",
            "WARNING: New fonts found, font cache will be re-built\n",
            "Feb 29, 2024 8:39:38 PM org.apache.pdfbox.pdmodel.font.FileSystemFontProvider <init>\n",
            "WARNING: Building on-disk font cache, this may take a while\n",
            "Feb 29, 2024 8:39:38 PM org.apache.pdfbox.pdmodel.font.FileSystemFontProvider <init>\n",
            "WARNING: Finished building on-disk font cache, found 17 fonts\n",
            "Feb 29, 2024 8:39:38 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font Symbol\n",
            "Feb 29, 2024 8:39:38 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font ZapfDingbats\n",
            "Feb 29, 2024 8:39:41 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\n",
            "WARNING: No Unicode mapping for parenleftbig (0) in font LWDDOL+CMEX10\n",
            "Feb 29, 2024 8:39:41 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\n",
            "WARNING: No Unicode mapping for parenrightbig (1) in font LWDDOL+CMEX10\n",
            "Feb 29, 2024 8:39:41 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\n",
            "WARNING: No Unicode mapping for productdisplay (89) in font LWDDOL+CMEX10\n",
            "Feb 29, 2024 8:39:41 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\n",
            "WARNING: No Unicode mapping for summationdisplay (88) in font LWDDOL+CMEX10\n",
            "Feb 29, 2024 8:39:41 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\n",
            "WARNING: No Unicode mapping for bardbl (107) in font QBOELJ+CMSY10\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table 1:\n",
            "                                       Method  test BLEU score (ntst14)\n",
            "0                         Bahdanau et al. [2]                     28.45\n",
            "1                        Baseline System [29]                     33.30\n",
            "2           Single forward LSTM, beam size 12                     26.17\n",
            "3          Single reversed LSTM, beam size 12                     30.59\n",
            "4   Ensemble of 5 reversed LSTMs, beam size 1                     33.00\n",
            "5  Ensemble of 2 reversed LSTMs, beam size 12                     33.27\n",
            "6   Ensemble of 5 reversed LSTMs, beam size 2                     34.50\n",
            "7  Ensemble of 5 reversed LSTMs, beam size 12                     34.81\n",
            "==================================================\n",
            "\n",
            "Table 2:\n",
            "                                              Method test BLEU score (ntst14)\n",
            "0                               Baseline System [29]                    33.30\n",
            "1                                     Cho et al. [5]                    34.54\n",
            "2                             Best WMT’14 result [9]                     37.0\n",
            "3  Rescoring the baseline 1000-best with a single...                    35.61\n",
            "4  Rescoring the baseline 1000-best with a single...                    35.85\n",
            "5  Rescoring the baseline 1000-best with an ensem...                     36.5\n",
            "6   Oracle Rescoring of the Baseline 1000-best lists                      ∼45\n",
            "==================================================\n",
            "\n",
            "Table 3:\n",
            "  LSTM  (34.8)LSTM  (34.8)\\r40\\rbaseline (33.3)40\\rbaseline (33.3)\\r3535\\r3030\\r2525\\r2020\\r4 7 81217222835790500100015002000250030003500\\rtest sentences sorted by their lengthtest sentences sorted by average word frequency rank  \\\n",
            "0                      LSTM  (34.8)\\rbaseline (33.3)                                                                                                                                                                                   \n",
            "1                                                NaN                                                                                                                                                                                   \n",
            "2                                                NaN                                                                                                                                                                                   \n",
            "3                                                NaN                                                                                                                                                                                   \n",
            "\n",
            "                      Unnamed: 0  Unnamed: 1  \n",
            "0                            NaN         NaN  \n",
            "1  LSTM  (34.8)\\rbaseline (33.3)         NaN  \n",
            "2                            NaN         NaN  \n",
            "3                            NaN         NaN  \n",
            "==================================================\n",
            "\n",
            "Table 4:\n",
            "  LSTM  (34.8)LSTM  (34.8)\\r40\\rbaseline (33.3)40\\rbaseline (33.3)\\r3535\\r3030\\r2525\\r2020\\r4 7 81217222835790500100015002000250030003500\\rtest sentences sorted by their lengthtest sentences sorted by average word frequency rank  \\\n",
            "0                      LSTM  (34.8)\\rbaseline (33.3)                                                                                                                                                                                   \n",
            "1                      LSTM  (34.8)\\rbaseline (33.3)                                                                                                                                                                                   \n",
            "2                                                NaN                                                                                                                                                                                   \n",
            "\n",
            "   Unnamed: 0  Unnamed: 1  \n",
            "0         NaN         NaN  \n",
            "1         NaN         NaN  \n",
            "2         NaN         NaN  \n",
            "==================================================\n",
            "\n",
            "Table 5:\n",
            "  LSTM  (34.8)LSTM  (34.8)\\r40\\rbaseline (33.3)40\\rbaseline (33.3)\\r3535\\r3030\\r2525\\r2020\\r4 7 81217222835790500100015002000250030003500\\rtest sentences sorted by their lengthtest sentences sorted by average word frequency rank  \\\n",
            "0                      LSTM  (34.8)\\rbaseline (33.3)                                                                                                                                                                                   \n",
            "1                      LSTM  (34.8)\\rbaseline (33.3)                                                                                                                                                                                   \n",
            "2                                                NaN                                                                                                                                                                                   \n",
            "3                                                NaN                                                                                                                                                                                   \n",
            "\n",
            "   Unnamed: 0                     Unnamed: 1                     Unnamed: 2  \\\n",
            "0         NaN                            NaN  LSTM  (34.8)\\rbaseline (33.3)   \n",
            "1         NaN  LSTM  (34.8)\\rbaseline (33.3)                            NaN   \n",
            "2         NaN                            NaN                            NaN   \n",
            "3         NaN                            NaN                            NaN   \n",
            "\n",
            "   Unnamed: 3  Unnamed: 4  \n",
            "0         NaN         NaN  \n",
            "1         NaN         NaN  \n",
            "2         NaN         NaN  \n",
            "3         NaN         NaN  \n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making Article Summarizer"
      ],
      "metadata": {
        "id": "O5YdiJMkc6Bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import re\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from textblob import TextBlob\n",
        "import string\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from statistics import mean\n",
        "from heapq import nlargest\n",
        "from wordcloud import WordCloud\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = punctuation + '\\n' + '—' + '“' + ',' + '”' + '‘' + '-' + '’'\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LuyNpb-QcfVi",
        "outputId": "331a51f6-4aeb-4b3a-8b90-dc44903cc4ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-1e444f5c5e06>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mpunctuation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpunctuation\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'—'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'“'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m','\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'”'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'‘'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'’'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "from dehyphen import FlairScorer\n",
        "\n",
        "scorer = FlairScorer(lang=\"de\")"
      ],
      "metadata": {
        "id": "w84JVEHyIu9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "A2D5wtzdKnCG",
        "outputId": "64691e1c-f6a2-487f-a45e-b38793a44f9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "sequence item 0: expected str instance, list found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-9796c2c78d98>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Convert the list to a single string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mcleaned_text_combined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Wrap the text to the specified width\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "HN4rtGjpYuwg",
        "outputId": "84c0fd36-de7c-4582-d062-d374fc65ebdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'module' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-e38f0cf44bd4>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Dehyphenate text for each page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdeyphenated_pages_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdehyphenate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_text\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpage_text\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_pages_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Now, dehyphenated_pages_text is a list containing dehyphenated text from each page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpage_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeyphenated_pages_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-e38f0cf44bd4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Dehyphenate text for each page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdeyphenated_pages_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdehyphenate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_text\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpage_text\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_pages_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Now, dehyphenated_pages_text is a list containing dehyphenated text from each page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpage_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeyphenated_pages_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-3c96d5af3c33>\u001b[0m in \u001b[0;36mdehyphenate_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdehyphenate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Apply dehyphenation to the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdehyphenated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdehyphen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdehyphenated_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_urls_and_emails(text):\n",
        "    # Remove URLs\n",
        "    text_without_urls = re.sub(r'http\\S+|www\\S+|\\S+\\.com\\S+|\\S+\\.in\\S+', '', text)\n",
        "\n",
        "    # Remove email addresses\n",
        "    text_without_emails = re.sub(r'\\S+@\\S+', '', text_without_urls)\n",
        "\n",
        "    return text_without_emails\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "output_text = remove_urls_and_emails(result_text)\n",
        "\n",
        "print(\"\\nText after removing URLs and emails:\")\n",
        "print(output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYPzxRqah2eA",
        "outputId": "118a3f11-0981-410b-ce6c-8464e9ceed95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Text after removing URLs and emails:\n",
            "v csCL  Dec \n",
            "\n",
            "V\n",
            "\n",
            "arxX\n",
            "\n",
            " \n",
            "\n",
            "Sequence to Sequence Learning\n",
            "with Neural Networks\n",
            "\n",
            "Ilya Sutskever Oriol Vinyals Quoc V Le\n",
            "Google Google Google\n",
            "ilyasugooglecom vinyalsgooglecom qvlgooglecom\n",
            "Abstract\n",
            "\n",
            "Deep Neural Networks DNNs are powerful models that have achieved excel\n",
            "lent performance on difficult learning tasks Although DNNs work well whenever\n",
            "large labeled training sets are available they cannot be used to map sequences to\n",
            "sequences In this paper we present a general endtoend approach to sequence\n",
            "learning that makes minimal assumptions on the sequence structure Our method\n",
            "uses a multilayered Long ShortTerm Memory LSTM to map the input sequence\n",
            "to a vector of a fixed dimensionality and then another deep LSTM to decode the\n",
            "target sequence from the vector Our main result is that on an English to French\n",
            "translation task from the WMT’  dataset the translations produced by the LSTM\n",
            "achieve a BLEU score of  on the entire test set where the LSTM’s BLEU\n",
            "score was penalized on outofvocabulary words Additionally the LSTM did not\n",
            "have difficulty on long sentences For comparison a phrasebased SMT system\n",
            "achieves a BLEU score of  on the same dataset When we used the LSTM\n",
            "to rerank the  hypotheses produced by the aforementioned SMT system its\n",
            "BLEU score increases to  which is close to the previous best result on this\n",
            "task The LSTM also learned sensible phrase and sentence representations that\n",
            "are sensitive to word order and are relatively invariant to the active and the pas\n",
            "sive voice Finally we found that reversing the order of the words in all source\n",
            "sentences but not target sentences improved the LSTM’s performance markedly\n",
            "because doing so introduced many short term dependencies between the source\n",
            "and the target sentence which made the optimization problem easier\n",
            "\n",
            " Introduction\n",
            "\n",
            "Deep Neural Networks DNNs are extremely powerful machine learning models that achieve ex\n",
            "cellent performance on difficult problems such as speech recognition   and visual object recog\n",
            "nition     DNNs are powerful because they can perform arbitrary parallel computation\n",
            "for a modest number of steps A surprising example of the power of DNNs is their ability to sort\n",
            "N WNbit numbers using only  hidden layers of quadratic size  So while neural networks are\n",
            "related to conventional statistical models they learn an intricate computation Furthermore large\n",
            "DNNs can be trained with supervised backpropagation whenever the labeled training set has enough\n",
            "information to specify the network’s parameters Thus if there exists a parameter setting of a large\n",
            "DNN that achieves good results for example because humans can solve the task very rapidly\n",
            "supervised backpropagation will find these parameters and solve the problem\n",
            "\n",
            "Despite their flexibility and power DNNs can only be applied to problems whose inputs and targets\n",
            "can be sensibly encoded with vectors of fixed dimensionality It is a significant limitation since\n",
            "many important problems are best expressed with sequences whose lengths are not known apriori\n",
            "For example speech recognition and machine translation are sequential problems Likewise ques\n",
            "tion answering can also be seen as mapping a sequence of words representing the question to a\n",
            "\fsequence of words representing the answer It is therefore clear that a domainindependent method\n",
            "that learns to map sequences to sequences would be useful\n",
            "\n",
            "Sequences pose a challenge for DNNs because they require that the dimensionality of the inputs and\n",
            "outputs is known and fixed In this paper we show that a straightforward application of the Long\n",
            "ShortTerm Memory LSTM architecture  can solve general sequence to sequence problems\n",
            "The idea is to use one LSTM to read the input sequence one timestep at a time to obtain large fixed\n",
            "dimensional vector representation and then to use another LSTM to extract the output sequence\n",
            "from that vector fig  The second LSTM is essentially a recurrent neural network language model\n",
            "   except that it is conditioned on the input sequence The LSTM’s ability to successfully\n",
            "learn on data with long range temporal dependencies makes it a natural choice for this application\n",
            "due to the considerable time lag between the inputs and their corresponding outputs fig \n",
            "\n",
            "There have been a number of related attempts to address the general sequence to sequence learning\n",
            "problem with neural networks Our approach is closely related to Kalchbrenner and Blunsom \n",
            "who were the first to map the entire input sentence to vector and is related to Cho et al  although\n",
            "the latter was used only for rescoring hypotheses produced by a phrasebased system Graves \n",
            "introduced a novel differentiable attention mechanism that allows neural networks to focus on dif\n",
            "ferent parts of their input and an elegant variant of this idea was successfully applied to machine\n",
            "translation by Bahdanau et al  The Connectionist Sequence Classification is another popular\n",
            "technique for mapping sequences to sequences with neural networks but it assumes a monotonic\n",
            "alignment between the inputs and the outputs \n",
            "\n",
            "W X Y Z EOS\n",
            "\n",
            "A B C EOS W X Y Z\n",
            "\n",
            "Figure  Our model reads an input sentence “ABC” and produces “WXYZ” as the output sentence The\n",
            "model stops making predictions after outputting the endofsentence token Note that the LSTM reads the\n",
            "input sentence in reverse because doing so introduces many short term dependencies in the data that make the\n",
            "optimization problem much easier\n",
            "\n",
            "The main result of this work is the following On the WMT’  English to French translation task\n",
            "we obtained a BLEU score of  by directly extracting translations from an ensemble of  deep\n",
            "LSTMs with M parameters and  dimensional state each using a simple lefttoright beam\n",
            "search decoder This is by far the best result achieved by direct translation with large neural net\n",
            "works For comparison the BLEU score of an SMT baseline on this dataset is   The \n",
            "BLEU score was achieved by an LSTM with a vocabulary of k words so the score was penalized\n",
            "whenever the reference translation contained a word not covered by these k This result shows\n",
            "that a relatively unoptimized smallvocabulary neural network architecture which has much room\n",
            "for improvement outperforms a phrasebased SMT system\n",
            "\n",
            "Finally we used the LSTM to rescore the publicly available best lists of the SMT baseline on\n",
            "the same task  By doing so we obtained a BLEU score of  which improves the baseline by\n",
            " BLEU points and is close to the previous best published result on this task which is  \n",
            "\n",
            "Surprisingly the LSTM did not suffer on very long sentences despite the recent experience of other\n",
            "researchers with related architectures  We were able to do well on long sentences because we\n",
            "reversed the order of words in the source sentence but not the target sentences in the training and test\n",
            "set By doing so we introduced many short term dependencies that made the optimization problem\n",
            "much simpler see sec  and  As a result SGD could learn LSTMs that had no trouble with\n",
            "long sentences The simple trick of reversing the words in the source sentence is one of the key\n",
            "technical contributions of this work\n",
            "\n",
            "A useful property of the LSTM is that it learns to map an input sentence of variable length into\n",
            "a fixeddimensional vector representation Given that translations tend to be paraphrases of the\n",
            "source sentences the translation objective encourages the LSTM to find sentence representations\n",
            "that capture their meaning as sentences with similar meanings are close to each other while different\n",
            "\fsentences meanings will be far A qualitative evaluation supports this claim showing that our model\n",
            "is aware of word order and is fairly invariant to the active and passive voice\n",
            "\n",
            " The model\n",
            "\n",
            "The Recurrent Neural Network RNN   is a natural generalization of feedforward neural\n",
            "networks to sequences Given a sequence of inputs  a standard RNN computes a\n",
            "sequence of outputs y yr by iterating the following equation\n",
            "\n",
            "h  sigm Wa  W he\n",
            "Ye  Whe\n",
            "\n",
            "The RNN can easily map sequences to sequences whenever the alignment between the inputs the\n",
            "outputs is known ahead of time However it is not clear how to apply an RNN to problems whose\n",
            "input and the output sequences have different lengths with complicated and nonmonotonic relation\n",
            "ships\n",
            "\n",
            "The simplest strategy for general sequence learning is to map the input sequence to a fixedsized\n",
            "vector using one RNN and then to map the vector to the target sequence with another RNN this\n",
            "approach has also been taken by Cho et al  While it could work in principle since the RNN is\n",
            "provided with all the relevant information it would be difficult to train the RNNs due to the resulting\n",
            "long term dependencies figure      However the Long ShortTerm Memory LSTM\n",
            " is known to learn problems with long range temporal dependencies so an LSTM may succeed\n",
            "in this setting\n",
            "\n",
            "The goal of the LSTM is to estimate the conditional probability pyy  where\n",
            "v is an input sequence and y yr’ is its corresponding output sequence whose length\n",
            "T’ may differ from ’ The LSTM computes this conditional probability by first obtaining the fixed\n",
            "dimensional representation v of the input sequence   given by the last hidden state of the\n",
            "LSTM and then computing the probability of yy with a standard LSTMLM formulation\n",
            "whose initial hidden state is set to the representation v of  \n",
            "\n",
            "T’\n",
            "Pyyreier   viyelos yi yea \n",
            "t\n",
            "\n",
            "In this equation each pyzv y Yz— distribution is represented with a softmax over all the\n",
            "words in the vocabulary We use the LSTM formulation from Graves  Note that we require that\n",
            "each sentence ends with a special endofsentence symbol “EOS” which enables the model to\n",
            "define a distribution over sequences of all possible lengths The overall scheme is outlined in figure\n",
            " where the shown LSTM computes the representation of “A” “B” “C” “EOS” and then uses\n",
            "this representation to compute the probability of “W” “X” “Y” “Z” “EOS”\n",
            "\n",
            "Our actual models differ from the above description in three important ways First we used two\n",
            "different LSTMs one for the input sequence and another for the output sequence because doing\n",
            "so increases the number model parameters at negligible computational cost and makes it natural to\n",
            "train the LSTM on multiple language pairs simultaneously  Second we found that deep LSTMs\n",
            "significantly outperformed shallow LSTMs so we chose an LSTM with four layers Third we found\n",
            "it extremely valuable to reverse the order of the words of the input sentence So for example instead\n",
            "of mapping the sentence a bc to the sentence a y the LSTM is asked to map c b a to a \n",
            "where a  ¥ is the translation of a b c This way a is in close proximity to a b is fairly close to \n",
            "and so on a fact that makes it easy for SGD to “establish communication” between the input and the\n",
            "output We found this simple data transformation to greatly improve the performance of the LSTM\n",
            "\n",
            " Experiments\n",
            "\n",
            "We applied our method to the WMT’ English to French MT task in two ways We used it to\n",
            "directly translate the input sentence without using a reference SMT system and we it to rescore the\n",
            "nbest lists of an SMT baseline We report the accuracy of these translation methods present sample\n",
            "translations and visualize the resulting sentence representation\n",
            "\f Dataset details\n",
            "\n",
            "We used the WMT’  English to French dataset We trained our models on a subset of M sen\n",
            "tences consisting of M French words and M English words which is a clean “selected”\n",
            "subset from  We chose this translation task and this specific training set subset because of the\n",
            "public availability of a tokenized training and test set together with best lists from the baseline\n",
            "SMT \n",
            "\n",
            "As typical neural language models rely on a vector representation for each word we used a fixed\n",
            "vocabulary for both languages We used  of the most frequent words for the source language\n",
            "and  of the most frequent words for the target language Every outofvocabulary word was\n",
            "replaced with a special “UNK” token\n",
            "\n",
            " Decoding and Rescoring\n",
            "\n",
            "The core of our experiments involved training a large deep LSTM on many sentence pairs We\n",
            "trained it by maximizing the log probability of a correct translation T’ given the source sentence S\n",
            "so the training objective is\n",
            "\n",
            "S S— logpTS\n",
            "\n",
            "TSES\n",
            "\n",
            "where S is the training set Once training is complete we produce translations by finding the most\n",
            "likely translation according to the LSTM\n",
            "\n",
            "A\n",
            "\n",
            "T  arg max pTS \n",
            "\n",
            "We search for the most likely translation using a simple lefttoright beam search decoder which\n",
            "maintains a small number B of partial hypotheses where a partial hypothesis is a prefix of some\n",
            "translation At each timestep we extend each partial hypothesis in the beam with every possible\n",
            "word in the vocabulary This greatly increases the number of the hypotheses so we discard all but\n",
            "the B most likely hypotheses according to the model’s log probability As soon as the “EOS”\n",
            "symbol is appended to a hypothesis it is removed from the beam and is added to the set of complete\n",
            "hypotheses While this decoder is approximate it is simple to implement Interestingly our system\n",
            "performs well even with a beam size of  and a beam of size  provides most of the benefits of beam\n",
            "search Table \n",
            "\n",
            "We also used the LSTM to rescore the best lists produced by the baseline system  To\n",
            "rescore an nbest list we computed the log probability of every hypothesis with our LSTM and took\n",
            "an even average with their score and the LSTM’s score\n",
            "\n",
            " Reversing the Source Sentences\n",
            "\n",
            "While the LSTM is capable of solving problems with long term dependencies we discovered that\n",
            "the LSTM learns much better when the source sentences are reversed the target sentences are not\n",
            "reversed By doing so the LSTM’s test perplexity dropped from  to  and the test BLEU\n",
            "scores of its decoded translations increased from  to \n",
            "\n",
            "While we do not have a complete explanation to this phenomenon we believe that it is caused by\n",
            "the introduction of many short term dependencies to the dataset Normally when we concatenate a\n",
            "source sentence with a target sentence each word in the source sentence is far from its corresponding\n",
            "word in the target sentence As a result the problem has a large “minimal time lag”  By\n",
            "reversing the words in the source sentence the average distance between corresponding words in\n",
            "the source and target language is unchanged However the first few words in the source language\n",
            "are now very close to the first few words in the target language so the problem’s minimal time lag is\n",
            "greatly reduced Thus backpropagation has an easier time “establishing communication” between\n",
            "the source sentence and the target sentence which in turn results in substantially improved overall\n",
            "performance\n",
            "\n",
            "Initially we believed that reversing the input sentences would only lead to more confident predic\n",
            "tions in the early parts of the target sentence and to less confident predictions in the later parts How\n",
            "ever LSTMs trained on reversed source sentences did much better on long sentences than LSTMs\n",
            "\ftrained on the raw source sentences see sec  which suggests that reversing the input sentences\n",
            "results in LSTMs with better memory utilization\n",
            "\n",
            " Training details\n",
            "\n",
            "We found that the LSTM models are fairly easy to train We used deep LSTMs with  layers\n",
            "with  cells at each layer and  dimensional word embeddings with an input vocabulary\n",
            "of  and an output vocabulary of  Thus the deep LSTM uses  real numbers to\n",
            "represent a sentence We found deep LSTMs to significantly outperform shallow LSTMs where\n",
            "each additional layer reduced perplexity by nearly  possibly due to their much larger hidden\n",
            "state We used a naive softmax over  words at each output The resulting LSTM has M\n",
            "parameters of which M are pure recurrent connections M for the “encoder” LSTM and M\n",
            "for the “decoder” LSTM The complete training details are given below\n",
            "\n",
            "e We initialized all of the LSTM’s parameters with the uniform distribution between \n",
            "and \n",
            "\n",
            "e We used stochastic gradient descent without momentum with a fixed learning rate of \n",
            "After  epochs we begun halving the learning rate every half epoch We trained our models\n",
            "for a total of  epochs\n",
            "\n",
            "e We used batches of  sequences for the gradient and divided it the size of the batch\n",
            "namely \n",
            "\n",
            "e Although LSTMs tend to not suffer from the vanishing gradient problem they can have\n",
            "exploding gradients Thus we enforced a hard constraint on the norm of the gradient \n",
            " by scaling it when its norm exceeded a threshold For each training batch we compute\n",
            "  g where g is the gradient divided by  If s   we set g  \n",
            "\n",
            "e Different sentences have different lengths Most sentences are short eg length \n",
            "but some sentences are long eg length   so a minibatch of  randomly chosen\n",
            "training sentences will have many short sentences and few long sentences and as a result\n",
            "much of the computation in the minibatch is wasted To address this problem we made sure\n",
            "that all sentences in a minibatch are roughly of the same length yielding a x speedup\n",
            "\n",
            " Parallelization\n",
            "\n",
            "A C implementation of deep LSTM with the configuration from the previous section on a sin\n",
            "gle GPU processes a speed of approximately  words per second This was too slow for our\n",
            "purposes so we parallelized our model using an GPU machine Each layer of the LSTM was\n",
            "executed on a different GPU and communicated its activations to the next GPU  layer as soon as\n",
            "they were computed Our models have  layers of LSTMs each of which resides on a separate\n",
            "GPU The remaining  GPUs were used to parallelize the softmax so each GPU was responsible\n",
            "for multiplying by a  x  matrix The resulting implementation achieved a speed of \n",
            "both English and French words per second with a minibatch size of  Training took about a ten\n",
            "days with this implementation\n",
            "\n",
            " Experimental Results\n",
            "\n",
            "We used the cased BLEU score  to evaluate the quality of our translations We computed our\n",
            "BLEU scores using multibleup on the tokenized predictions and ground truth This way\n",
            "of evaluating the BELU score is consistent with  and  and reproduces the  score of \n",
            "However if we evaluate the best WMT’  system  whose predictions can be downloaded from\n",
            "statmtorgmatrix in this manner we get  which is greater than the  reported by\n",
            "statmtorgmatrix\n",
            "\n",
            "The results are presented in tables  and  Our best results are obtained with an ensemble of LSTMs\n",
            "that differ in their random initializations and in the random order of minibatches While the decoded\n",
            "translations of the LSTM ensemble do not outperform the best WMT’  system it is the first time\n",
            "that a pure neural translation system outperforms a phrasebased SMT baseline on a large scale MT\n",
            "\n",
            "‘There several variants of the BLEU score and each variant is defined with a perl script\n",
            "\ftest BLEU score ntsti\n",
            "Bahdanau et al  \n",
            "Baseline System  \n",
            "\n",
            "Single forward LSTM beam size  \n",
            "\n",
            "Table  The performance of the LSTM on WMT’  English to French test set ntst Note that\n",
            "an ensemble of  LSTMs with a beam of size  is cheaper than of a single LSTM with a beam of\n",
            "size \n",
            "\n",
            " \n",
            "\n",
            "test BLEU score ntsti\n",
            "Baseline System  \n",
            "\n",
            "Cho et al  \n",
            "Best WMT result \n",
            "\n",
            "Rescoring the baseline best with a single forward LSTM \n",
            "Rescoring the baseline best with a single reversed LSTM \n",
            "Rescoring the baseline best with an ensemble of  reversed LSTMs\n",
            "\n",
            "Oracle Rescoring of the Baseline best lists\n",
            "\n",
            " \n",
            "\n",
            "Table  Methods that use neural networks together with an SMT system on the WMT’  English\n",
            "to French test set ntst\n",
            "\n",
            "task by a sizeable margin despite its inability to handle outofvocabulary words The LSTM is\n",
            "within  BLEU points of the best WMT’  result if it is used to rescore the best list of the\n",
            "baseline system\n",
            "\n",
            " Performance on long sentences\n",
            "\n",
            "We were surprised to discover that the LSTM did well on long sentences which is shown quantita\n",
            "tively in figure  Table  presents several examples of long sentences and their translations\n",
            "\n",
            " Model Analysis\n",
            "\n",
            "al O  was given a card by her in the garden\n",
            "\n",
            "F OMary admires John  O Inthe garden  she gave me a card\n",
            "O She gave me a card in the garden\n",
            " OMary is in love with John\n",
            "\n",
            "OMary respects John\n",
            "aL OJohn admires Mary\n",
            " O She was given a card by me in the garden\n",
            "\n",
            " OJohn is in love with Mar\n",
            "y © In the garden   gave her a card\n",
            "\n",
            "  OJohn respects Mary © I gave her a card in the garden\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "    L L L L J  L L L L L  J\n",
            "                 \n",
            "\n",
            "Figure  The figure shows a dimensional PCA projection of the LSTM hidden states that are obtained\n",
            "after processing the phrases in the figures The phrases are clustered by meaning which in these examples is\n",
            "primarily a function of word order which would be difficult to capture with a bagofwords model Notice that\n",
            "both clusters have similar internal structure\n",
            "\n",
            "One of the attractive features of our model is its ability to turn a sequence of words into a vector\n",
            "of fixed dimensionality Figure  visualizes some of the learned representations The figure clearly\n",
            "shows that the representations are sensitive to the order of words while being fairly insensitive to the\n",
            "\fOur model  Ulrich UNK  membre du conseil d’ administration du constructeur automobile Audi \n",
            "affirme qu’ ils’ agit d’ une pratique courante depuis des années pour que les teléphones\n",
            "portables puissent étre collectés avant les reunions du conseil d’ administration afin qu’ ils\n",
            "ne soient pas utilises comme appareils d’ écoute a distance \n",
            "\n",
            "Truth Ulrich Hackenberg  membre du conseil d’ administration du constructeur automobile Audi \n",
            "déclare que la collecte des téléphones portables avant les reunions du conseil  afin qu’ ils\n",
            "ne puissent pas étre utilisés comme appareils d’ écoute a distance  est une pratique courante\n",
            "depuis des années \n",
            "\n",
            "Our model  “ Les telephones cellulaires  qui sont vraiment une question  non seulement parce qu’ ils\n",
            "pourraient potentiellement causer des interférences avec les appareils de navigation  mais\n",
            "nous savons  selon la FCC  qu’ ils pourraient interférer avec les tours de téléphone cellulaire\n",
            "lorsqu’ ils sont dans I’ air”  dit UNK \n",
            "\n",
            "Truth “ Les telephones portables sont véritablement un probleme  non seulement parce qu’ ils\n",
            "pourraient éventuellement créer des interférences avec les instruments de navigation  mais\n",
            "parce que nous savons  d’ apres la FCC  qu’ ils pourraient perturber les antennesrelais de\n",
            "telephonie mobile s’ ils sont utilisés a bord ”  a declaré Rosenker \n",
            "\n",
            "Our model  Avec la crémation  il y a un “ sentiment de violence contre le corps d’ un étre cher ” \n",
            "\n",
            "qui sera “ réduit a une pile de cendres ” en trés peu de temps au lieu d’ un processus de\n",
            "decomposition “ qui accompagnera les étapes du deuil ” \n",
            "\n",
            "Truth Il y a avec la cremation  “ une violence faite au corps aimé ” \n",
            "\n",
            "a qui va étre “ reduit a un tas de cendres ” en trés peu de temps  et non aprés un processus de\n",
            "\n",
            "decomposition  qui “ accompagnerait les phases du deuil ” \n",
            "\n",
            " \n",
            "\n",
            "Table  A few examples of long translations produced by the LSTM alongside the ground truth\n",
            "translations The reader can verify that the translations are sensible using Google translate\n",
            "\n",
            "   \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "— LSTM  — LSTM \n",
            " e—e baseline   e—e baseline \n",
            " \n",
            "Y Y\n",
            "° °\n",
            "U U\n",
            "wn wn\n",
            "D D\n",
            "   \n",
            "a a\n",
            " \n",
            "              \n",
            "              \n",
            "test sentences sorted by their length test sentences sorted by average word frequency rank\n",
            "\n",
            "Figure  The left plot shows the performance of our system as a function of sentence length where the\n",
            "xaxis corresponds to the test sentences sorted by their length and is marked by the actual sequence lengths\n",
            "There is no degradation on sentences with less than  words there is only a minor degradation on the longest\n",
            "sentences The right plot shows the LSTM’s performance on sentences with progressively more rare words\n",
            "where the xaxis corresponds to the test sentences sorted by their “average word frequency rank”\n",
            "\n",
            "replacement of an active voice with a passive voice The twodimensional projections are obtained\n",
            "using PCA\n",
            "\n",
            " Related work\n",
            "\n",
            "There is a large body of work on applications of neural networks to machine translation So far\n",
            "the simplest and most effective way of applying an RNNLanguage Model RNNLM  or a\n",
            "\fFeedforward Neural Network Language Model NNLM  to an MT task is by rescoring the n\n",
            "best lists of a strong MT baseline  which reliably improves translation quality\n",
            "\n",
            "More recently researchers have begun to look into ways of including information about the source\n",
            "language into the NNLM Examples of this work include Auli et al  who combine an NNLM\n",
            "with a topic model of the input sentence which improves rescoring performance Devlin et al \n",
            "followed a similar approach but they incorporated their NNLM into the decoder of an MT system\n",
            "and used the decoder’s alignment information to provide the NNLM with the most useful words in\n",
            "the input sentence Their approach was highly successful and it achieved large improvements over\n",
            "their baseline\n",
            "\n",
            "Our work is closely related to Kalchbrenner and Blunsom  who were the first to map the input\n",
            "sentence into a vector and then back to a sentence although they map sentences to vectors using\n",
            "convolutional neural networks which lose the ordering of the words Similarly to this work Cho et\n",
            "al  used an LSTMlike RNN architecture to map sentences into vectors and back although their\n",
            "primary focus was on integrating their neural network into an SMT system Bahdanau et al  also\n",
            "attempted direct translations with a neural network that used an attention mechanism to overcome\n",
            "the poor performance on long sentences experienced by Cho et al  and achieved encouraging\n",
            "results Likewise PougetAbadie et al  attempted to address the memory problem of Cho et\n",
            "al  by translating pieces of the source sentence in way that produces smooth translations which\n",
            "is similar to a phrasebased approach We suspect that they could achieve similar improvements by\n",
            "simply training their networks on reversed source sentences\n",
            "\n",
            "Endtoend training is also the focus of Hermann et al  whose model represents the inputs and\n",
            "outputs by feedforward networks and map them to similar points in space However their approach\n",
            "cannot generate translations directly to get a translation they need to do a look up for closest vector\n",
            "in the precomputed database of sentences or to rescore a sentence\n",
            "\n",
            " Conclusion\n",
            "\n",
            "In this work we showed that a large deep LSTM that has a limited vocabulary and that makes\n",
            "almost no assumption about problem structure can outperform a standard SMTbased system whose\n",
            "vocabulary is unlimited on a largescale MT task The success of our simple LSTMbased approach\n",
            "on MT suggests that it should do well on many other sequence learning problems provided they\n",
            "have enough training data\n",
            "\n",
            "We were surprised by the extent of the improvement obtained by reversing the words in the source\n",
            "sentences We conclude that it is important to find a problem encoding that has the greatest number\n",
            "of short term dependencies as they make the learning problem much simpler In particular while\n",
            "we were unable to train a standard RNN on the nonreversed translation problem shown in fig \n",
            "we believe that a standard RNN should be easily trainable when the source sentences are reversed\n",
            "although we did not verify it experimentally\n",
            "\n",
            "We were also surprised by the ability of the LSTM to correctly translate very long sentences We\n",
            "were initially convinced that the LSTM would fail on long sentences due to its limited memory\n",
            "and other researchers reported poor performance on long sentences with a model similar to ours\n",
            "   And yet LSTMs trained on the reversed dataset had little difficulty translating long\n",
            "sentences\n",
            "\n",
            "Most importantly we demonstrated that a simple straightforward and a relatively unoptimized ap\n",
            "proach can outperform an SMT system so further work will likely lead to even greater translation\n",
            "accuracies These results suggest that our approach will likely do well on other challenging sequence\n",
            "to sequence problems\n",
            "\n",
            " Acknowledgments\n",
            "\n",
            "We thank Samy Bengio Jeff Dean Matthieu Devin Geoffrey Hinton Nal Kalchbrenner Thang Luong Wolf\n",
            "gang Macherey Rajat Monga Vincent Vanhoucke Peng Xu Wojciech Zaremba and the Google Brain team\n",
            "for useful comments and discussions\n",
            "\fReferences\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "M Auli M Galley C Quirk and G Zweig Joint language and translation modeling with recurrent\n",
            "neural networks In EMNLP \n",
            "\n",
            "D Bahdanau K Cho and Y Bengio Neural machine translation by jointly learning to align and translate\n",
            "arXiv preprint arXiv  \n",
            "\n",
            "Y Bengio R Ducharme P Vincent and C Jauvin A neural probabilistic language model In Journal of\n",
            "Machine Learning Research pages  \n",
            "\n",
            "Y Bengio P Simard and P Frasconi Learning longterm dependencies with gradient descent is difficult\n",
            "IEEE Transactions on Neural Networks — \n",
            "\n",
            "K Cho B Merrienboer C Gulcehre F Bougares H Schwenk and Y Bengio Learning phrase represen\n",
            "tations using RNN encoderdecoder for statistical machine translation In Arxiv preprint arXiv \n",
            "\n",
            "\n",
            "D Ciresan U Meier and J Schmidhuber Multicolumn deep neural networks for image classification\n",
            "In CVPR \n",
            "\n",
            "G E Dahl D Yu L Deng and A Acero Contextdependent pretrained deep neural networks for large\n",
            "vocabulary speech recognition IEEE Transactions on Audio Speech and Language Processing  Special\n",
            "Issue on Deep Learning for Speech and Language Processing \n",
            "\n",
            "J Devlin R Zbib Z Huang T Lamar R Schwartz and J Makhoul Fast and robust neural network\n",
            "joint models for statistical machine translation In ACL \n",
            "\n",
            "Nadir Durrani Barry Haddow Philipp Koehn and Kenneth Heafield Edinburgh’s phrasebased machine\n",
            "translation systems for wmt In WMT \n",
            "\n",
            "A Graves Generating sequences with recurrent neural networks In Arxiv preprint arXiv \n",
            "\n",
            "\n",
            "A Graves S Fernandez F Gomez and J Schmidhuber Connectionist temporal classification labelling\n",
            "unsegmented sequence data with recurrent neural networks In ICML \n",
            "\n",
            "K M Hermann and P Blunsom Multilingual distributed representations without word alignment In\n",
            "ICLR \n",
            "\n",
            "G Hinton L Deng D Yu G Dahl A Mohamed N Jaitly A Senior V Vanhoucke P Nguyen\n",
            "T Sainath and B Kingsbury Deep neural networks for acoustic modeling in speech recognition IEEE\n",
            "Signal Processing Magazine \n",
            "\n",
            "S Hochreiter Untersuchungen zu dynamischen neuronalen netzen Master’s thesis Institut fur Infor\n",
            "matik Technische Universitat Munchen \n",
            "\n",
            "S Hochreiter Y Bengio P Frasconi and J Schmidhuber Gradient flow in recurrent nets the difficulty\n",
            "of learning longterm dependencies \n",
            "\n",
            "S Hochreiter and J Schmidhuber Long shortterm memory Neural Computation \n",
            "\n",
            "S Hochreiter and J Schmidhuber LSTM can solve hard long time lag problems \n",
            "\n",
            "N Kalchbrenner and P Blunsom Recurrent continuous translation models In EMNLP \n",
            "\n",
            "A Krizhevsky I Sutskever and G E Hinton ImageNet classification with deep convolutional neural\n",
            "networks In NIPS \n",
            "\n",
            "QV Le MA Ranzato R Monga M Devin K Chen GS Corrado J Dean and AY Ng Building\n",
            "highlevel features using large scale unsupervised learning In ICML \n",
            "\n",
            "Y LeCun L Bottou Y Bengio and P Haffner Gradientbased learning applied to document recognition\n",
            "Proceedings of the IEEE \n",
            "\n",
            "T Mikolov Statistical Language Models based on Neural Networks PhD thesis Brno University of\n",
            "Technology \n",
            "\n",
            "T Mikolov M Karafiat L Burget J Cernocky and S Khudanpur Recurrent neural network based\n",
            "language model In INTERSPEECH pages  \n",
            "\n",
            "K Papineni S Roukos T Ward and W J Zhu BLEU a method for automatic evaluation of machine\n",
            "translation In ACL \n",
            "\n",
            "R Pascanu T Mikolov and Y Bengio On the difficulty of training recurrent neural networks arXiv\n",
            "preprint arXiv \n",
            "\n",
            "J PougetAbadie D Bahdanau B van Merrienboer K Cho and Y Bengio Overcoming the\n",
            "curse of sentence length for neural machine translation using automatic segmentation arXiv preprint\n",
            "arXiv \n",
            "\n",
            "A Razborov On small depth threshold circuits In Proc rd Scandinavian Workshop on Algorithm\n",
            "Theory \n",
            "\n",
            "D Rumelhart G E Hinton and R J Williams Learning representations by backpropagating errors\n",
            "Nature — \n",
            "\n",
            "H Schwenk University le mans \n",
            "jointpaper  Online accessed September\n",
            "\n",
            "M Sundermeyer R Schluter and H Ney LSTM neural networks for language modeling In INTER\n",
            "SPEECH \n",
            "\n",
            "P Werbos Backpropagation through time what it does and how to do it Proceedings of IEEE \n",
            "\f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing Unwanted White Space, New Lines and Tabs\n",
        "\" \".join(result_text.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "W30Pi1YpcJBT",
        "outputId": "86c4cbdc-d94f-4a40-b726-0748838572be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'v csCL Dec V arxX Sequence to Sequence Learning with Neural Networks Ilya Sutskever Oriol Vinyals Quoc V Le Google Google Google ilyasugooglecom vinyalsgooglecom qvlgooglecom Abstract Deep Neural Networks DNNs are powerful models that have achieved excel lent performance on difficult learning tasks Although DNNs work well whenever large labeled training sets are available they cannot be used to map sequences to sequences In this paper we present a general endtoend approach to sequence learning that makes minimal assumptions on the sequence structure Our method uses a multilayered Long ShortTerm Memory LSTM to map the input sequence to a vector of a fixed dimensionality and then another deep LSTM to decode the target sequence from the vector Our main result is that on an English to French translation task from the WMT’ dataset the translations produced by the LSTM achieve a BLEU score of on the entire test set where the LSTM’s BLEU score was penalized on outofvocabulary words Additionally the LSTM did not have difficulty on long sentences For comparison a phrasebased SMT system achieves a BLEU score of on the same dataset When we used the LSTM to rerank the hypotheses produced by the aforementioned SMT system its BLEU score increases to which is close to the previous best result on this task The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the pas sive voice Finally we found that reversing the order of the words in all source sentences but not target sentences improved the LSTM’s performance markedly because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier Introduction Deep Neural Networks DNNs are extremely powerful machine learning models that achieve ex cellent performance on difficult problems such as speech recognition and visual object recog nition DNNs are powerful because they can perform arbitrary parallel computation for a modest number of steps A surprising example of the power of DNNs is their ability to sort N WNbit numbers using only hidden layers of quadratic size So while neural networks are related to conventional statistical models they learn an intricate computation Furthermore large DNNs can be trained with supervised backpropagation whenever the labeled training set has enough information to specify the network’s parameters Thus if there exists a parameter setting of a large DNN that achieves good results for example because humans can solve the task very rapidly supervised backpropagation will find these parameters and solve the problem Despite their flexibility and power DNNs can only be applied to problems whose inputs and targets can be sensibly encoded with vectors of fixed dimensionality It is a significant limitation since many important problems are best expressed with sequences whose lengths are not known apriori For example speech recognition and machine translation are sequential problems Likewise ques tion answering can also be seen as mapping a sequence of words representing the question to a sequence of words representing the answer It is therefore clear that a domainindependent method that learns to map sequences to sequences would be useful Sequences pose a challenge for DNNs because they require that the dimensionality of the inputs and outputs is known and fixed In this paper we show that a straightforward application of the Long ShortTerm Memory LSTM architecture can solve general sequence to sequence problems The idea is to use one LSTM to read the input sequence one timestep at a time to obtain large fixed dimensional vector representation and then to use another LSTM to extract the output sequence from that vector fig The second LSTM is essentially a recurrent neural network language model except that it is conditioned on the input sequence The LSTM’s ability to successfully learn on data with long range temporal dependencies makes it a natural choice for this application due to the considerable time lag between the inputs and their corresponding outputs fig There have been a number of related attempts to address the general sequence to sequence learning problem with neural networks Our approach is closely related to Kalchbrenner and Blunsom who were the first to map the entire input sentence to vector and is related to Cho et al although the latter was used only for rescoring hypotheses produced by a phrasebased system Graves introduced a novel differentiable attention mechanism that allows neural networks to focus on dif ferent parts of their input and an elegant variant of this idea was successfully applied to machine translation by Bahdanau et al The Connectionist Sequence Classification is another popular technique for mapping sequences to sequences with neural networks but it assumes a monotonic alignment between the inputs and the outputs W X Y Z EOS A B C EOS W X Y Z Figure Our model reads an input sentence “ABC” and produces “WXYZ” as the output sentence The model stops making predictions after outputting the endofsentence token Note that the LSTM reads the input sentence in reverse because doing so introduces many short term dependencies in the data that make the optimization problem much easier The main result of this work is the following On the WMT’ English to French translation task we obtained a BLEU score of by directly extracting translations from an ensemble of deep LSTMs with M parameters and dimensional state each using a simple lefttoright beam search decoder This is by far the best result achieved by direct translation with large neural net works For comparison the BLEU score of an SMT baseline on this dataset is The BLEU score was achieved by an LSTM with a vocabulary of k words so the score was penalized whenever the reference translation contained a word not covered by these k This result shows that a relatively unoptimized smallvocabulary neural network architecture which has much room for improvement outperforms a phrasebased SMT system Finally we used the LSTM to rescore the publicly available best lists of the SMT baseline on the same task By doing so we obtained a BLEU score of which improves the baseline by BLEU points and is close to the previous best published result on this task which is Surprisingly the LSTM did not suffer on very long sentences despite the recent experience of other researchers with related architectures We were able to do well on long sentences because we reversed the order of words in the source sentence but not the target sentences in the training and test set By doing so we introduced many short term dependencies that made the optimization problem much simpler see sec and As a result SGD could learn LSTMs that had no trouble with long sentences The simple trick of reversing the words in the source sentence is one of the key technical contributions of this work A useful property of the LSTM is that it learns to map an input sentence of variable length into a fixeddimensional vector representation Given that translations tend to be paraphrases of the source sentences the translation objective encourages the LSTM to find sentence representations that capture their meaning as sentences with similar meanings are close to each other while different sentences meanings will be far A qualitative evaluation supports this claim showing that our model is aware of word order and is fairly invariant to the active and passive voice The model The Recurrent Neural Network RNN is a natural generalization of feedforward neural networks to sequences Given a sequence of inputs a standard RNN computes a sequence of outputs y yr by iterating the following equation h sigm Wa W he Ye Whe The RNN can easily map sequences to sequences whenever the alignment between the inputs the outputs is known ahead of time However it is not clear how to apply an RNN to problems whose input and the output sequences have different lengths with complicated and nonmonotonic relation ships The simplest strategy for general sequence learning is to map the input sequence to a fixedsized vector using one RNN and then to map the vector to the target sequence with another RNN this approach has also been taken by Cho et al While it could work in principle since the RNN is provided with all the relevant information it would be difficult to train the RNNs due to the resulting long term dependencies figure However the Long ShortTerm Memory LSTM is known to learn problems with long range temporal dependencies so an LSTM may succeed in this setting The goal of the LSTM is to estimate the conditional probability pyy where v is an input sequence and y yr’ is its corresponding output sequence whose length T’ may differ from ’ The LSTM computes this conditional probability by first obtaining the fixed dimensional representation v of the input sequence given by the last hidden state of the LSTM and then computing the probability of yy with a standard LSTMLM formulation whose initial hidden state is set to the representation v of T’ Pyyreier viyelos yi yea t In this equation each pyzv y Yz— distribution is represented with a softmax over all the words in the vocabulary We use the LSTM formulation from Graves Note that we require that each sentence ends with a special endofsentence symbol “EOS” which enables the model to define a distribution over sequences of all possible lengths The overall scheme is outlined in figure where the shown LSTM computes the representation of “A” “B” “C” “EOS” and then uses this representation to compute the probability of “W” “X” “Y” “Z” “EOS” Our actual models differ from the above description in three important ways First we used two different LSTMs one for the input sequence and another for the output sequence because doing so increases the number model parameters at negligible computational cost and makes it natural to train the LSTM on multiple language pairs simultaneously Second we found that deep LSTMs significantly outperformed shallow LSTMs so we chose an LSTM with four layers Third we found it extremely valuable to reverse the order of the words of the input sentence So for example instead of mapping the sentence a bc to the sentence a y the LSTM is asked to map c b a to a where a ¥ is the translation of a b c This way a is in close proximity to a b is fairly close to and so on a fact that makes it easy for SGD to “establish communication” between the input and the output We found this simple data transformation to greatly improve the performance of the LSTM Experiments We applied our method to the WMT’ English to French MT task in two ways We used it to directly translate the input sentence without using a reference SMT system and we it to rescore the nbest lists of an SMT baseline We report the accuracy of these translation methods present sample translations and visualize the resulting sentence representation Dataset details We used the WMT’ English to French dataset We trained our models on a subset of M sen tences consisting of M French words and M English words which is a clean “selected” subset from We chose this translation task and this specific training set subset because of the public availability of a tokenized training and test set together with best lists from the baseline SMT As typical neural language models rely on a vector representation for each word we used a fixed vocabulary for both languages We used of the most frequent words for the source language and of the most frequent words for the target language Every outofvocabulary word was replaced with a special “UNK” token Decoding and Rescoring The core of our experiments involved training a large deep LSTM on many sentence pairs We trained it by maximizing the log probability of a correct translation T’ given the source sentence S so the training objective is S S— logpTS TSES where S is the training set Once training is complete we produce translations by finding the most likely translation according to the LSTM A T arg max pTS We search for the most likely translation using a simple lefttoright beam search decoder which maintains a small number B of partial hypotheses where a partial hypothesis is a prefix of some translation At each timestep we extend each partial hypothesis in the beam with every possible word in the vocabulary This greatly increases the number of the hypotheses so we discard all but the B most likely hypotheses according to the model’s log probability As soon as the “EOS” symbol is appended to a hypothesis it is removed from the beam and is added to the set of complete hypotheses While this decoder is approximate it is simple to implement Interestingly our system performs well even with a beam size of and a beam of size provides most of the benefits of beam search Table We also used the LSTM to rescore the best lists produced by the baseline system To rescore an nbest list we computed the log probability of every hypothesis with our LSTM and took an even average with their score and the LSTM’s score Reversing the Source Sentences While the LSTM is capable of solving problems with long term dependencies we discovered that the LSTM learns much better when the source sentences are reversed the target sentences are not reversed By doing so the LSTM’s test perplexity dropped from to and the test BLEU scores of its decoded translations increased from to While we do not have a complete explanation to this phenomenon we believe that it is caused by the introduction of many short term dependencies to the dataset Normally when we concatenate a source sentence with a target sentence each word in the source sentence is far from its corresponding word in the target sentence As a result the problem has a large “minimal time lag” By reversing the words in the source sentence the average distance between corresponding words in the source and target language is unchanged However the first few words in the source language are now very close to the first few words in the target language so the problem’s minimal time lag is greatly reduced Thus backpropagation has an easier time “establishing communication” between the source sentence and the target sentence which in turn results in substantially improved overall performance Initially we believed that reversing the input sentences would only lead to more confident predic tions in the early parts of the target sentence and to less confident predictions in the later parts How ever LSTMs trained on reversed source sentences did much better on long sentences than LSTMs trained on the raw source sentences see sec which suggests that reversing the input sentences results in LSTMs with better memory utilization Training details We found that the LSTM models are fairly easy to train We used deep LSTMs with layers with cells at each layer and dimensional word embeddings with an input vocabulary of and an output vocabulary of Thus the deep LSTM uses real numbers to represent a sentence We found deep LSTMs to significantly outperform shallow LSTMs where each additional layer reduced perplexity by nearly possibly due to their much larger hidden state We used a naive softmax over words at each output The resulting LSTM has M parameters of which M are pure recurrent connections M for the “encoder” LSTM and M for the “decoder” LSTM The complete training details are given below e We initialized all of the LSTM’s parameters with the uniform distribution between and e We used stochastic gradient descent without momentum with a fixed learning rate of After epochs we begun halving the learning rate every half epoch We trained our models for a total of epochs e We used batches of sequences for the gradient and divided it the size of the batch namely e Although LSTMs tend to not suffer from the vanishing gradient problem they can have exploding gradients Thus we enforced a hard constraint on the norm of the gradient by scaling it when its norm exceeded a threshold For each training batch we compute g where g is the gradient divided by If s we set g e Different sentences have different lengths Most sentences are short eg length but some sentences are long eg length so a minibatch of randomly chosen training sentences will have many short sentences and few long sentences and as a result much of the computation in the minibatch is wasted To address this problem we made sure that all sentences in a minibatch are roughly of the same length yielding a x speedup Parallelization A C implementation of deep LSTM with the configuration from the previous section on a sin gle GPU processes a speed of approximately words per second This was too slow for our purposes so we parallelized our model using an GPU machine Each layer of the LSTM was executed on a different GPU and communicated its activations to the next GPU layer as soon as they were computed Our models have layers of LSTMs each of which resides on a separate GPU The remaining GPUs were used to parallelize the softmax so each GPU was responsible for multiplying by a x matrix The resulting implementation achieved a speed of both English and French words per second with a minibatch size of Training took about a ten days with this implementation Experimental Results We used the cased BLEU score to evaluate the quality of our translations We computed our BLEU scores using multibleup on the tokenized predictions and ground truth This way of evaluating the BELU score is consistent with and and reproduces the score of However if we evaluate the best WMT’ system whose predictions can be downloaded from statmtorgmatrix in this manner we get which is greater than the reported by statmtorgmatrix The results are presented in tables and Our best results are obtained with an ensemble of LSTMs that differ in their random initializations and in the random order of minibatches While the decoded translations of the LSTM ensemble do not outperform the best WMT’ system it is the first time that a pure neural translation system outperforms a phrasebased SMT baseline on a large scale MT ‘There several variants of the BLEU score and each variant is defined with a perl script test BLEU score ntsti Bahdanau et al Baseline System Single forward LSTM beam size Table The performance of the LSTM on WMT’ English to French test set ntst Note that an ensemble of LSTMs with a beam of size is cheaper than of a single LSTM with a beam of size test BLEU score ntsti Baseline System Cho et al Best WMT result Rescoring the baseline best with a single forward LSTM Rescoring the baseline best with a single reversed LSTM Rescoring the baseline best with an ensemble of reversed LSTMs Oracle Rescoring of the Baseline best lists Table Methods that use neural networks together with an SMT system on the WMT’ English to French test set ntst task by a sizeable margin despite its inability to handle outofvocabulary words The LSTM is within BLEU points of the best WMT’ result if it is used to rescore the best list of the baseline system Performance on long sentences We were surprised to discover that the LSTM did well on long sentences which is shown quantita tively in figure Table presents several examples of long sentences and their translations Model Analysis al O was given a card by her in the garden F OMary admires John O Inthe garden she gave me a card O She gave me a card in the garden OMary is in love with John OMary respects John aL OJohn admires Mary O She was given a card by me in the garden OJohn is in love with Mar y © In the garden gave her a card OJohn respects Mary © I gave her a card in the garden L L L L J L L L L L J Figure The figure shows a dimensional PCA projection of the LSTM hidden states that are obtained after processing the phrases in the figures The phrases are clustered by meaning which in these examples is primarily a function of word order which would be difficult to capture with a bagofwords model Notice that both clusters have similar internal structure One of the attractive features of our model is its ability to turn a sequence of words into a vector of fixed dimensionality Figure visualizes some of the learned representations The figure clearly shows that the representations are sensitive to the order of words while being fairly insensitive to the Our model Ulrich UNK membre du conseil d’ administration du constructeur automobile Audi affirme qu’ ils’ agit d’ une pratique courante depuis des années pour que les teléphones portables puissent étre collectés avant les reunions du conseil d’ administration afin qu’ ils ne soient pas utilises comme appareils d’ écoute a distance Truth Ulrich Hackenberg membre du conseil d’ administration du constructeur automobile Audi déclare que la collecte des téléphones portables avant les reunions du conseil afin qu’ ils ne puissent pas étre utilisés comme appareils d’ écoute a distance est une pratique courante depuis des années Our model “ Les telephones cellulaires qui sont vraiment une question non seulement parce qu’ ils pourraient potentiellement causer des interférences avec les appareils de navigation mais nous savons selon la FCC qu’ ils pourraient interférer avec les tours de téléphone cellulaire lorsqu’ ils sont dans I’ air” dit UNK Truth “ Les telephones portables sont véritablement un probleme non seulement parce qu’ ils pourraient éventuellement créer des interférences avec les instruments de navigation mais parce que nous savons d’ apres la FCC qu’ ils pourraient perturber les antennesrelais de telephonie mobile s’ ils sont utilisés a bord ” a declaré Rosenker Our model Avec la crémation il y a un “ sentiment de violence contre le corps d’ un étre cher ” qui sera “ réduit a une pile de cendres ” en trés peu de temps au lieu d’ un processus de decomposition “ qui accompagnera les étapes du deuil ” Truth Il y a avec la cremation “ une violence faite au corps aimé ” a qui va étre “ reduit a un tas de cendres ” en trés peu de temps et non aprés un processus de decomposition qui “ accompagnerait les phases du deuil ” Table A few examples of long translations produced by the LSTM alongside the ground truth translations The reader can verify that the translations are sensible using Google translate — LSTM — LSTM e—e baseline e—e baseline Y Y ° ° U U wn wn D D a a test sentences sorted by their length test sentences sorted by average word frequency rank Figure The left plot shows the performance of our system as a function of sentence length where the xaxis corresponds to the test sentences sorted by their length and is marked by the actual sequence lengths There is no degradation on sentences with less than words there is only a minor degradation on the longest sentences The right plot shows the LSTM’s performance on sentences with progressively more rare words where the xaxis corresponds to the test sentences sorted by their “average word frequency rank” replacement of an active voice with a passive voice The twodimensional projections are obtained using PCA Related work There is a large body of work on applications of neural networks to machine translation So far the simplest and most effective way of applying an RNNLanguage Model RNNLM or a Feedforward Neural Network Language Model NNLM to an MT task is by rescoring the n best lists of a strong MT baseline which reliably improves translation quality More recently researchers have begun to look into ways of including information about the source language into the NNLM Examples of this work include Auli et al who combine an NNLM with a topic model of the input sentence which improves rescoring performance Devlin et al followed a similar approach but they incorporated their NNLM into the decoder of an MT system and used the decoder’s alignment information to provide the NNLM with the most useful words in the input sentence Their approach was highly successful and it achieved large improvements over their baseline Our work is closely related to Kalchbrenner and Blunsom who were the first to map the input sentence into a vector and then back to a sentence although they map sentences to vectors using convolutional neural networks which lose the ordering of the words Similarly to this work Cho et al used an LSTMlike RNN architecture to map sentences into vectors and back although their primary focus was on integrating their neural network into an SMT system Bahdanau et al also attempted direct translations with a neural network that used an attention mechanism to overcome the poor performance on long sentences experienced by Cho et al and achieved encouraging results Likewise PougetAbadie et al attempted to address the memory problem of Cho et al by translating pieces of the source sentence in way that produces smooth translations which is similar to a phrasebased approach We suspect that they could achieve similar improvements by simply training their networks on reversed source sentences Endtoend training is also the focus of Hermann et al whose model represents the inputs and outputs by feedforward networks and map them to similar points in space However their approach cannot generate translations directly to get a translation they need to do a look up for closest vector in the precomputed database of sentences or to rescore a sentence Conclusion In this work we showed that a large deep LSTM that has a limited vocabulary and that makes almost no assumption about problem structure can outperform a standard SMTbased system whose vocabulary is unlimited on a largescale MT task The success of our simple LSTMbased approach on MT suggests that it should do well on many other sequence learning problems provided they have enough training data We were surprised by the extent of the improvement obtained by reversing the words in the source sentences We conclude that it is important to find a problem encoding that has the greatest number of short term dependencies as they make the learning problem much simpler In particular while we were unable to train a standard RNN on the nonreversed translation problem shown in fig we believe that a standard RNN should be easily trainable when the source sentences are reversed although we did not verify it experimentally We were also surprised by the ability of the LSTM to correctly translate very long sentences We were initially convinced that the LSTM would fail on long sentences due to its limited memory and other researchers reported poor performance on long sentences with a model similar to ours And yet LSTMs trained on the reversed dataset had little difficulty translating long sentences Most importantly we demonstrated that a simple straightforward and a relatively unoptimized ap proach can outperform an SMT system so further work will likely lead to even greater translation accuracies These results suggest that our approach will likely do well on other challenging sequence to sequence problems Acknowledgments We thank Samy Bengio Jeff Dean Matthieu Devin Geoffrey Hinton Nal Kalchbrenner Thang Luong Wolf gang Macherey Rajat Monga Vincent Vanhoucke Peng Xu Wojciech Zaremba and the Google Brain team for useful comments and discussions References M Auli M Galley C Quirk and G Zweig Joint language and translation modeling with recurrent neural networks In EMNLP D Bahdanau K Cho and Y Bengio Neural machine translation by jointly learning to align and translate arXiv preprint arXiv Y Bengio R Ducharme P Vincent and C Jauvin A neural probabilistic language model In Journal of Machine Learning Research pages Y Bengio P Simard and P Frasconi Learning longterm dependencies with gradient descent is difficult IEEE Transactions on Neural Networks — K Cho B Merrienboer C Gulcehre F Bougares H Schwenk and Y Bengio Learning phrase represen tations using RNN encoderdecoder for statistical machine translation In Arxiv preprint arXiv D Ciresan U Meier and J Schmidhuber Multicolumn deep neural networks for image classification In CVPR G E Dahl D Yu L Deng and A Acero Contextdependent pretrained deep neural networks for large vocabulary speech recognition IEEE Transactions on Audio Speech and Language Processing Special Issue on Deep Learning for Speech and Language Processing J Devlin R Zbib Z Huang T Lamar R Schwartz and J Makhoul Fast and robust neural network joint models for statistical machine translation In ACL Nadir Durrani Barry Haddow Philipp Koehn and Kenneth Heafield Edinburgh’s phrasebased machine translation systems for wmt In WMT A Graves Generating sequences with recurrent neural networks In Arxiv preprint arXiv A Graves S Fernandez F Gomez and J Schmidhuber Connectionist temporal classification labelling unsegmented sequence data with recurrent neural networks In ICML K M Hermann and P Blunsom Multilingual distributed representations without word alignment In ICLR G Hinton L Deng D Yu G Dahl A Mohamed N Jaitly A Senior V Vanhoucke P Nguyen T Sainath and B Kingsbury Deep neural networks for acoustic modeling in speech recognition IEEE Signal Processing Magazine S Hochreiter Untersuchungen zu dynamischen neuronalen netzen Master’s thesis Institut fur Infor matik Technische Universitat Munchen S Hochreiter Y Bengio P Frasconi and J Schmidhuber Gradient flow in recurrent nets the difficulty of learning longterm dependencies S Hochreiter and J Schmidhuber Long shortterm memory Neural Computation S Hochreiter and J Schmidhuber LSTM can solve hard long time lag problems N Kalchbrenner and P Blunsom Recurrent continuous translation models In EMNLP A Krizhevsky I Sutskever and G E Hinton ImageNet classification with deep convolutional neural networks In NIPS QV Le MA Ranzato R Monga M Devin K Chen GS Corrado J Dean and AY Ng Building highlevel features using large scale unsupervised learning In ICML Y LeCun L Bottou Y Bengio and P Haffner Gradientbased learning applied to document recognition Proceedings of the IEEE T Mikolov Statistical Language Models based on Neural Networks PhD thesis Brno University of Technology T Mikolov M Karafiat L Burget J Cernocky and S Khudanpur Recurrent neural network based language model In INTERSPEECH pages K Papineni S Roukos T Ward and W J Zhu BLEU a method for automatic evaluation of machine translation In ACL R Pascanu T Mikolov and Y Bengio On the difficulty of training recurrent neural networks arXiv preprint arXiv J PougetAbadie D Bahdanau B van Merrienboer K Cho and Y Bengio Overcoming the curse of sentence length for neural machine translation using automatic segmentation arXiv preprint arXiv A Razborov On small depth threshold circuits In Proc rd Scandinavian Workshop on Algorithm Theory D Rumelhart G E Hinton and R J Williams Learning representations by backpropagating errors Nature — H Schwenk University le mans httpwwwliumunivlemansfrschwenkcslm jointpaper Online accessed September M Sundermeyer R Schluter and H Ney LSTM neural networks for language modeling In INTER SPEECH P Werbos Backpropagation through time what it does and how to do it Proceedings of IEEE'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing unwanted Emoji's\n",
        "emojis_pattern = re.compile(pattern=\"[\"\n",
        "                    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                    u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                    u\"\\U00002702-\\U000027B0\"\n",
        "                    u\"\\U00002702-\\U000027B0\"\n",
        "                    u\"\\U000024C2-\\U0001F251\"\n",
        "                    u\"\\U0001f926-\\U0001f937\"\n",
        "                    u\"\\U00010000-\\U0010ffff\"\n",
        "                    u\"\\u2640-\\u2642\"\n",
        "                    u\"\\u2600-\\u2B55\"\n",
        "                    u\"\\u200d\"\n",
        "                    u\"\\u23cf\"\n",
        "                    u\"\\u23e9\"\n",
        "                    u\"\\u231a\"\n",
        "                    u\"\\ufe0f\"  # dingbats\n",
        "                    u\"\\u3030\"\n",
        "                \"]+\", flags = re.UNICODE)\n",
        "emojis_pattern.sub(r'', result_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "8UOpEHLEguoU",
        "outputId": "6b0b8f85-754e-4b48-a697-ef85f2476439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'v csCL  Dec \\n\\nV\\n\\narxX\\n\\n \\n\\nSequence to Sequence Learning\\nwith Neural Networks\\n\\nIlya Sutskever Oriol Vinyals Quoc V Le\\nGoogle Google Google\\nilyasugooglecom vinyalsgooglecom qvlgooglecom\\nAbstract\\n\\nDeep Neural Networks DNNs are powerful models that have achieved excel\\nlent performance on difficult learning tasks Although DNNs work well whenever\\nlarge labeled training sets are available they cannot be used to map sequences to\\nsequences In this paper we present a general endtoend approach to sequence\\nlearning that makes minimal assumptions on the sequence structure Our method\\nuses a multilayered Long ShortTerm Memory LSTM to map the input sequence\\nto a vector of a fixed dimensionality and then another deep LSTM to decode the\\ntarget sequence from the vector Our main result is that on an English to French\\ntranslation task from the WMT’  dataset the translations produced by the LSTM\\nachieve a BLEU score of  on the entire test set where the LSTM’s BLEU\\nscore was penalized on outofvocabulary words Additionally the LSTM did not\\nhave difficulty on long sentences For comparison a phrasebased SMT system\\nachieves a BLEU score of  on the same dataset When we used the LSTM\\nto rerank the  hypotheses produced by the aforementioned SMT system its\\nBLEU score increases to  which is close to the previous best result on this\\ntask The LSTM also learned sensible phrase and sentence representations that\\nare sensitive to word order and are relatively invariant to the active and the pas\\nsive voice Finally we found that reversing the order of the words in all source\\nsentences but not target sentences improved the LSTM’s performance markedly\\nbecause doing so introduced many short term dependencies between the source\\nand the target sentence which made the optimization problem easier\\n\\n Introduction\\n\\nDeep Neural Networks DNNs are extremely powerful machine learning models that achieve ex\\ncellent performance on difficult problems such as speech recognition   and visual object recog\\nnition     DNNs are powerful because they can perform arbitrary parallel computation\\nfor a modest number of steps A surprising example of the power of DNNs is their ability to sort\\nN WNbit numbers using only  hidden layers of quadratic size  So while neural networks are\\nrelated to conventional statistical models they learn an intricate computation Furthermore large\\nDNNs can be trained with supervised backpropagation whenever the labeled training set has enough\\ninformation to specify the network’s parameters Thus if there exists a parameter setting of a large\\nDNN that achieves good results for example because humans can solve the task very rapidly\\nsupervised backpropagation will find these parameters and solve the problem\\n\\nDespite their flexibility and power DNNs can only be applied to problems whose inputs and targets\\ncan be sensibly encoded with vectors of fixed dimensionality It is a significant limitation since\\nmany important problems are best expressed with sequences whose lengths are not known apriori\\nFor example speech recognition and machine translation are sequential problems Likewise ques\\ntion answering can also be seen as mapping a sequence of words representing the question to a\\n\\x0csequence of words representing the answer It is therefore clear that a domainindependent method\\nthat learns to map sequences to sequences would be useful\\n\\nSequences pose a challenge for DNNs because they require that the dimensionality of the inputs and\\noutputs is known and fixed In this paper we show that a straightforward application of the Long\\nShortTerm Memory LSTM architecture  can solve general sequence to sequence problems\\nThe idea is to use one LSTM to read the input sequence one timestep at a time to obtain large fixed\\ndimensional vector representation and then to use another LSTM to extract the output sequence\\nfrom that vector fig  The second LSTM is essentially a recurrent neural network language model\\n   except that it is conditioned on the input sequence The LSTM’s ability to successfully\\nlearn on data with long range temporal dependencies makes it a natural choice for this application\\ndue to the considerable time lag between the inputs and their corresponding outputs fig \\n\\nThere have been a number of related attempts to address the general sequence to sequence learning\\nproblem with neural networks Our approach is closely related to Kalchbrenner and Blunsom \\nwho were the first to map the entire input sentence to vector and is related to Cho et al  although\\nthe latter was used only for rescoring hypotheses produced by a phrasebased system Graves \\nintroduced a novel differentiable attention mechanism that allows neural networks to focus on dif\\nferent parts of their input and an elegant variant of this idea was successfully applied to machine\\ntranslation by Bahdanau et al  The Connectionist Sequence Classification is another popular\\ntechnique for mapping sequences to sequences with neural networks but it assumes a monotonic\\nalignment between the inputs and the outputs \\n\\nW X Y Z EOS\\n\\nA B C EOS W X Y Z\\n\\nFigure  Our model reads an input sentence “ABC” and produces “WXYZ” as the output sentence The\\nmodel stops making predictions after outputting the endofsentence token Note that the LSTM reads the\\ninput sentence in reverse because doing so introduces many short term dependencies in the data that make the\\noptimization problem much easier\\n\\nThe main result of this work is the following On the WMT’  English to French translation task\\nwe obtained a BLEU score of  by directly extracting translations from an ensemble of  deep\\nLSTMs with M parameters and  dimensional state each using a simple lefttoright beam\\nsearch decoder This is by far the best result achieved by direct translation with large neural net\\nworks For comparison the BLEU score of an SMT baseline on this dataset is   The \\nBLEU score was achieved by an LSTM with a vocabulary of k words so the score was penalized\\nwhenever the reference translation contained a word not covered by these k This result shows\\nthat a relatively unoptimized smallvocabulary neural network architecture which has much room\\nfor improvement outperforms a phrasebased SMT system\\n\\nFinally we used the LSTM to rescore the publicly available best lists of the SMT baseline on\\nthe same task  By doing so we obtained a BLEU score of  which improves the baseline by\\n BLEU points and is close to the previous best published result on this task which is  \\n\\nSurprisingly the LSTM did not suffer on very long sentences despite the recent experience of other\\nresearchers with related architectures  We were able to do well on long sentences because we\\nreversed the order of words in the source sentence but not the target sentences in the training and test\\nset By doing so we introduced many short term dependencies that made the optimization problem\\nmuch simpler see sec  and  As a result SGD could learn LSTMs that had no trouble with\\nlong sentences The simple trick of reversing the words in the source sentence is one of the key\\ntechnical contributions of this work\\n\\nA useful property of the LSTM is that it learns to map an input sentence of variable length into\\na fixeddimensional vector representation Given that translations tend to be paraphrases of the\\nsource sentences the translation objective encourages the LSTM to find sentence representations\\nthat capture their meaning as sentences with similar meanings are close to each other while different\\n\\x0csentences meanings will be far A qualitative evaluation supports this claim showing that our model\\nis aware of word order and is fairly invariant to the active and passive voice\\n\\n The model\\n\\nThe Recurrent Neural Network RNN   is a natural generalization of feedforward neural\\nnetworks to sequences Given a sequence of inputs  a standard RNN computes a\\nsequence of outputs y yr by iterating the following equation\\n\\nh  sigm Wa  W he\\nYe  Whe\\n\\nThe RNN can easily map sequences to sequences whenever the alignment between the inputs the\\noutputs is known ahead of time However it is not clear how to apply an RNN to problems whose\\ninput and the output sequences have different lengths with complicated and nonmonotonic relation\\nships\\n\\nThe simplest strategy for general sequence learning is to map the input sequence to a fixedsized\\nvector using one RNN and then to map the vector to the target sequence with another RNN this\\napproach has also been taken by Cho et al  While it could work in principle since the RNN is\\nprovided with all the relevant information it would be difficult to train the RNNs due to the resulting\\nlong term dependencies figure      However the Long ShortTerm Memory LSTM\\n is known to learn problems with long range temporal dependencies so an LSTM may succeed\\nin this setting\\n\\nThe goal of the LSTM is to estimate the conditional probability pyy  where\\nv is an input sequence and y yr’ is its corresponding output sequence whose length\\nT’ may differ from ’ The LSTM computes this conditional probability by first obtaining the fixed\\ndimensional representation v of the input sequence   given by the last hidden state of the\\nLSTM and then computing the probability of yy with a standard LSTMLM formulation\\nwhose initial hidden state is set to the representation v of  \\n\\nT’\\nPyyreier   viyelos yi yea \\nt\\n\\nIn this equation each pyzv y Yz— distribution is represented with a softmax over all the\\nwords in the vocabulary We use the LSTM formulation from Graves  Note that we require that\\neach sentence ends with a special endofsentence symbol “EOS” which enables the model to\\ndefine a distribution over sequences of all possible lengths The overall scheme is outlined in figure\\n where the shown LSTM computes the representation of “A” “B” “C” “EOS” and then uses\\nthis representation to compute the probability of “W” “X” “Y” “Z” “EOS”\\n\\nOur actual models differ from the above description in three important ways First we used two\\ndifferent LSTMs one for the input sequence and another for the output sequence because doing\\nso increases the number model parameters at negligible computational cost and makes it natural to\\ntrain the LSTM on multiple language pairs simultaneously  Second we found that deep LSTMs\\nsignificantly outperformed shallow LSTMs so we chose an LSTM with four layers Third we found\\nit extremely valuable to reverse the order of the words of the input sentence So for example instead\\nof mapping the sentence a bc to the sentence a y the LSTM is asked to map c b a to a \\nwhere a  ¥ is the translation of a b c This way a is in close proximity to a b is fairly close to \\nand so on a fact that makes it easy for SGD to “establish communication” between the input and the\\noutput We found this simple data transformation to greatly improve the performance of the LSTM\\n\\n Experiments\\n\\nWe applied our method to the WMT’ English to French MT task in two ways We used it to\\ndirectly translate the input sentence without using a reference SMT system and we it to rescore the\\nnbest lists of an SMT baseline We report the accuracy of these translation methods present sample\\ntranslations and visualize the resulting sentence representation\\n\\x0c Dataset details\\n\\nWe used the WMT’  English to French dataset We trained our models on a subset of M sen\\ntences consisting of M French words and M English words which is a clean “selected”\\nsubset from  We chose this translation task and this specific training set subset because of the\\npublic availability of a tokenized training and test set together with best lists from the baseline\\nSMT \\n\\nAs typical neural language models rely on a vector representation for each word we used a fixed\\nvocabulary for both languages We used  of the most frequent words for the source language\\nand  of the most frequent words for the target language Every outofvocabulary word was\\nreplaced with a special “UNK” token\\n\\n Decoding and Rescoring\\n\\nThe core of our experiments involved training a large deep LSTM on many sentence pairs We\\ntrained it by maximizing the log probability of a correct translation T’ given the source sentence S\\nso the training objective is\\n\\nS S— logpTS\\n\\nTSES\\n\\nwhere S is the training set Once training is complete we produce translations by finding the most\\nlikely translation according to the LSTM\\n\\nA\\n\\nT  arg max pTS \\n\\nWe search for the most likely translation using a simple lefttoright beam search decoder which\\nmaintains a small number B of partial hypotheses where a partial hypothesis is a prefix of some\\ntranslation At each timestep we extend each partial hypothesis in the beam with every possible\\nword in the vocabulary This greatly increases the number of the hypotheses so we discard all but\\nthe B most likely hypotheses according to the model’s log probability As soon as the “EOS”\\nsymbol is appended to a hypothesis it is removed from the beam and is added to the set of complete\\nhypotheses While this decoder is approximate it is simple to implement Interestingly our system\\nperforms well even with a beam size of  and a beam of size  provides most of the benefits of beam\\nsearch Table \\n\\nWe also used the LSTM to rescore the best lists produced by the baseline system  To\\nrescore an nbest list we computed the log probability of every hypothesis with our LSTM and took\\nan even average with their score and the LSTM’s score\\n\\n Reversing the Source Sentences\\n\\nWhile the LSTM is capable of solving problems with long term dependencies we discovered that\\nthe LSTM learns much better when the source sentences are reversed the target sentences are not\\nreversed By doing so the LSTM’s test perplexity dropped from  to  and the test BLEU\\nscores of its decoded translations increased from  to \\n\\nWhile we do not have a complete explanation to this phenomenon we believe that it is caused by\\nthe introduction of many short term dependencies to the dataset Normally when we concatenate a\\nsource sentence with a target sentence each word in the source sentence is far from its corresponding\\nword in the target sentence As a result the problem has a large “minimal time lag”  By\\nreversing the words in the source sentence the average distance between corresponding words in\\nthe source and target language is unchanged However the first few words in the source language\\nare now very close to the first few words in the target language so the problem’s minimal time lag is\\ngreatly reduced Thus backpropagation has an easier time “establishing communication” between\\nthe source sentence and the target sentence which in turn results in substantially improved overall\\nperformance\\n\\nInitially we believed that reversing the input sentences would only lead to more confident predic\\ntions in the early parts of the target sentence and to less confident predictions in the later parts How\\never LSTMs trained on reversed source sentences did much better on long sentences than LSTMs\\n\\x0ctrained on the raw source sentences see sec  which suggests that reversing the input sentences\\nresults in LSTMs with better memory utilization\\n\\n Training details\\n\\nWe found that the LSTM models are fairly easy to train We used deep LSTMs with  layers\\nwith  cells at each layer and  dimensional word embeddings with an input vocabulary\\nof  and an output vocabulary of  Thus the deep LSTM uses  real numbers to\\nrepresent a sentence We found deep LSTMs to significantly outperform shallow LSTMs where\\neach additional layer reduced perplexity by nearly  possibly due to their much larger hidden\\nstate We used a naive softmax over  words at each output The resulting LSTM has M\\nparameters of which M are pure recurrent connections M for the “encoder” LSTM and M\\nfor the “decoder” LSTM The complete training details are given below\\n\\ne We initialized all of the LSTM’s parameters with the uniform distribution between \\nand \\n\\ne We used stochastic gradient descent without momentum with a fixed learning rate of \\nAfter  epochs we begun halving the learning rate every half epoch We trained our models\\nfor a total of  epochs\\n\\ne We used batches of  sequences for the gradient and divided it the size of the batch\\nnamely \\n\\ne Although LSTMs tend to not suffer from the vanishing gradient problem they can have\\nexploding gradients Thus we enforced a hard constraint on the norm of the gradient \\n by scaling it when its norm exceeded a threshold For each training batch we compute\\n  g where g is the gradient divided by  If s   we set g  \\n\\ne Different sentences have different lengths Most sentences are short eg length \\nbut some sentences are long eg length   so a minibatch of  randomly chosen\\ntraining sentences will have many short sentences and few long sentences and as a result\\nmuch of the computation in the minibatch is wasted To address this problem we made sure\\nthat all sentences in a minibatch are roughly of the same length yielding a x speedup\\n\\n Parallelization\\n\\nA C implementation of deep LSTM with the configuration from the previous section on a sin\\ngle GPU processes a speed of approximately  words per second This was too slow for our\\npurposes so we parallelized our model using an GPU machine Each layer of the LSTM was\\nexecuted on a different GPU and communicated its activations to the next GPU  layer as soon as\\nthey were computed Our models have  layers of LSTMs each of which resides on a separate\\nGPU The remaining  GPUs were used to parallelize the softmax so each GPU was responsible\\nfor multiplying by a  x  matrix The resulting implementation achieved a speed of \\nboth English and French words per second with a minibatch size of  Training took about a ten\\ndays with this implementation\\n\\n Experimental Results\\n\\nWe used the cased BLEU score  to evaluate the quality of our translations We computed our\\nBLEU scores using multibleup on the tokenized predictions and ground truth This way\\nof evaluating the BELU score is consistent with  and  and reproduces the  score of \\nHowever if we evaluate the best WMT’  system  whose predictions can be downloaded from\\nstatmtorgmatrix in this manner we get  which is greater than the  reported by\\nstatmtorgmatrix\\n\\nThe results are presented in tables  and  Our best results are obtained with an ensemble of LSTMs\\nthat differ in their random initializations and in the random order of minibatches While the decoded\\ntranslations of the LSTM ensemble do not outperform the best WMT’  system it is the first time\\nthat a pure neural translation system outperforms a phrasebased SMT baseline on a large scale MT\\n\\n‘There several variants of the BLEU score and each variant is defined with a perl script\\n\\x0ctest BLEU score ntsti\\nBahdanau et al  \\nBaseline System  \\n\\nSingle forward LSTM beam size  \\n\\nTable  The performance of the LSTM on WMT’  English to French test set ntst Note that\\nan ensemble of  LSTMs with a beam of size  is cheaper than of a single LSTM with a beam of\\nsize \\n\\n \\n\\ntest BLEU score ntsti\\nBaseline System  \\n\\nCho et al  \\nBest WMT result \\n\\nRescoring the baseline best with a single forward LSTM \\nRescoring the baseline best with a single reversed LSTM \\nRescoring the baseline best with an ensemble of  reversed LSTMs\\n\\nOracle Rescoring of the Baseline best lists\\n\\n \\n\\nTable  Methods that use neural networks together with an SMT system on the WMT’  English\\nto French test set ntst\\n\\ntask by a sizeable margin despite its inability to handle outofvocabulary words The LSTM is\\nwithin  BLEU points of the best WMT’  result if it is used to rescore the best list of the\\nbaseline system\\n\\n Performance on long sentences\\n\\nWe were surprised to discover that the LSTM did well on long sentences which is shown quantita\\ntively in figure  Table  presents several examples of long sentences and their translations\\n\\n Model Analysis\\n\\nal O  was given a card by her in the garden\\n\\nF OMary admires John  O Inthe garden  she gave me a card\\nO She gave me a card in the garden\\n OMary is in love with John\\n\\nOMary respects John\\naL OJohn admires Mary\\n O She was given a card by me in the garden\\n\\n OJohn is in love with Mar\\ny © In the garden   gave her a card\\n\\n  OJohn respects Mary © I gave her a card in the garden\\n\\n \\n\\n \\n\\n \\n\\n \\n\\n    L L L L J  L L L L L  J\\n                 \\n\\nFigure  The figure shows a dimensional PCA projection of the LSTM hidden states that are obtained\\nafter processing the phrases in the figures The phrases are clustered by meaning which in these examples is\\nprimarily a function of word order which would be difficult to capture with a bagofwords model Notice that\\nboth clusters have similar internal structure\\n\\nOne of the attractive features of our model is its ability to turn a sequence of words into a vector\\nof fixed dimensionality Figure  visualizes some of the learned representations The figure clearly\\nshows that the representations are sensitive to the order of words while being fairly insensitive to the\\n\\x0cOur model  Ulrich UNK  membre du conseil d’ administration du constructeur automobile Audi \\naffirme qu’ ils’ agit d’ une pratique courante depuis des années pour que les teléphones\\nportables puissent étre collectés avant les reunions du conseil d’ administration afin qu’ ils\\nne soient pas utilises comme appareils d’ écoute a distance \\n\\nTruth Ulrich Hackenberg  membre du conseil d’ administration du constructeur automobile Audi \\ndéclare que la collecte des téléphones portables avant les reunions du conseil  afin qu’ ils\\nne puissent pas étre utilisés comme appareils d’ écoute a distance  est une pratique courante\\ndepuis des années \\n\\nOur model  “ Les telephones cellulaires  qui sont vraiment une question  non seulement parce qu’ ils\\npourraient potentiellement causer des interférences avec les appareils de navigation  mais\\nnous savons  selon la FCC  qu’ ils pourraient interférer avec les tours de téléphone cellulaire\\nlorsqu’ ils sont dans I’ air”  dit UNK \\n\\nTruth “ Les telephones portables sont véritablement un probleme  non seulement parce qu’ ils\\npourraient éventuellement créer des interférences avec les instruments de navigation  mais\\nparce que nous savons  d’ apres la FCC  qu’ ils pourraient perturber les antennesrelais de\\ntelephonie mobile s’ ils sont utilisés a bord ”  a declaré Rosenker \\n\\nOur model  Avec la crémation  il y a un “ sentiment de violence contre le corps d’ un étre cher ” \\n\\nqui sera “ réduit a une pile de cendres ” en trés peu de temps au lieu d’ un processus de\\ndecomposition “ qui accompagnera les étapes du deuil ” \\n\\nTruth Il y a avec la cremation  “ une violence faite au corps aimé ” \\n\\na qui va étre “ reduit a un tas de cendres ” en trés peu de temps  et non aprés un processus de\\n\\ndecomposition  qui “ accompagnerait les phases du deuil ” \\n\\n \\n\\nTable  A few examples of long translations produced by the LSTM alongside the ground truth\\ntranslations The reader can verify that the translations are sensible using Google translate\\n\\n   \\n\\n \\n\\n \\n\\n— LSTM  — LSTM \\n e—e baseline   e—e baseline \\n \\nY Y\\n° °\\nU U\\nwn wn\\nD D\\n   \\na a\\n \\n              \\n              \\ntest sentences sorted by their length test sentences sorted by average word frequency rank\\n\\nFigure  The left plot shows the performance of our system as a function of sentence length where the\\nxaxis corresponds to the test sentences sorted by their length and is marked by the actual sequence lengths\\nThere is no degradation on sentences with less than  words there is only a minor degradation on the longest\\nsentences The right plot shows the LSTM’s performance on sentences with progressively more rare words\\nwhere the xaxis corresponds to the test sentences sorted by their “average word frequency rank”\\n\\nreplacement of an active voice with a passive voice The twodimensional projections are obtained\\nusing PCA\\n\\n Related work\\n\\nThere is a large body of work on applications of neural networks to machine translation So far\\nthe simplest and most effective way of applying an RNNLanguage Model RNNLM  or a\\n\\x0cFeedforward Neural Network Language Model NNLM  to an MT task is by rescoring the n\\nbest lists of a strong MT baseline  which reliably improves translation quality\\n\\nMore recently researchers have begun to look into ways of including information about the source\\nlanguage into the NNLM Examples of this work include Auli et al  who combine an NNLM\\nwith a topic model of the input sentence which improves rescoring performance Devlin et al \\nfollowed a similar approach but they incorporated their NNLM into the decoder of an MT system\\nand used the decoder’s alignment information to provide the NNLM with the most useful words in\\nthe input sentence Their approach was highly successful and it achieved large improvements over\\ntheir baseline\\n\\nOur work is closely related to Kalchbrenner and Blunsom  who were the first to map the input\\nsentence into a vector and then back to a sentence although they map sentences to vectors using\\nconvolutional neural networks which lose the ordering of the words Similarly to this work Cho et\\nal  used an LSTMlike RNN architecture to map sentences into vectors and back although their\\nprimary focus was on integrating their neural network into an SMT system Bahdanau et al  also\\nattempted direct translations with a neural network that used an attention mechanism to overcome\\nthe poor performance on long sentences experienced by Cho et al  and achieved encouraging\\nresults Likewise PougetAbadie et al  attempted to address the memory problem of Cho et\\nal  by translating pieces of the source sentence in way that produces smooth translations which\\nis similar to a phrasebased approach We suspect that they could achieve similar improvements by\\nsimply training their networks on reversed source sentences\\n\\nEndtoend training is also the focus of Hermann et al  whose model represents the inputs and\\noutputs by feedforward networks and map them to similar points in space However their approach\\ncannot generate translations directly to get a translation they need to do a look up for closest vector\\nin the precomputed database of sentences or to rescore a sentence\\n\\n Conclusion\\n\\nIn this work we showed that a large deep LSTM that has a limited vocabulary and that makes\\nalmost no assumption about problem structure can outperform a standard SMTbased system whose\\nvocabulary is unlimited on a largescale MT task The success of our simple LSTMbased approach\\non MT suggests that it should do well on many other sequence learning problems provided they\\nhave enough training data\\n\\nWe were surprised by the extent of the improvement obtained by reversing the words in the source\\nsentences We conclude that it is important to find a problem encoding that has the greatest number\\nof short term dependencies as they make the learning problem much simpler In particular while\\nwe were unable to train a standard RNN on the nonreversed translation problem shown in fig \\nwe believe that a standard RNN should be easily trainable when the source sentences are reversed\\nalthough we did not verify it experimentally\\n\\nWe were also surprised by the ability of the LSTM to correctly translate very long sentences We\\nwere initially convinced that the LSTM would fail on long sentences due to its limited memory\\nand other researchers reported poor performance on long sentences with a model similar to ours\\n   And yet LSTMs trained on the reversed dataset had little difficulty translating long\\nsentences\\n\\nMost importantly we demonstrated that a simple straightforward and a relatively unoptimized ap\\nproach can outperform an SMT system so further work will likely lead to even greater translation\\naccuracies These results suggest that our approach will likely do well on other challenging sequence\\nto sequence problems\\n\\n Acknowledgments\\n\\nWe thank Samy Bengio Jeff Dean Matthieu Devin Geoffrey Hinton Nal Kalchbrenner Thang Luong Wolf\\ngang Macherey Rajat Monga Vincent Vanhoucke Peng Xu Wojciech Zaremba and the Google Brain team\\nfor useful comments and discussions\\n\\x0cReferences\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nM Auli M Galley C Quirk and G Zweig Joint language and translation modeling with recurrent\\nneural networks In EMNLP \\n\\nD Bahdanau K Cho and Y Bengio Neural machine translation by jointly learning to align and translate\\narXiv preprint arXiv  \\n\\nY Bengio R Ducharme P Vincent and C Jauvin A neural probabilistic language model In Journal of\\nMachine Learning Research pages  \\n\\nY Bengio P Simard and P Frasconi Learning longterm dependencies with gradient descent is difficult\\nIEEE Transactions on Neural Networks — \\n\\nK Cho B Merrienboer C Gulcehre F Bougares H Schwenk and Y Bengio Learning phrase represen\\ntations using RNN encoderdecoder for statistical machine translation In Arxiv preprint arXiv \\n\\n\\nD Ciresan U Meier and J Schmidhuber Multicolumn deep neural networks for image classification\\nIn CVPR \\n\\nG E Dahl D Yu L Deng and A Acero Contextdependent pretrained deep neural networks for large\\nvocabulary speech recognition IEEE Transactions on Audio Speech and Language Processing  Special\\nIssue on Deep Learning for Speech and Language Processing \\n\\nJ Devlin R Zbib Z Huang T Lamar R Schwartz and J Makhoul Fast and robust neural network\\njoint models for statistical machine translation In ACL \\n\\nNadir Durrani Barry Haddow Philipp Koehn and Kenneth Heafield Edinburgh’s phrasebased machine\\ntranslation systems for wmt In WMT \\n\\nA Graves Generating sequences with recurrent neural networks In Arxiv preprint arXiv \\n\\n\\nA Graves S Fernandez F Gomez and J Schmidhuber Connectionist temporal classification labelling\\nunsegmented sequence data with recurrent neural networks In ICML \\n\\nK M Hermann and P Blunsom Multilingual distributed representations without word alignment In\\nICLR \\n\\nG Hinton L Deng D Yu G Dahl A Mohamed N Jaitly A Senior V Vanhoucke P Nguyen\\nT Sainath and B Kingsbury Deep neural networks for acoustic modeling in speech recognition IEEE\\nSignal Processing Magazine \\n\\nS Hochreiter Untersuchungen zu dynamischen neuronalen netzen Master’s thesis Institut fur Infor\\nmatik Technische Universitat Munchen \\n\\nS Hochreiter Y Bengio P Frasconi and J Schmidhuber Gradient flow in recurrent nets the difficulty\\nof learning longterm dependencies \\n\\nS Hochreiter and J Schmidhuber Long shortterm memory Neural Computation \\n\\nS Hochreiter and J Schmidhuber LSTM can solve hard long time lag problems \\n\\nN Kalchbrenner and P Blunsom Recurrent continuous translation models In EMNLP \\n\\nA Krizhevsky I Sutskever and G E Hinton ImageNet classification with deep convolutional neural\\nnetworks In NIPS \\n\\nQV Le MA Ranzato R Monga M Devin K Chen GS Corrado J Dean and AY Ng Building\\nhighlevel features using large scale unsupervised learning In ICML \\n\\nY LeCun L Bottou Y Bengio and P Haffner Gradientbased learning applied to document recognition\\nProceedings of the IEEE \\n\\nT Mikolov Statistical Language Models based on Neural Networks PhD thesis Brno University of\\nTechnology \\n\\nT Mikolov M Karafiat L Burget J Cernocky and S Khudanpur Recurrent neural network based\\nlanguage model In INTERSPEECH pages  \\n\\nK Papineni S Roukos T Ward and W J Zhu BLEU a method for automatic evaluation of machine\\ntranslation In ACL \\n\\nR Pascanu T Mikolov and Y Bengio On the difficulty of training recurrent neural networks arXiv\\npreprint arXiv \\n\\nJ PougetAbadie D Bahdanau B van Merrienboer K Cho and Y Bengio Overcoming the\\ncurse of sentence length for neural machine translation using automatic segmentation arXiv preprint\\narXiv \\n\\nA Razborov On small depth threshold circuits In Proc rd Scandinavian Workshop on Algorithm\\nTheory \\n\\nD Rumelhart G E Hinton and R J Williams Learning representations by backpropagating errors\\nNature — \\n\\nH Schwenk University le mans httpwwwliumunivlemansfrschwenkcslm\\njointpaper  Online accessed September\\n\\nM Sundermeyer R Schluter and H Ney LSTM neural networks for language modeling In INTER\\nSPEECH \\n\\nP Werbos Backpropagation through time what it does and how to do it Proceedings of IEEE \\n\\x0c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspellchecker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaxAto67igX4",
        "outputId": "59099311-a88f-4df3-b23b-5ba5f66cce29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker()\n",
        "\n",
        "def correct_spelling(text, width=80):\n",
        "    corrected_text = list()\n",
        "    misspelled_words = spell.unknown(text.split())\n",
        "\n",
        "    for word in text.split():\n",
        "        next_word = word\n",
        "        if word in misspelled_words:\n",
        "            # Get a list of suggested corrections\n",
        "            candidates = spell.candidates(word)\n",
        "\n",
        "            # Choose the best candidate (if any)\n",
        "            if candidates:\n",
        "                next_word = spell.correction(word)\n",
        "            else:\n",
        "                # If no candidates, keep the original word\n",
        "                next_word = word\n",
        "\n",
        "        corrected_text.append(next_word)\n",
        "\n",
        "    corrected_text = \" \".join(corrected_text)\n",
        "\n",
        "    # Wrap the text to the specified width\n",
        "    wrapped_text = textwrap.fill(corrected_text, width=width)\n",
        "\n",
        "    return wrapped_text\n",
        "\n",
        "# Example usage:\n",
        "corrected_result = correct_spelling(result_text)\n",
        "print(corrected_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rg5Ipf9tg8Yp",
        "outputId": "570d19b0-6e06-401b-e931-2dcebecfc523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v csCL Dec V arxX Sequence to Sequence Learning with Neural Networks Ilya\n",
            "Sutskever Oriol Vinyals Quoc V Le Google Google Google ilyasugooglecom\n",
            "vinyalsgooglecom qvlgooglecom Abstract Deep Neural Networks DNNs are powerful\n",
            "models that have achieved excel lent performance on difficult learning tasks\n",
            "Although DNNs work well whenever large labeled training sets are available they\n",
            "cannot be used to map sequences to sequences In this paper we present a general\n",
            "endtoend approach to sequence learning that makes minimal assumptions on the\n",
            "sequence structure Our method uses a multilayered Long ShortTerm Memory LSTM to\n",
            "map the input sequence to a vector of a fixed dimensionality and then another\n",
            "deep LSTM to decode the target sequence from the vector Our main result is that\n",
            "on an English to French translation task from the WMT’ dataset the translations\n",
            "produced by the LSTM achieve a BLEU score of on the entire test set where the\n",
            "LSTM’s BLEU score was penalized on outofvocabulary words Additionally the LSTM\n",
            "did not have difficulty on long sentences For comparison a phrasebased SMT\n",
            "system achieves a BLEU score of on the same dataset When we used the LSTM to\n",
            "reran the hypotheses produced by the aforementioned SMT system its BLEU score\n",
            "increases to which is close to the previous best result on this task The LSTM\n",
            "also learned sensible phrase and sentence representations that are sensitive to\n",
            "word order and are relatively invariant to the active and the pas give voice\n",
            "Finally we found that reversing the order of the words in all source sentences\n",
            "but not target sentences improved the LSTM’s performance markedly because doing\n",
            "so introduced many short term dependencies between the source and the target\n",
            "sentence which made the optimization problem easier Introduction Deep Neural\n",
            "Networks DNNs are extremely powerful machine learning models that achieve ex\n",
            "client performance on difficult problems such as speech recognition and visual\n",
            "object recon nation DNNs are powerful because they can perform arbitrary\n",
            "parallel computation for a modest number of steps A surprising example of the\n",
            "power of DNNs is their ability to sort N WNbit numbers using only hidden layers\n",
            "of quadratic size So while neural networks are related to conventional\n",
            "statistical models they learn an intricate computation Furthermore large DNNs\n",
            "can be trained with supervised backpropagation whenever the labeled training set\n",
            "has enough information to specify the networks parameters Thus if there exists a\n",
            "parameter setting of a large DNN that achieves good results for example because\n",
            "humans can solve the task very rapidly supervised backpropagation will find\n",
            "these parameters and solve the problem Despite their flexibility and power DNNs\n",
            "can only be applied to problems whose inputs and targets can be sensibly encoded\n",
            "with vectors of fixed dimensionality It is a significant limitation since many\n",
            "important problems are best expressed with sequences whose lengths are not known\n",
            "prior For example speech recognition and machine translation are sequential\n",
            "problems Likewise quest ton answering can also be seen as mapping a sequence of\n",
            "words representing the question to a sequence of words representing the answer\n",
            "It is therefore clear that a domainindependent method that learns to map\n",
            "sequences to sequences would be useful Sequences pose a challenge for DNNs\n",
            "because they require that the dimensionality of the inputs and outputs is known\n",
            "and fixed In this paper we show that a straightforward application of the Long\n",
            "ShortTerm Memory LSTM architecture can solve general sequence to sequence\n",
            "problems The idea is to use one LSTM to read the input sequence one trimester at\n",
            "a time to obtain large fixed dimensional vector representation and then to use\n",
            "another LSTM to extract the output sequence from that vector fig The second LSTM\n",
            "is essentially a recurrent neural network language model except that it is\n",
            "conditioned on the input sequence The LSTM’s ability to successfully learn on\n",
            "data with long range temporal dependencies makes it a natural choice for this\n",
            "application due to the considerable time lag between the inputs and their\n",
            "corresponding outputs fig There have been a number of related attempts to\n",
            "address the general sequence to sequence learning problem with neural networks\n",
            "Our approach is closely related to Kalchbrenner and Blunsom who were the first\n",
            "to map the entire input sentence to vector and is related to Cho et a although\n",
            "the latter was used only for restoring hypotheses produced by a phrasebased\n",
            "system Graves introduced a novel differentiable attention mechanism that allows\n",
            "neural networks to focus on if fervent parts of their input and an elegant\n",
            "variant of this idea was successfully applied to machine translation by Bahdanau\n",
            "et a The Connectionist Sequence Classification is another popular technique for\n",
            "mapping sequences to sequences with neural networks but it assumes a monotonic\n",
            "alignment between the inputs and the outputs W X Y Z EOS A B C EOS W X Y Z\n",
            "Figure Our model reads an input sentence “ABC” and produces “WXYZ” as the output\n",
            "sentence The model stops making predictions after outputting the endofsentence\n",
            "token Note that the LSTM reads the input sentence in reverse because doing so\n",
            "introduces many short term dependencies in the data that make the optimization\n",
            "problem much easier The main result of this work is the following On the WMT’\n",
            "English to French translation task we obtained a BLEU score of by directly\n",
            "extracting translations from an ensemble of deep LSTMs with M parameters and\n",
            "dimensional state each using a simple lefttoright beam search decoder This is by\n",
            "far the best result achieved by direct translation with large neural net works\n",
            "For comparison the BLEU score of an SMT baseline on this dataset is The BLEU\n",
            "score was achieved by an LSTM with a vocabulary of k words so the score was\n",
            "penalized whenever the reference translation contained a word not covered by\n",
            "these k This result shows that a relatively optimized smallvocabulary neural\n",
            "network architecture which has much room for improvement outperforms a\n",
            "phrasebased SMT system Finally we used the LSTM to rescore the publicly\n",
            "available best lists of the SMT baseline on the same task By doing so we\n",
            "obtained a BLEU score of which improves the baseline by BLEU points and is close\n",
            "to the previous best published result on this task which is Surprisingly the\n",
            "LSTM did not suffer on very long sentences despite the recent experience of\n",
            "other researchers with related architectures We were able to do well on long\n",
            "sentences because we reversed the order of words in the source sentence but not\n",
            "the target sentences in the training and test set By doing so we introduced many\n",
            "short term dependencies that made the optimization problem much simpler see see\n",
            "and As a result SGD could learn LSTMs that had no trouble with long sentences\n",
            "The simple trick of reversing the words in the source sentence is one of the key\n",
            "technical contributions of this work A useful property of the LSTM is that it\n",
            "learns to map an input sentence of variable length into a fixeddimensional\n",
            "vector representation Given that translations tend to be paraphrases of the\n",
            "source sentences the translation objective encourages the LSTM to find sentence\n",
            "representations that capture their meaning as sentences with similar meanings\n",
            "are close to each other while different sentences meanings will be far A\n",
            "qualitative evaluation supports this claim showing that our model is aware of\n",
            "word order and is fairly invariant to the active and passive voice The model The\n",
            "Recurrent Neural Network RNN is a natural generalization of feedforward neural\n",
            "networks to sequences Given a sequence of inputs a standard RNN computes a\n",
            "sequence of outputs y or by iterating the following equation h sign Wa W he Ye\n",
            "Whe The RNN can easily map sequences to sequences whenever the alignment between\n",
            "the inputs the outputs is known ahead of time However it is not clear how to\n",
            "apply an RNN to problems whose input and the output sequences have different\n",
            "lengths with complicated and nonmonotonic relation ships The simplest strategy\n",
            "for general sequence learning is to map the input sequence to a fixedsized\n",
            "vector using one RNN and then to map the vector to the target sequence with\n",
            "another RNN this approach has also been taken by Cho et a While it could work in\n",
            "principle since the RNN is provided with all the relevant information it would\n",
            "be difficult to train the RNNs due to the resulting long term dependencies\n",
            "figure However the Long ShortTerm Memory LSTM is known to learn problems with\n",
            "long range temporal dependencies so an LSTM may succeed in this setting The goal\n",
            "of the LSTM is to estimate the conditional probability pay where v is an input\n",
            "sequence and y you is its corresponding output sequence whose length T’ may\n",
            "differ from i The LSTM computes this conditional probability by first obtaining\n",
            "the fixed dimensional representation v of the input sequence given by the last\n",
            "hidden state of the LSTM and then computing the probability of my with a\n",
            "standard LSTMLM formulation whose initial hidden state is set to the\n",
            "representation v of T’ Pyyreier videos i yea t In this equation each perv y Yz—\n",
            "distribution is represented with a softa over all the words in the vocabulary We\n",
            "use the LSTM formulation from Graves Note that we require that each sentence\n",
            "ends with a special endofsentence symbol “EOS” which enables the model to define\n",
            "a distribution over sequences of all possible lengths The overall scheme is\n",
            "outlined in figure where the shown LSTM computes the representation of “A” “B”\n",
            "“C” “EOS” and then uses this representation to compute the probability of “W”\n",
            "“X” “Y” “Z” “EOS” Our actual models differ from the above description in three\n",
            "important ways First we used two different LSTMs one for the input sequence and\n",
            "another for the output sequence because doing so increases the number model\n",
            "parameters at negligible computational cost and makes it natural to train the\n",
            "LSTM on multiple language pairs simultaneously Second we found that deep LSTMs\n",
            "significantly outperformed shallow LSTMs so we chose an LSTM with four layers\n",
            "Third we found it extremely valuable to reverse the order of the words of the\n",
            "input sentence So for example instead of mapping the sentence a be to the\n",
            "sentence a y the LSTM is asked to map c b a to a where a i is the translation of\n",
            "a b c This way a is in close proximity to a b is fairly close to and so on a\n",
            "fact that makes it easy for SGD to establish communication between the input and\n",
            "the output We found this simple data transformation to greatly improve the\n",
            "performance of the LSTM Experiments We applied our method to the WMT’ English to\n",
            "French MT task in two ways We used it to directly translate the input sentence\n",
            "without using a reference SMT system and we it to rescore the best lists of an\n",
            "SMT baseline We report the accuracy of these translation methods present sample\n",
            "translations and visualize the resulting sentence representation Dataset details\n",
            "We used the WMT’ English to French dataset We trained our models on a subset of\n",
            "M see fences consisting of M French words and M English words which is a clean\n",
            "selected subset from We chose this translation task and this specific training\n",
            "set subset because of the public availability of a tokenized training and test\n",
            "set together with best lists from the baseline SMT As typical neural language\n",
            "models rely on a vector representation for each word we used a fixed vocabulary\n",
            "for both languages We used of the most frequent words for the source language\n",
            "and of the most frequent words for the target language Every outofvocabulary\n",
            "word was replaced with a special “UNK” token Decoding and Rescoring The core of\n",
            "our experiments involved training a large deep LSTM on many sentence pairs We\n",
            "trained it by maximizing the log probability of a correct translation T’ given\n",
            "the source sentence S so the training objective is S S— logpTS TSES where S is\n",
            "the training set Once training is complete we produce translations by finding\n",
            "the most likely translation according to the LSTM A T are man pTS We search for\n",
            "the most likely translation using a simple lefttoright beam search decoder which\n",
            "maintains a small number B of partial hypotheses where a partial hypothesis is a\n",
            "prefix of some translation At each trimester we extend each partial hypothesis\n",
            "in the beam with every possible word in the vocabulary This greatly increases\n",
            "the number of the hypotheses so we discard all but the B most likely hypotheses\n",
            "according to the models log probability As soon as the “EOS” symbol is appended\n",
            "to a hypothesis it is removed from the beam and is added to the set of complete\n",
            "hypotheses While this decoder is approximate it is simple to implement\n",
            "Interestingly our system performs well even with a beam size of and a beam of\n",
            "size provides most of the benefits of beam search Table We also used the LSTM to\n",
            "rescore the best lists produced by the baseline system To rescore an best list\n",
            "we computed the log probability of every hypothesis with our LSTM and took an\n",
            "even average with their score and the LSTM’s score Reversing the Source\n",
            "Sentences While the LSTM is capable of solving problems with long term\n",
            "dependencies we discovered that the LSTM learns much better when the source\n",
            "sentences are reversed the target sentences are not reversed By doing so the\n",
            "LSTM’s test perplexity dropped from to and the test BLEU scores of its decoded\n",
            "translations increased from to While we do not have a complete explanation to\n",
            "this phenomenon we believe that it is caused by the introduction of many short\n",
            "term dependencies to the dataset Normally when we concatenate a source sentence\n",
            "with a target sentence each word in the source sentence is far from its\n",
            "corresponding word in the target sentence As a result the problem has a large\n",
            "minimal time lag By reversing the words in the source sentence the average\n",
            "distance between corresponding words in the source and target language is\n",
            "unchanged However the first few words in the source language are now very close\n",
            "to the first few words in the target language so the problems minimal time lag\n",
            "is greatly reduced Thus backpropagation has an easier time establishing\n",
            "communication between the source sentence and the target sentence which in turn\n",
            "results in substantially improved overall performance Initially we believed that\n",
            "reversing the input sentences would only lead to more confident predict tons in\n",
            "the early parts of the target sentence and to less confident predictions in the\n",
            "later parts How ever LSTMs trained on reversed source sentences did much better\n",
            "on long sentences than LSTMs trained on the raw source sentences see see which\n",
            "suggests that reversing the input sentences results in LSTMs with better memory\n",
            "utilization Training details We found that the LSTM models are fairly easy to\n",
            "train We used deep LSTMs with layers with cells at each layer and dimensional\n",
            "word embeddings with an input vocabulary of and an output vocabulary of Thus the\n",
            "deep LSTM uses real numbers to represent a sentence We found deep LSTMs to\n",
            "significantly outperform shallow LSTMs where each additional layer reduced\n",
            "perplexity by nearly possibly due to their much larger hidden state We used a\n",
            "naive softa over words at each output The resulting LSTM has M parameters of\n",
            "which M are pure recurrent connections M for the encoder LSTM and M for the\n",
            "decoder LSTM The complete training details are given below e We initialized all\n",
            "of the LSTM’s parameters with the uniform distribution between and e We used\n",
            "stochastic gradient descent without momentum with a fixed learning rate of After\n",
            "epochs we begun halving the learning rate every half epoch We trained our models\n",
            "for a total of epochs e We used batches of sequences for the gradient and\n",
            "divided it the size of the batch namely e Although LSTMs tend to not suffer from\n",
            "the vanishing gradient problem they can have exploding gradients Thus we\n",
            "enforced a hard constraint on the norm of the gradient by scaling it when its\n",
            "norm exceeded a threshold For each training batch we compute g where g is the\n",
            "gradient divided by If s we set g e Different sentences have different lengths\n",
            "Most sentences are short beg length but some sentences are long beg length so a\n",
            "minibatch of randomly chosen training sentences will have many short sentences\n",
            "and few long sentences and as a result much of the computation in the minibatch\n",
            "is wasted To address this problem we made sure that all sentences in a minibatch\n",
            "are roughly of the same length yielding a x speedup Parallelization A C\n",
            "implementation of deep LSTM with the configuration from the previous section on\n",
            "a sin glue GPU processes a speed of approximately words per second This was too\n",
            "slow for our purposes so we parallelized our model using an GPU machine Each\n",
            "layer of the LSTM was executed on a different GPU and communicated its\n",
            "activations to the next GPU layer as soon as they were computed Our models have\n",
            "layers of LSTMs each of which resides on a separate GPU The remaining GPUs were\n",
            "used to parallelize the softa so each GPU was responsible for multiplying by a x\n",
            "matrix The resulting implementation achieved a speed of both English and French\n",
            "words per second with a minibatch size of Training took about a ten days with\n",
            "this implementation Experimental Results We used the cased BLEU score to\n",
            "evaluate the quality of our translations We computed our BLEU scores using\n",
            "multibleup on the tokenized predictions and ground truth This way of evaluating\n",
            "the BELU score is consistent with and and reproduces the score of However if we\n",
            "evaluate the best WMT’ system whose predictions can be downloaded from\n",
            "statmtorgmatrix in this manner we get which is greater than the reported by\n",
            "statmtorgmatrix The results are presented in tables and Our best results are\n",
            "obtained with an ensemble of LSTMs that differ in their random initializations\n",
            "and in the random order of minibatches While the decoded translations of the\n",
            "LSTM ensemble do not outperform the best WMT’ system it is the first time that a\n",
            "pure neural translation system outperforms a phrasebased SMT baseline on a large\n",
            "scale MT ‘There several variants of the BLEU score and each variant is defined\n",
            "with a per script test BLEU score nasty Bahdanau et a Baseline System Single\n",
            "forward LSTM beam size Table The performance of the LSTM on WMT’ English to\n",
            "French test set nest Note that an ensemble of LSTMs with a beam of size is\n",
            "cheaper than of a single LSTM with a beam of size test BLEU score nasty Baseline\n",
            "System Cho et a Best WMT result Rescoring the baseline best with a single\n",
            "forward LSTM Rescoring the baseline best with a single reversed LSTM Rescoring\n",
            "the baseline best with an ensemble of reversed LSTMs Oracle Rescoring of the\n",
            "Baseline best lists Table Methods that use neural networks together with an SMT\n",
            "system on the WMT’ English to French test set nest task by a sizeable margin\n",
            "despite its inability to handle outofvocabulary words The LSTM is within BLEU\n",
            "points of the best WMT’ result if it is used to rescore the best list of the\n",
            "baseline system Performance on long sentences We were surprised to discover that\n",
            "the LSTM did well on long sentences which is shown quantity lively in figure\n",
            "Table presents several examples of long sentences and their translations Model\n",
            "Analysis a O was given a card by her in the garden F OMary admires John O Inthe\n",
            "garden she gave me a card O She gave me a card in the garden OMary is in love\n",
            "with John OMary respects John aL OJohn admires Mary O She was given a card by me\n",
            "in the garden OJohn is in love with Mar y i In the garden gave her a card OJohn\n",
            "respects Mary i I gave her a card in the garden L L L L J L L L L L J Figure The\n",
            "figure shows a dimensional PCA projection of the LSTM hidden states that are\n",
            "obtained after processing the phrases in the figures The phrases are clustered\n",
            "by meaning which in these examples is primarily a function of word order which\n",
            "would be difficult to capture with a bagofwords model Notice that both clusters\n",
            "have similar internal structure One of the attractive features of our model is\n",
            "its ability to turn a sequence of words into a vector of fixed dimensionality\n",
            "Figure visualizes some of the learned representations The figure clearly shows\n",
            "that the representations are sensitive to the order of words while being fairly\n",
            "insensitive to the Our model Ulrich UNK member do counsel do administration do\n",
            "constructer automobile Audi affirm qua is git do one pratique courante deputy\n",
            "does ankles pour due let telephones portables puissant the collects savant let\n",
            "reunions do counsel do administration fin qua is ne sent pas utilizes come\n",
            "apparels do cute a distance Truth Ulrich Hackenberg member do counsel do\n",
            "administration do constructer automobile Audi declare due la collect does\n",
            "telephones portables savant let reunions do counsel fin qua is ne puissant pas\n",
            "the utilizes come apparels do cute a distance best one pratique courante deputy\n",
            "does ankles Our model i Les telephones cellular's quit son raiment one question\n",
            "non element pace qua is pourraient potentiellement causer does interferences\n",
            "aver let apparels de navigation main nous salons salon la FCC qua is pourraient\n",
            "interferer aver let tours de telephone cellular lorsqu’ is son days I’ air dit\n",
            "UNK Truth i Les telephones portables son véritablement in problem non element\n",
            "pace qua is pourraient éventuellement crier does interferences aver let\n",
            "instruments de navigation main pace due nous salons do acres la FCC qua is\n",
            "pourraient perturbed let antennesrelais de telephone mobile so is son utilizes a\n",
            "word i a declare Rosenker Our model Avec la cremation i y a in i sentiment de\n",
            "violence contra we corps do in the her i quit sera i réduit a one pile de\n",
            "candles i en très per de temps a lieu do in processes de decomposition i quit\n",
            "accompagnera let tapes do devil i Truth Il y a aver la cremation i one violence\n",
            "faith a corps aim i a quit a the i credit a in was de candles i en très per de\n",
            "temps et non arms in processes de decomposition quit i accompagnerait let phases\n",
            "do devil i Table A few examples of long translations produced by the LSTM\n",
            "alongside the ground truth translations The reader can verify that the\n",
            "translations are sensible using Google translate i LSTM i LSTM eye baseline eye\n",
            "baseline Y Y i i U U in in D D a a test sentences sorted by their length test\n",
            "sentences sorted by average word frequency rank Figure The left plot shows the\n",
            "performance of our system as a function of sentence length where the axis\n",
            "corresponds to the test sentences sorted by their length and is marked by the\n",
            "actual sequence lengths There is no degradation on sentences with less than\n",
            "words there is only a minor degradation on the longest sentences The right plot\n",
            "shows the LSTM’s performance on sentences with progressively more rare words\n",
            "where the axis corresponds to the test sentences sorted by their average word\n",
            "frequency rank replacement of an active voice with a passive voice The\n",
            "tridimensional projections are obtained using PCA Related work There is a large\n",
            "body of work on applications of neural networks to machine translation So far\n",
            "the simplest and most effective way of applying an RNNLanguage Model RNNLM or a\n",
            "Feedforward Neural Network Language Model NNLM to an MT task is by restoring the\n",
            "n best lists of a strong MT baseline which reliably improves translation quality\n",
            "More recently researchers have begun to look into ways of including information\n",
            "about the source language into the NNLM Examples of this work include Auli et a\n",
            "who combine an NNLM with a topic model of the input sentence which improves\n",
            "restoring performance Devlin et a followed a similar approach but they\n",
            "incorporated their NNLM into the decoder of an MT system and used the decoder's\n",
            "alignment information to provide the NNLM with the most useful words in the\n",
            "input sentence Their approach was highly successful and it achieved large\n",
            "improvements over their baseline Our work is closely related to Kalchbrenner and\n",
            "Blunsom who were the first to map the input sentence into a vector and then back\n",
            "to a sentence although they map sentences to vectors using convolutions neural\n",
            "networks which lose the ordering of the words Similarly to this work Cho et a\n",
            "used an LSTMlike RNN architecture to map sentences into vectors and back\n",
            "although their primary focus was on integrating their neural network into an SMT\n",
            "system Bahdanau et a also attempted direct translations with a neural network\n",
            "that used an attention mechanism to overcome the poor performance on long\n",
            "sentences experienced by Cho et a and achieved encouraging results Likewise\n",
            "PougetAbadie et a attempted to address the memory problem of Cho et a by\n",
            "translating pieces of the source sentence in way that produces smooth\n",
            "translations which is similar to a phrasebased approach We suspect that they\n",
            "could achieve similar improvements by simply training their networks on reversed\n",
            "source sentences Endtoend training is also the focus of Hermann et a whose model\n",
            "represents the inputs and outputs by feedforward networks and map them to\n",
            "similar points in space However their approach cannot generate translations\n",
            "directly to get a translation they need to do a look up for closest vector in\n",
            "the precomputed database of sentences or to rescore a sentence Conclusion In\n",
            "this work we showed that a large deep LSTM that has a limited vocabulary and\n",
            "that makes almost no assumption about problem structure can outperform a\n",
            "standard SMTbased system whose vocabulary is unlimited on a largescale MT task\n",
            "The success of our simple LSTMbased approach on MT suggests that it should do\n",
            "well on many other sequence learning problems provided they have enough training\n",
            "data We were surprised by the extent of the improvement obtained by reversing\n",
            "the words in the source sentences We conclude that it is important to find a\n",
            "problem encoding that has the greatest number of short term dependencies as they\n",
            "make the learning problem much simpler In particular while we were unable to\n",
            "train a standard RNN on the nonreversed translation problem shown in fig we\n",
            "believe that a standard RNN should be easily trainable when the source sentences\n",
            "are reversed although we did not verify it experimentally We were also surprised\n",
            "by the ability of the LSTM to correctly translate very long sentences We were\n",
            "initially convinced that the LSTM would fail on long sentences due to its\n",
            "limited memory and other researchers reported poor performance on long sentences\n",
            "with a model similar to ours And yet LSTMs trained on the reversed dataset had\n",
            "little difficulty translating long sentences Most importantly we demonstrated\n",
            "that a simple straightforward and a relatively optimized a preach can outperform\n",
            "an SMT system so further work will likely lead to even greater translation\n",
            "accuracies These results suggest that our approach will likely do well on other\n",
            "challenging sequence to sequence problems Acknowledgments We thank Samy Bengio\n",
            "Jeff Dean Matthieu Devin Geoffrey Hinton Nal Kalchbrenner Thang Luong Wolf gang\n",
            "Macherey Rajat Monga Vincent Vanhoucke Peng Xu Wojciech Zaremba and the Google\n",
            "Brain team for useful comments and discussions References M Auli M Galley C\n",
            "Quirk and G Zweig Joint language and translation modeling with recurrent neural\n",
            "networks In EMNLP D Bahdanau K Cho and Y Bengio Neural machine translation by\n",
            "jointly learning to align and translate arXiv reprint arXiv Y Bengio R Ducharme\n",
            "P Vincent and C Jauvin A neural probabilistic language model In Journal of\n",
            "Machine Learning Research pages Y Bengio P Simard and P Frasconi Learning longer\n",
            "dependencies with gradient descent is difficult IEEE Transactions on Neural\n",
            "Networks i K Cho B Merrienboer C Gulcehre F Bougares H Schwenk and Y Bengio\n",
            "Learning phrase represent nations using RNN encoderdecoder for statistical\n",
            "machine translation In Arxiv reprint arXiv D Ciresan U Meier and J Schmidhuber\n",
            "Multicolumn deep neural networks for image classification In CVPR G E Dahl D Yu\n",
            "L Deng and A Acero Contextdependent retrained deep neural networks for large\n",
            "vocabulary speech recognition IEEE Transactions on Audio Speech and Language\n",
            "Processing Special Issue on Deep Learning for Speech and Language Processing J\n",
            "Devlin R Zbib Z Huang T Lamar R Schwartz and J Makhoul Fast and robust neural\n",
            "network joint models for statistical machine translation In ACL Nadir Durrani\n",
            "Barry Haddow Philipp Koehn and Kenneth Heafield Edinburgh’s phrasebased machine\n",
            "translation systems for wet In WMT A Graves Generating sequences with recurrent\n",
            "neural networks In Arxiv reprint arXiv A Graves S Fernandez F Gomez and J\n",
            "Schmidhuber Connectionist temporal classification labelling unsegmented sequence\n",
            "data with recurrent neural networks In ICML K M Hermann and P Blunsom\n",
            "Multilingual distributed representations without word alignment In ICLR G Hinton\n",
            "L Deng D Yu G Dahl A Mohamed N Jaitly A Senior V Vanhoucke P Nguyen T Sainath\n",
            "and B Kingsbury Deep neural networks for acoustic modeling in speech recognition\n",
            "IEEE Signal Processing Magazine S Hochreiter Untersuchungen u dynamischen\n",
            "neuronal nether Master’s thesis Institut fur Infor batik Technische Universitat\n",
            "Munchen S Hochreiter Y Bengio P Frasconi and J Schmidhuber Gradient flow in\n",
            "recurrent nets the difficulty of learning longer dependencies S Hochreiter and J\n",
            "Schmidhuber Long shorter memory Neural Computation S Hochreiter and J\n",
            "Schmidhuber LSTM can solve hard long time lag problems N Kalchbrenner and P\n",
            "Blunsom Recurrent continuous translation models In EMNLP A Krizhevsky I\n",
            "Sutskever and G E Hinton ImageNet classification with deep convolutions neural\n",
            "networks In NIPS QV Le MA Ranzato R Monga M Devin K Chen GS Corrado J Dean and\n",
            "AY Ng Building highlevel features using large scale unsupervised learning In\n",
            "ICML Y LeCun L Bottou Y Bengio and P Haffner Gradientbased learning applied to\n",
            "document recognition Proceedings of the IEEE T Mikolov Statistical Language\n",
            "Models based on Neural Networks PhD thesis Brno University of Technology T\n",
            "Mikolov M Karafiat L Burget J Cernocky and S Khudanpur Recurrent neural network\n",
            "based language model In INTERSPEECH pages K Papineni S Roukos T Ward and W J Zhu\n",
            "BLEU a method for automatic evaluation of machine translation In ACL R Pascanu T\n",
            "Mikolov and Y Bengio On the difficulty of training recurrent neural networks\n",
            "arXiv reprint arXiv J PougetAbadie D Bahdanau B van Merrienboer K Cho and Y\n",
            "Bengio Overcoming the curse of sentence length for neural machine translation\n",
            "using automatic segmentation arXiv reprint arXiv A Razborov On small depth\n",
            "threshold circuits In Proc red Scandinavian Workshop on Algorithm Theory D\n",
            "Rumelhart G E Hinton and R J Williams Learning representations by\n",
            "backpropagating errors Nature i H Schwenk University we mans\n",
            "httpwwwliumunivlemansfrschwenkcslm jointpaper Online accessed September M\n",
            "Sundermeyer R Schluter and H Ney LSTM neural networks for language modeling In\n",
            "INTER SPEECH P Werbos Backpropagation through time what it does and how to do it\n",
            "Proceedings of IEEE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the words\n",
        "from gensim.utils import tokenize\n",
        "\n",
        "gensim_tokens = list(tokenize(result_text))\n",
        "print(len(gensim_tokens))\n",
        "print(gensim_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SYtLjfLkp_b",
        "outputId": "9c56befa-d0bb-4cba-9202-b1843f36a3df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5189\n",
            "['v', 'csCL', 'Dec', 'V', 'arxX', 'Sequence', 'to', 'Sequence', 'Learning', 'with', 'Neural', 'Networks', 'Ilya', 'Sutskever', 'Oriol', 'Vinyals', 'Quoc', 'V', 'Le', 'Google', 'Google', 'Google', 'ilyasugooglecom', 'vinyalsgooglecom', 'qvlgooglecom', 'Abstract', 'Deep', 'Neural', 'Networks', 'DNNs', 'are', 'powerful', 'models', 'that', 'have', 'achieved', 'excel', 'lent', 'performance', 'on', 'difficult', 'learning', 'tasks', 'Although', 'DNNs', 'work', 'well', 'whenever', 'large', 'labeled', 'training', 'sets', 'are', 'available', 'they', 'cannot', 'be', 'used', 'to', 'map', 'sequences', 'to', 'sequences', 'In', 'this', 'paper', 'we', 'present', 'a', 'general', 'endtoend', 'approach', 'to', 'sequence', 'learning', 'that', 'makes', 'minimal', 'assumptions', 'on', 'the', 'sequence', 'structure', 'Our', 'method', 'uses', 'a', 'multilayered', 'Long', 'ShortTerm', 'Memory', 'LSTM', 'to', 'map', 'the', 'input', 'sequence', 'to', 'a', 'vector', 'of', 'a', 'fixed', 'dimensionality', 'and', 'then', 'another', 'deep', 'LSTM', 'to', 'decode', 'the', 'target', 'sequence', 'from', 'the', 'vector', 'Our', 'main', 'result', 'is', 'that', 'on', 'an', 'English', 'to', 'French', 'translation', 'task', 'from', 'the', 'WMT', 'dataset', 'the', 'translations', 'produced', 'by', 'the', 'LSTM', 'achieve', 'a', 'BLEU', 'score', 'of', 'on', 'the', 'entire', 'test', 'set', 'where', 'the', 'LSTM', 's', 'BLEU', 'score', 'was', 'penalized', 'on', 'outofvocabulary', 'words', 'Additionally', 'the', 'LSTM', 'did', 'not', 'have', 'difficulty', 'on', 'long', 'sentences', 'For', 'comparison', 'a', 'phrasebased', 'SMT', 'system', 'achieves', 'a', 'BLEU', 'score', 'of', 'on', 'the', 'same', 'dataset', 'When', 'we', 'used', 'the', 'LSTM', 'to', 'rerank', 'the', 'hypotheses', 'produced', 'by', 'the', 'aforementioned', 'SMT', 'system', 'its', 'BLEU', 'score', 'increases', 'to', 'which', 'is', 'close', 'to', 'the', 'previous', 'best', 'result', 'on', 'this', 'task', 'The', 'LSTM', 'also', 'learned', 'sensible', 'phrase', 'and', 'sentence', 'representations', 'that', 'are', 'sensitive', 'to', 'word', 'order', 'and', 'are', 'relatively', 'invariant', 'to', 'the', 'active', 'and', 'the', 'pas', 'sive', 'voice', 'Finally', 'we', 'found', 'that', 'reversing', 'the', 'order', 'of', 'the', 'words', 'in', 'all', 'source', 'sentences', 'but', 'not', 'target', 'sentences', 'improved', 'the', 'LSTM', 's', 'performance', 'markedly', 'because', 'doing', 'so', 'introduced', 'many', 'short', 'term', 'dependencies', 'between', 'the', 'source', 'and', 'the', 'target', 'sentence', 'which', 'made', 'the', 'optimization', 'problem', 'easier', 'Introduction', 'Deep', 'Neural', 'Networks', 'DNNs', 'are', 'extremely', 'powerful', 'machine', 'learning', 'models', 'that', 'achieve', 'ex', 'cellent', 'performance', 'on', 'difficult', 'problems', 'such', 'as', 'speech', 'recognition', 'and', 'visual', 'object', 'recog', 'nition', 'DNNs', 'are', 'powerful', 'because', 'they', 'can', 'perform', 'arbitrary', 'parallel', 'computation', 'for', 'a', 'modest', 'number', 'of', 'steps', 'A', 'surprising', 'example', 'of', 'the', 'power', 'of', 'DNNs', 'is', 'their', 'ability', 'to', 'sort', 'N', 'WNbit', 'numbers', 'using', 'only', 'hidden', 'layers', 'of', 'quadratic', 'size', 'So', 'while', 'neural', 'networks', 'are', 'related', 'to', 'conventional', 'statistical', 'models', 'they', 'learn', 'an', 'intricate', 'computation', 'Furthermore', 'large', 'DNNs', 'can', 'be', 'trained', 'with', 'supervised', 'backpropagation', 'whenever', 'the', 'labeled', 'training', 'set', 'has', 'enough', 'information', 'to', 'specify', 'the', 'network', 's', 'parameters', 'Thus', 'if', 'there', 'exists', 'a', 'parameter', 'setting', 'of', 'a', 'large', 'DNN', 'that', 'achieves', 'good', 'results', 'for', 'example', 'because', 'humans', 'can', 'solve', 'the', 'task', 'very', 'rapidly', 'supervised', 'backpropagation', 'will', 'find', 'these', 'parameters', 'and', 'solve', 'the', 'problem', 'Despite', 'their', 'flexibility', 'and', 'power', 'DNNs', 'can', 'only', 'be', 'applied', 'to', 'problems', 'whose', 'inputs', 'and', 'targets', 'can', 'be', 'sensibly', 'encoded', 'with', 'vectors', 'of', 'fixed', 'dimensionality', 'It', 'is', 'a', 'significant', 'limitation', 'since', 'many', 'important', 'problems', 'are', 'best', 'expressed', 'with', 'sequences', 'whose', 'lengths', 'are', 'not', 'known', 'apriori', 'For', 'example', 'speech', 'recognition', 'and', 'machine', 'translation', 'are', 'sequential', 'problems', 'Likewise', 'ques', 'tion', 'answering', 'can', 'also', 'be', 'seen', 'as', 'mapping', 'a', 'sequence', 'of', 'words', 'representing', 'the', 'question', 'to', 'a', 'sequence', 'of', 'words', 'representing', 'the', 'answer', 'It', 'is', 'therefore', 'clear', 'that', 'a', 'domainindependent', 'method', 'that', 'learns', 'to', 'map', 'sequences', 'to', 'sequences', 'would', 'be', 'useful', 'Sequences', 'pose', 'a', 'challenge', 'for', 'DNNs', 'because', 'they', 'require', 'that', 'the', 'dimensionality', 'of', 'the', 'inputs', 'and', 'outputs', 'is', 'known', 'and', 'fixed', 'In', 'this', 'paper', 'we', 'show', 'that', 'a', 'straightforward', 'application', 'of', 'the', 'Long', 'ShortTerm', 'Memory', 'LSTM', 'architecture', 'can', 'solve', 'general', 'sequence', 'to', 'sequence', 'problems', 'The', 'idea', 'is', 'to', 'use', 'one', 'LSTM', 'to', 'read', 'the', 'input', 'sequence', 'one', 'timestep', 'at', 'a', 'time', 'to', 'obtain', 'large', 'fixed', 'dimensional', 'vector', 'representation', 'and', 'then', 'to', 'use', 'another', 'LSTM', 'to', 'extract', 'the', 'output', 'sequence', 'from', 'that', 'vector', 'fig', 'The', 'second', 'LSTM', 'is', 'essentially', 'a', 'recurrent', 'neural', 'network', 'language', 'model', 'except', 'that', 'it', 'is', 'conditioned', 'on', 'the', 'input', 'sequence', 'The', 'LSTM', 's', 'ability', 'to', 'successfully', 'learn', 'on', 'data', 'with', 'long', 'range', 'temporal', 'dependencies', 'makes', 'it', 'a', 'natural', 'choice', 'for', 'this', 'application', 'due', 'to', 'the', 'considerable', 'time', 'lag', 'between', 'the', 'inputs', 'and', 'their', 'corresponding', 'outputs', 'fig', 'There', 'have', 'been', 'a', 'number', 'of', 'related', 'attempts', 'to', 'address', 'the', 'general', 'sequence', 'to', 'sequence', 'learning', 'problem', 'with', 'neural', 'networks', 'Our', 'approach', 'is', 'closely', 'related', 'to', 'Kalchbrenner', 'and', 'Blunsom', 'who', 'were', 'the', 'first', 'to', 'map', 'the', 'entire', 'input', 'sentence', 'to', 'vector', 'and', 'is', 'related', 'to', 'Cho', 'et', 'al', 'although', 'the', 'latter', 'was', 'used', 'only', 'for', 'rescoring', 'hypotheses', 'produced', 'by', 'a', 'phrasebased', 'system', 'Graves', 'introduced', 'a', 'novel', 'differentiable', 'attention', 'mechanism', 'that', 'allows', 'neural', 'networks', 'to', 'focus', 'on', 'dif', 'ferent', 'parts', 'of', 'their', 'input', 'and', 'an', 'elegant', 'variant', 'of', 'this', 'idea', 'was', 'successfully', 'applied', 'to', 'machine', 'translation', 'by', 'Bahdanau', 'et', 'al', 'The', 'Connectionist', 'Sequence', 'Classification', 'is', 'another', 'popular', 'technique', 'for', 'mapping', 'sequences', 'to', 'sequences', 'with', 'neural', 'networks', 'but', 'it', 'assumes', 'a', 'monotonic', 'alignment', 'between', 'the', 'inputs', 'and', 'the', 'outputs', 'W', 'X', 'Y', 'Z', 'EOS', 'A', 'B', 'C', 'EOS', 'W', 'X', 'Y', 'Z', 'Figure', 'Our', 'model', 'reads', 'an', 'input', 'sentence', 'ABC', 'and', 'produces', 'WXYZ', 'as', 'the', 'output', 'sentence', 'The', 'model', 'stops', 'making', 'predictions', 'after', 'outputting', 'the', 'endofsentence', 'token', 'Note', 'that', 'the', 'LSTM', 'reads', 'the', 'input', 'sentence', 'in', 'reverse', 'because', 'doing', 'so', 'introduces', 'many', 'short', 'term', 'dependencies', 'in', 'the', 'data', 'that', 'make', 'the', 'optimization', 'problem', 'much', 'easier', 'The', 'main', 'result', 'of', 'this', 'work', 'is', 'the', 'following', 'On', 'the', 'WMT', 'English', 'to', 'French', 'translation', 'task', 'we', 'obtained', 'a', 'BLEU', 'score', 'of', 'by', 'directly', 'extracting', 'translations', 'from', 'an', 'ensemble', 'of', 'deep', 'LSTMs', 'with', 'M', 'parameters', 'and', 'dimensional', 'state', 'each', 'using', 'a', 'simple', 'lefttoright', 'beam', 'search', 'decoder', 'This', 'is', 'by', 'far', 'the', 'best', 'result', 'achieved', 'by', 'direct', 'translation', 'with', 'large', 'neural', 'net', 'works', 'For', 'comparison', 'the', 'BLEU', 'score', 'of', 'an', 'SMT', 'baseline', 'on', 'this', 'dataset', 'is', 'The', 'BLEU', 'score', 'was', 'achieved', 'by', 'an', 'LSTM', 'with', 'a', 'vocabulary', 'of', 'k', 'words', 'so', 'the', 'score', 'was', 'penalized', 'whenever', 'the', 'reference', 'translation', 'contained', 'a', 'word', 'not', 'covered', 'by', 'these', 'k', 'This', 'result', 'shows', 'that', 'a', 'relatively', 'unoptimized', 'smallvocabulary', 'neural', 'network', 'architecture', 'which', 'has', 'much', 'room', 'for', 'improvement', 'outperforms', 'a', 'phrasebased', 'SMT', 'system', 'Finally', 'we', 'used', 'the', 'LSTM', 'to', 'rescore', 'the', 'publicly', 'available', 'best', 'lists', 'of', 'the', 'SMT', 'baseline', 'on', 'the', 'same', 'task', 'By', 'doing', 'so', 'we', 'obtained', 'a', 'BLEU', 'score', 'of', 'which', 'improves', 'the', 'baseline', 'by', 'BLEU', 'points', 'and', 'is', 'close', 'to', 'the', 'previous', 'best', 'published', 'result', 'on', 'this', 'task', 'which', 'is', 'Surprisingly', 'the', 'LSTM', 'did', 'not', 'suffer', 'on', 'very', 'long', 'sentences', 'despite', 'the', 'recent', 'experience', 'of', 'other', 'researchers', 'with', 'related', 'architectures', 'We', 'were', 'able', 'to', 'do', 'well', 'on', 'long', 'sentences', 'because', 'we', 'reversed', 'the', 'order', 'of', 'words', 'in', 'the', 'source', 'sentence', 'but', 'not', 'the', 'target', 'sentences', 'in', 'the', 'training', 'and', 'test', 'set', 'By', 'doing', 'so', 'we', 'introduced', 'many', 'short', 'term', 'dependencies', 'that', 'made', 'the', 'optimization', 'problem', 'much', 'simpler', 'see', 'sec', 'and', 'As', 'a', 'result', 'SGD', 'could', 'learn', 'LSTMs', 'that', 'had', 'no', 'trouble', 'with', 'long', 'sentences', 'The', 'simple', 'trick', 'of', 'reversing', 'the', 'words', 'in', 'the', 'source', 'sentence', 'is', 'one', 'of', 'the', 'key', 'technical', 'contributions', 'of', 'this', 'work', 'A', 'useful', 'property', 'of', 'the', 'LSTM', 'is', 'that', 'it', 'learns', 'to', 'map', 'an', 'input', 'sentence', 'of', 'variable', 'length', 'into', 'a', 'fixeddimensional', 'vector', 'representation', 'Given', 'that', 'translations', 'tend', 'to', 'be', 'paraphrases', 'of', 'the', 'source', 'sentences', 'the', 'translation', 'objective', 'encourages', 'the', 'LSTM', 'to', 'find', 'sentence', 'representations', 'that', 'capture', 'their', 'meaning', 'as', 'sentences', 'with', 'similar', 'meanings', 'are', 'close', 'to', 'each', 'other', 'while', 'different', 'sentences', 'meanings', 'will', 'be', 'far', 'A', 'qualitative', 'evaluation', 'supports', 'this', 'claim', 'showing', 'that', 'our', 'model', 'is', 'aware', 'of', 'word', 'order', 'and', 'is', 'fairly', 'invariant', 'to', 'the', 'active', 'and', 'passive', 'voice', 'The', 'model', 'The', 'Recurrent', 'Neural', 'Network', 'RNN', 'is', 'a', 'natural', 'generalization', 'of', 'feedforward', 'neural', 'networks', 'to', 'sequences', 'Given', 'a', 'sequence', 'of', 'inputs', 'a', 'standard', 'RNN', 'computes', 'a', 'sequence', 'of', 'outputs', 'y', 'yr', 'by', 'iterating', 'the', 'following', 'equation', 'h', 'sigm', 'Wa', 'W', 'he', 'Ye', 'Whe', 'The', 'RNN', 'can', 'easily', 'map', 'sequences', 'to', 'sequences', 'whenever', 'the', 'alignment', 'between', 'the', 'inputs', 'the', 'outputs', 'is', 'known', 'ahead', 'of', 'time', 'However', 'it', 'is', 'not', 'clear', 'how', 'to', 'apply', 'an', 'RNN', 'to', 'problems', 'whose', 'input', 'and', 'the', 'output', 'sequences', 'have', 'different', 'lengths', 'with', 'complicated', 'and', 'nonmonotonic', 'relation', 'ships', 'The', 'simplest', 'strategy', 'for', 'general', 'sequence', 'learning', 'is', 'to', 'map', 'the', 'input', 'sequence', 'to', 'a', 'fixedsized', 'vector', 'using', 'one', 'RNN', 'and', 'then', 'to', 'map', 'the', 'vector', 'to', 'the', 'target', 'sequence', 'with', 'another', 'RNN', 'this', 'approach', 'has', 'also', 'been', 'taken', 'by', 'Cho', 'et', 'al', 'While', 'it', 'could', 'work', 'in', 'principle', 'since', 'the', 'RNN', 'is', 'provided', 'with', 'all', 'the', 'relevant', 'information', 'it', 'would', 'be', 'difficult', 'to', 'train', 'the', 'RNNs', 'due', 'to', 'the', 'resulting', 'long', 'term', 'dependencies', 'figure', 'However', 'the', 'Long', 'ShortTerm', 'Memory', 'LSTM', 'is', 'known', 'to', 'learn', 'problems', 'with', 'long', 'range', 'temporal', 'dependencies', 'so', 'an', 'LSTM', 'may', 'succeed', 'in', 'this', 'setting', 'The', 'goal', 'of', 'the', 'LSTM', 'is', 'to', 'estimate', 'the', 'conditional', 'probability', 'pyy', 'where', 'v', 'is', 'an', 'input', 'sequence', 'and', 'y', 'yr', 'is', 'its', 'corresponding', 'output', 'sequence', 'whose', 'length', 'T', 'may', 'differ', 'from', 'The', 'LSTM', 'computes', 'this', 'conditional', 'probability', 'by', 'first', 'obtaining', 'the', 'fixed', 'dimensional', 'representation', 'v', 'of', 'the', 'input', 'sequence', 'given', 'by', 'the', 'last', 'hidden', 'state', 'of', 'the', 'LSTM', 'and', 'then', 'computing', 'the', 'probability', 'of', 'yy', 'with', 'a', 'standard', 'LSTMLM', 'formulation', 'whose', 'initial', 'hidden', 'state', 'is', 'set', 'to', 'the', 'representation', 'v', 'of', 'T', 'Pyyreier', 'viyelos', 'yi', 'yea', 't', 'In', 'this', 'equation', 'each', 'pyzv', 'y', 'Yz', 'distribution', 'is', 'represented', 'with', 'a', 'softmax', 'over', 'all', 'the', 'words', 'in', 'the', 'vocabulary', 'We', 'use', 'the', 'LSTM', 'formulation', 'from', 'Graves', 'Note', 'that', 'we', 'require', 'that', 'each', 'sentence', 'ends', 'with', 'a', 'special', 'endofsentence', 'symbol', 'EOS', 'which', 'enables', 'the', 'model', 'to', 'define', 'a', 'distribution', 'over', 'sequences', 'of', 'all', 'possible', 'lengths', 'The', 'overall', 'scheme', 'is', 'outlined', 'in', 'figure', 'where', 'the', 'shown', 'LSTM', 'computes', 'the', 'representation', 'of', 'A', 'B', 'C', 'EOS', 'and', 'then', 'uses', 'this', 'representation', 'to', 'compute', 'the', 'probability', 'of', 'W', 'X', 'Y', 'Z', 'EOS', 'Our', 'actual', 'models', 'differ', 'from', 'the', 'above', 'description', 'in', 'three', 'important', 'ways', 'First', 'we', 'used', 'two', 'different', 'LSTMs', 'one', 'for', 'the', 'input', 'sequence', 'and', 'another', 'for', 'the', 'output', 'sequence', 'because', 'doing', 'so', 'increases', 'the', 'number', 'model', 'parameters', 'at', 'negligible', 'computational', 'cost', 'and', 'makes', 'it', 'natural', 'to', 'train', 'the', 'LSTM', 'on', 'multiple', 'language', 'pairs', 'simultaneously', 'Second', 'we', 'found', 'that', 'deep', 'LSTMs', 'significantly', 'outperformed', 'shallow', 'LSTMs', 'so', 'we', 'chose', 'an', 'LSTM', 'with', 'four', 'layers', 'Third', 'we', 'found', 'it', 'extremely', 'valuable', 'to', 'reverse', 'the', 'order', 'of', 'the', 'words', 'of', 'the', 'input', 'sentence', 'So', 'for', 'example', 'instead', 'of', 'mapping', 'the', 'sentence', 'a', 'bc', 'to', 'the', 'sentence', 'a', 'y', 'the', 'LSTM', 'is', 'asked', 'to', 'map', 'c', 'b', 'a', 'to', 'a', 'where', 'a', 'is', 'the', 'translation', 'of', 'a', 'b', 'c', 'This', 'way', 'a', 'is', 'in', 'close', 'proximity', 'to', 'a', 'b', 'is', 'fairly', 'close', 'to', 'and', 'so', 'on', 'a', 'fact', 'that', 'makes', 'it', 'easy', 'for', 'SGD', 'to', 'establish', 'communication', 'between', 'the', 'input', 'and', 'the', 'output', 'We', 'found', 'this', 'simple', 'data', 'transformation', 'to', 'greatly', 'improve', 'the', 'performance', 'of', 'the', 'LSTM', 'Experiments', 'We', 'applied', 'our', 'method', 'to', 'the', 'WMT', 'English', 'to', 'French', 'MT', 'task', 'in', 'two', 'ways', 'We', 'used', 'it', 'to', 'directly', 'translate', 'the', 'input', 'sentence', 'without', 'using', 'a', 'reference', 'SMT', 'system', 'and', 'we', 'it', 'to', 'rescore', 'the', 'nbest', 'lists', 'of', 'an', 'SMT', 'baseline', 'We', 'report', 'the', 'accuracy', 'of', 'these', 'translation', 'methods', 'present', 'sample', 'translations', 'and', 'visualize', 'the', 'resulting', 'sentence', 'representation', 'Dataset', 'details', 'We', 'used', 'the', 'WMT', 'English', 'to', 'French', 'dataset', 'We', 'trained', 'our', 'models', 'on', 'a', 'subset', 'of', 'M', 'sen', 'tences', 'consisting', 'of', 'M', 'French', 'words', 'and', 'M', 'English', 'words', 'which', 'is', 'a', 'clean', 'selected', 'subset', 'from', 'We', 'chose', 'this', 'translation', 'task', 'and', 'this', 'specific', 'training', 'set', 'subset', 'because', 'of', 'the', 'public', 'availability', 'of', 'a', 'tokenized', 'training', 'and', 'test', 'set', 'together', 'with', 'best', 'lists', 'from', 'the', 'baseline', 'SMT', 'As', 'typical', 'neural', 'language', 'models', 'rely', 'on', 'a', 'vector', 'representation', 'for', 'each', 'word', 'we', 'used', 'a', 'fixed', 'vocabulary', 'for', 'both', 'languages', 'We', 'used', 'of', 'the', 'most', 'frequent', 'words', 'for', 'the', 'source', 'language', 'and', 'of', 'the', 'most', 'frequent', 'words', 'for', 'the', 'target', 'language', 'Every', 'outofvocabulary', 'word', 'was', 'replaced', 'with', 'a', 'special', 'UNK', 'token', 'Decoding', 'and', 'Rescoring', 'The', 'core', 'of', 'our', 'experiments', 'involved', 'training', 'a', 'large', 'deep', 'LSTM', 'on', 'many', 'sentence', 'pairs', 'We', 'trained', 'it', 'by', 'maximizing', 'the', 'log', 'probability', 'of', 'a', 'correct', 'translation', 'T', 'given', 'the', 'source', 'sentence', 'S', 'so', 'the', 'training', 'objective', 'is', 'S', 'S', 'logpTS', 'TSES', 'where', 'S', 'is', 'the', 'training', 'set', 'Once', 'training', 'is', 'complete', 'we', 'produce', 'translations', 'by', 'finding', 'the', 'most', 'likely', 'translation', 'according', 'to', 'the', 'LSTM', 'A', 'T', 'arg', 'max', 'pTS', 'We', 'search', 'for', 'the', 'most', 'likely', 'translation', 'using', 'a', 'simple', 'lefttoright', 'beam', 'search', 'decoder', 'which', 'maintains', 'a', 'small', 'number', 'B', 'of', 'partial', 'hypotheses', 'where', 'a', 'partial', 'hypothesis', 'is', 'a', 'prefix', 'of', 'some', 'translation', 'At', 'each', 'timestep', 'we', 'extend', 'each', 'partial', 'hypothesis', 'in', 'the', 'beam', 'with', 'every', 'possible', 'word', 'in', 'the', 'vocabulary', 'This', 'greatly', 'increases', 'the', 'number', 'of', 'the', 'hypotheses', 'so', 'we', 'discard', 'all', 'but', 'the', 'B', 'most', 'likely', 'hypotheses', 'according', 'to', 'the', 'model', 's', 'log', 'probability', 'As', 'soon', 'as', 'the', 'EOS', 'symbol', 'is', 'appended', 'to', 'a', 'hypothesis', 'it', 'is', 'removed', 'from', 'the', 'beam', 'and', 'is', 'added', 'to', 'the', 'set', 'of', 'complete', 'hypotheses', 'While', 'this', 'decoder', 'is', 'approximate', 'it', 'is', 'simple', 'to', 'implement', 'Interestingly', 'our', 'system', 'performs', 'well', 'even', 'with', 'a', 'beam', 'size', 'of', 'and', 'a', 'beam', 'of', 'size', 'provides', 'most', 'of', 'the', 'benefits', 'of', 'beam', 'search', 'Table', 'We', 'also', 'used', 'the', 'LSTM', 'to', 'rescore', 'the', 'best', 'lists', 'produced', 'by', 'the', 'baseline', 'system', 'To', 'rescore', 'an', 'nbest', 'list', 'we', 'computed', 'the', 'log', 'probability', 'of', 'every', 'hypothesis', 'with', 'our', 'LSTM', 'and', 'took', 'an', 'even', 'average', 'with', 'their', 'score', 'and', 'the', 'LSTM', 's', 'score', 'Reversing', 'the', 'Source', 'Sentences', 'While', 'the', 'LSTM', 'is', 'capable', 'of', 'solving', 'problems', 'with', 'long', 'term', 'dependencies', 'we', 'discovered', 'that', 'the', 'LSTM', 'learns', 'much', 'better', 'when', 'the', 'source', 'sentences', 'are', 'reversed', 'the', 'target', 'sentences', 'are', 'not', 'reversed', 'By', 'doing', 'so', 'the', 'LSTM', 's', 'test', 'perplexity', 'dropped', 'from', 'to', 'and', 'the', 'test', 'BLEU', 'scores', 'of', 'its', 'decoded', 'translations', 'increased', 'from', 'to', 'While', 'we', 'do', 'not', 'have', 'a', 'complete', 'explanation', 'to', 'this', 'phenomenon', 'we', 'believe', 'that', 'it', 'is', 'caused', 'by', 'the', 'introduction', 'of', 'many', 'short', 'term', 'dependencies', 'to', 'the', 'dataset', 'Normally', 'when', 'we', 'concatenate', 'a', 'source', 'sentence', 'with', 'a', 'target', 'sentence', 'each', 'word', 'in', 'the', 'source', 'sentence', 'is', 'far', 'from', 'its', 'corresponding', 'word', 'in', 'the', 'target', 'sentence', 'As', 'a', 'result', 'the', 'problem', 'has', 'a', 'large', 'minimal', 'time', 'lag', 'By', 'reversing', 'the', 'words', 'in', 'the', 'source', 'sentence', 'the', 'average', 'distance', 'between', 'corresponding', 'words', 'in', 'the', 'source', 'and', 'target', 'language', 'is', 'unchanged', 'However', 'the', 'first', 'few', 'words', 'in', 'the', 'source', 'language', 'are', 'now', 'very', 'close', 'to', 'the', 'first', 'few', 'words', 'in', 'the', 'target', 'language', 'so', 'the', 'problem', 's', 'minimal', 'time', 'lag', 'is', 'greatly', 'reduced', 'Thus', 'backpropagation', 'has', 'an', 'easier', 'time', 'establishing', 'communication', 'between', 'the', 'source', 'sentence', 'and', 'the', 'target', 'sentence', 'which', 'in', 'turn', 'results', 'in', 'substantially', 'improved', 'overall', 'performance', 'Initially', 'we', 'believed', 'that', 'reversing', 'the', 'input', 'sentences', 'would', 'only', 'lead', 'to', 'more', 'confident', 'predic', 'tions', 'in', 'the', 'early', 'parts', 'of', 'the', 'target', 'sentence', 'and', 'to', 'less', 'confident', 'predictions', 'in', 'the', 'later', 'parts', 'How', 'ever', 'LSTMs', 'trained', 'on', 'reversed', 'source', 'sentences', 'did', 'much', 'better', 'on', 'long', 'sentences', 'than', 'LSTMs', 'trained', 'on', 'the', 'raw', 'source', 'sentences', 'see', 'sec', 'which', 'suggests', 'that', 'reversing', 'the', 'input', 'sentences', 'results', 'in', 'LSTMs', 'with', 'better', 'memory', 'utilization', 'Training', 'details', 'We', 'found', 'that', 'the', 'LSTM', 'models', 'are', 'fairly', 'easy', 'to', 'train', 'We', 'used', 'deep', 'LSTMs', 'with', 'layers', 'with', 'cells', 'at', 'each', 'layer', 'and', 'dimensional', 'word', 'embeddings', 'with', 'an', 'input', 'vocabulary', 'of', 'and', 'an', 'output', 'vocabulary', 'of', 'Thus', 'the', 'deep', 'LSTM', 'uses', 'real', 'numbers', 'to', 'represent', 'a', 'sentence', 'We', 'found', 'deep', 'LSTMs', 'to', 'significantly', 'outperform', 'shallow', 'LSTMs', 'where', 'each', 'additional', 'layer', 'reduced', 'perplexity', 'by', 'nearly', 'possibly', 'due', 'to', 'their', 'much', 'larger', 'hidden', 'state', 'We', 'used', 'a', 'naive', 'softmax', 'over', 'words', 'at', 'each', 'output', 'The', 'resulting', 'LSTM', 'has', 'M', 'parameters', 'of', 'which', 'M', 'are', 'pure', 'recurrent', 'connections', 'M', 'for', 'the', 'encoder', 'LSTM', 'and', 'M', 'for', 'the', 'decoder', 'LSTM', 'The', 'complete', 'training', 'details', 'are', 'given', 'below', 'e', 'We', 'initialized', 'all', 'of', 'the', 'LSTM', 's', 'parameters', 'with', 'the', 'uniform', 'distribution', 'between', 'and', 'e', 'We', 'used', 'stochastic', 'gradient', 'descent', 'without', 'momentum', 'with', 'a', 'fixed', 'learning', 'rate', 'of', 'After', 'epochs', 'we', 'begun', 'halving', 'the', 'learning', 'rate', 'every', 'half', 'epoch', 'We', 'trained', 'our', 'models', 'for', 'a', 'total', 'of', 'epochs', 'e', 'We', 'used', 'batches', 'of', 'sequences', 'for', 'the', 'gradient', 'and', 'divided', 'it', 'the', 'size', 'of', 'the', 'batch', 'namely', 'e', 'Although', 'LSTMs', 'tend', 'to', 'not', 'suffer', 'from', 'the', 'vanishing', 'gradient', 'problem', 'they', 'can', 'have', 'exploding', 'gradients', 'Thus', 'we', 'enforced', 'a', 'hard', 'constraint', 'on', 'the', 'norm', 'of', 'the', 'gradient', 'by', 'scaling', 'it', 'when', 'its', 'norm', 'exceeded', 'a', 'threshold', 'For', 'each', 'training', 'batch', 'we', 'compute', 'g', 'where', 'g', 'is', 'the', 'gradient', 'divided', 'by', 'If', 's', 'we', 'set', 'g', 'e', 'Different', 'sentences', 'have', 'different', 'lengths', 'Most', 'sentences', 'are', 'short', 'eg', 'length', 'but', 'some', 'sentences', 'are', 'long', 'eg', 'length', 'so', 'a', 'minibatch', 'of', 'randomly', 'chosen', 'training', 'sentences', 'will', 'have', 'many', 'short', 'sentences', 'and', 'few', 'long', 'sentences', 'and', 'as', 'a', 'result', 'much', 'of', 'the', 'computation', 'in', 'the', 'minibatch', 'is', 'wasted', 'To', 'address', 'this', 'problem', 'we', 'made', 'sure', 'that', 'all', 'sentences', 'in', 'a', 'minibatch', 'are', 'roughly', 'of', 'the', 'same', 'length', 'yielding', 'a', 'x', 'speedup', 'Parallelization', 'A', 'C', 'implementation', 'of', 'deep', 'LSTM', 'with', 'the', 'configuration', 'from', 'the', 'previous', 'section', 'on', 'a', 'sin', 'gle', 'GPU', 'processes', 'a', 'speed', 'of', 'approximately', 'words', 'per', 'second', 'This', 'was', 'too', 'slow', 'for', 'our', 'purposes', 'so', 'we', 'parallelized', 'our', 'model', 'using', 'an', 'GPU', 'machine', 'Each', 'layer', 'of', 'the', 'LSTM', 'was', 'executed', 'on', 'a', 'different', 'GPU', 'and', 'communicated', 'its', 'activations', 'to', 'the', 'next', 'GPU', 'layer', 'as', 'soon', 'as', 'they', 'were', 'computed', 'Our', 'models', 'have', 'layers', 'of', 'LSTMs', 'each', 'of', 'which', 'resides', 'on', 'a', 'separate', 'GPU', 'The', 'remaining', 'GPUs', 'were', 'used', 'to', 'parallelize', 'the', 'softmax', 'so', 'each', 'GPU', 'was', 'responsible', 'for', 'multiplying', 'by', 'a', 'x', 'matrix', 'The', 'resulting', 'implementation', 'achieved', 'a', 'speed', 'of', 'both', 'English', 'and', 'French', 'words', 'per', 'second', 'with', 'a', 'minibatch', 'size', 'of', 'Training', 'took', 'about', 'a', 'ten', 'days', 'with', 'this', 'implementation', 'Experimental', 'Results', 'We', 'used', 'the', 'cased', 'BLEU', 'score', 'to', 'evaluate', 'the', 'quality', 'of', 'our', 'translations', 'We', 'computed', 'our', 'BLEU', 'scores', 'using', 'multibleup', 'on', 'the', 'tokenized', 'predictions', 'and', 'ground', 'truth', 'This', 'way', 'of', 'evaluating', 'the', 'BELU', 'score', 'is', 'consistent', 'with', 'and', 'and', 'reproduces', 'the', 'score', 'of', 'However', 'if', 'we', 'evaluate', 'the', 'best', 'WMT', 'system', 'whose', 'predictions', 'can', 'be', 'downloaded', 'from', 'statmtorgmatrix', 'in', 'this', 'manner', 'we', 'get', 'which', 'is', 'greater', 'than', 'the', 'reported', 'by', 'statmtorgmatrix', 'The', 'results', 'are', 'presented', 'in', 'tables', 'and', 'Our', 'best', 'results', 'are', 'obtained', 'with', 'an', 'ensemble', 'of', 'LSTMs', 'that', 'differ', 'in', 'their', 'random', 'initializations', 'and', 'in', 'the', 'random', 'order', 'of', 'minibatches', 'While', 'the', 'decoded', 'translations', 'of', 'the', 'LSTM', 'ensemble', 'do', 'not', 'outperform', 'the', 'best', 'WMT', 'system', 'it', 'is', 'the', 'first', 'time', 'that', 'a', 'pure', 'neural', 'translation', 'system', 'outperforms', 'a', 'phrasebased', 'SMT', 'baseline', 'on', 'a', 'large', 'scale', 'MT', 'There', 'several', 'variants', 'of', 'the', 'BLEU', 'score', 'and', 'each', 'variant', 'is', 'defined', 'with', 'a', 'perl', 'script', 'test', 'BLEU', 'score', 'ntsti', 'Bahdanau', 'et', 'al', 'Baseline', 'System', 'Single', 'forward', 'LSTM', 'beam', 'size', 'Table', 'The', 'performance', 'of', 'the', 'LSTM', 'on', 'WMT', 'English', 'to', 'French', 'test', 'set', 'ntst', 'Note', 'that', 'an', 'ensemble', 'of', 'LSTMs', 'with', 'a', 'beam', 'of', 'size', 'is', 'cheaper', 'than', 'of', 'a', 'single', 'LSTM', 'with', 'a', 'beam', 'of', 'size', 'test', 'BLEU', 'score', 'ntsti', 'Baseline', 'System', 'Cho', 'et', 'al', 'Best', 'WMT', 'result', 'Rescoring', 'the', 'baseline', 'best', 'with', 'a', 'single', 'forward', 'LSTM', 'Rescoring', 'the', 'baseline', 'best', 'with', 'a', 'single', 'reversed', 'LSTM', 'Rescoring', 'the', 'baseline', 'best', 'with', 'an', 'ensemble', 'of', 'reversed', 'LSTMs', 'Oracle', 'Rescoring', 'of', 'the', 'Baseline', 'best', 'lists', 'Table', 'Methods', 'that', 'use', 'neural', 'networks', 'together', 'with', 'an', 'SMT', 'system', 'on', 'the', 'WMT', 'English', 'to', 'French', 'test', 'set', 'ntst', 'task', 'by', 'a', 'sizeable', 'margin', 'despite', 'its', 'inability', 'to', 'handle', 'outofvocabulary', 'words', 'The', 'LSTM', 'is', 'within', 'BLEU', 'points', 'of', 'the', 'best', 'WMT', 'result', 'if', 'it', 'is', 'used', 'to', 'rescore', 'the', 'best', 'list', 'of', 'the', 'baseline', 'system', 'Performance', 'on', 'long', 'sentences', 'We', 'were', 'surprised', 'to', 'discover', 'that', 'the', 'LSTM', 'did', 'well', 'on', 'long', 'sentences', 'which', 'is', 'shown', 'quantita', 'tively', 'in', 'figure', 'Table', 'presents', 'several', 'examples', 'of', 'long', 'sentences', 'and', 'their', 'translations', 'Model', 'Analysis', 'al', 'O', 'was', 'given', 'a', 'card', 'by', 'her', 'in', 'the', 'garden', 'F', 'OMary', 'admires', 'John', 'O', 'Inthe', 'garden', 'she', 'gave', 'me', 'a', 'card', 'O', 'She', 'gave', 'me', 'a', 'card', 'in', 'the', 'garden', 'OMary', 'is', 'in', 'love', 'with', 'John', 'OMary', 'respects', 'John', 'aL', 'OJohn', 'admires', 'Mary', 'O', 'She', 'was', 'given', 'a', 'card', 'by', 'me', 'in', 'the', 'garden', 'OJohn', 'is', 'in', 'love', 'with', 'Mar', 'y', 'In', 'the', 'garden', 'gave', 'her', 'a', 'card', 'OJohn', 'respects', 'Mary', 'I', 'gave', 'her', 'a', 'card', 'in', 'the', 'garden', 'L', 'L', 'L', 'L', 'J', 'L', 'L', 'L', 'L', 'L', 'J', 'Figure', 'The', 'figure', 'shows', 'a', 'dimensional', 'PCA', 'projection', 'of', 'the', 'LSTM', 'hidden', 'states', 'that', 'are', 'obtained', 'after', 'processing', 'the', 'phrases', 'in', 'the', 'figures', 'The', 'phrases', 'are', 'clustered', 'by', 'meaning', 'which', 'in', 'these', 'examples', 'is', 'primarily', 'a', 'function', 'of', 'word', 'order', 'which', 'would', 'be', 'difficult', 'to', 'capture', 'with', 'a', 'bagofwords', 'model', 'Notice', 'that', 'both', 'clusters', 'have', 'similar', 'internal', 'structure', 'One', 'of', 'the', 'attractive', 'features', 'of', 'our', 'model', 'is', 'its', 'ability', 'to', 'turn', 'a', 'sequence', 'of', 'words', 'into', 'a', 'vector', 'of', 'fixed', 'dimensionality', 'Figure', 'visualizes', 'some', 'of', 'the', 'learned', 'representations', 'The', 'figure', 'clearly', 'shows', 'that', 'the', 'representations', 'are', 'sensitive', 'to', 'the', 'order', 'of', 'words', 'while', 'being', 'fairly', 'insensitive', 'to', 'the', 'Our', 'model', 'Ulrich', 'UNK', 'membre', 'du', 'conseil', 'd', 'administration', 'du', 'constructeur', 'automobile', 'Audi', 'affirme', 'qu', 'ils', 'agit', 'd', 'une', 'pratique', 'courante', 'depuis', 'des', 'années', 'pour', 'que', 'les', 'teléphones', 'portables', 'puissent', 'étre', 'collectés', 'avant', 'les', 'reunions', 'du', 'conseil', 'd', 'administration', 'afin', 'qu', 'ils', 'ne', 'soient', 'pas', 'utilises', 'comme', 'appareils', 'd', 'écoute', 'a', 'distance', 'Truth', 'Ulrich', 'Hackenberg', 'membre', 'du', 'conseil', 'd', 'administration', 'du', 'constructeur', 'automobile', 'Audi', 'déclare', 'que', 'la', 'collecte', 'des', 'téléphones', 'portables', 'avant', 'les', 'reunions', 'du', 'conseil', 'afin', 'qu', 'ils', 'ne', 'puissent', 'pas', 'étre', 'utilisés', 'comme', 'appareils', 'd', 'écoute', 'a', 'distance', 'est', 'une', 'pratique', 'courante', 'depuis', 'des', 'années', 'Our', 'model', 'Les', 'telephones', 'cellulaires', 'qui', 'sont', 'vraiment', 'une', 'question', 'non', 'seulement', 'parce', 'qu', 'ils', 'pourraient', 'potentiellement', 'causer', 'des', 'interférences', 'avec', 'les', 'appareils', 'de', 'navigation', 'mais', 'nous', 'savons', 'selon', 'la', 'FCC', 'qu', 'ils', 'pourraient', 'interférer', 'avec', 'les', 'tours', 'de', 'téléphone', 'cellulaire', 'lorsqu', 'ils', 'sont', 'dans', 'I', 'air', 'dit', 'UNK', 'Truth', 'Les', 'telephones', 'portables', 'sont', 'véritablement', 'un', 'probleme', 'non', 'seulement', 'parce', 'qu', 'ils', 'pourraient', 'éventuellement', 'créer', 'des', 'interférences', 'avec', 'les', 'instruments', 'de', 'navigation', 'mais', 'parce', 'que', 'nous', 'savons', 'd', 'apres', 'la', 'FCC', 'qu', 'ils', 'pourraient', 'perturber', 'les', 'antennesrelais', 'de', 'telephonie', 'mobile', 's', 'ils', 'sont', 'utilisés', 'a', 'bord', 'a', 'declaré', 'Rosenker', 'Our', 'model', 'Avec', 'la', 'crémation', 'il', 'y', 'a', 'un', 'sentiment', 'de', 'violence', 'contre', 'le', 'corps', 'd', 'un', 'étre', 'cher', 'qui', 'sera', 'réduit', 'a', 'une', 'pile', 'de', 'cendres', 'en', 'trés', 'peu', 'de', 'temps', 'au', 'lieu', 'd', 'un', 'processus', 'de', 'decomposition', 'qui', 'accompagnera', 'les', 'étapes', 'du', 'deuil', 'Truth', 'Il', 'y', 'a', 'avec', 'la', 'cremation', 'une', 'violence', 'faite', 'au', 'corps', 'aimé', 'a', 'qui', 'va', 'étre', 'reduit', 'a', 'un', 'tas', 'de', 'cendres', 'en', 'trés', 'peu', 'de', 'temps', 'et', 'non', 'aprés', 'un', 'processus', 'de', 'decomposition', 'qui', 'accompagnerait', 'les', 'phases', 'du', 'deuil', 'Table', 'A', 'few', 'examples', 'of', 'long', 'translations', 'produced', 'by', 'the', 'LSTM', 'alongside', 'the', 'ground', 'truth', 'translations', 'The', 'reader', 'can', 'verify', 'that', 'the', 'translations', 'are', 'sensible', 'using', 'Google', 'translate', 'LSTM', 'LSTM', 'e', 'e', 'baseline', 'e', 'e', 'baseline', 'Y', 'Y', 'U', 'U', 'wn', 'wn', 'D', 'D', 'a', 'a', 'test', 'sentences', 'sorted', 'by', 'their', 'length', 'test', 'sentences', 'sorted', 'by', 'average', 'word', 'frequency', 'rank', 'Figure', 'The', 'left', 'plot', 'shows', 'the', 'performance', 'of', 'our', 'system', 'as', 'a', 'function', 'of', 'sentence', 'length', 'where', 'the', 'xaxis', 'corresponds', 'to', 'the', 'test', 'sentences', 'sorted', 'by', 'their', 'length', 'and', 'is', 'marked', 'by', 'the', 'actual', 'sequence', 'lengths', 'There', 'is', 'no', 'degradation', 'on', 'sentences', 'with', 'less', 'than', 'words', 'there', 'is', 'only', 'a', 'minor', 'degradation', 'on', 'the', 'longest', 'sentences', 'The', 'right', 'plot', 'shows', 'the', 'LSTM', 's', 'performance', 'on', 'sentences', 'with', 'progressively', 'more', 'rare', 'words', 'where', 'the', 'xaxis', 'corresponds', 'to', 'the', 'test', 'sentences', 'sorted', 'by', 'their', 'average', 'word', 'frequency', 'rank', 'replacement', 'of', 'an', 'active', 'voice', 'with', 'a', 'passive', 'voice', 'The', 'twodimensional', 'projections', 'are', 'obtained', 'using', 'PCA', 'Related', 'work', 'There', 'is', 'a', 'large', 'body', 'of', 'work', 'on', 'applications', 'of', 'neural', 'networks', 'to', 'machine', 'translation', 'So', 'far', 'the', 'simplest', 'and', 'most', 'effective', 'way', 'of', 'applying', 'an', 'RNNLanguage', 'Model', 'RNNLM', 'or', 'a', 'Feedforward', 'Neural', 'Network', 'Language', 'Model', 'NNLM', 'to', 'an', 'MT', 'task', 'is', 'by', 'rescoring', 'the', 'n', 'best', 'lists', 'of', 'a', 'strong', 'MT', 'baseline', 'which', 'reliably', 'improves', 'translation', 'quality', 'More', 'recently', 'researchers', 'have', 'begun', 'to', 'look', 'into', 'ways', 'of', 'including', 'information', 'about', 'the', 'source', 'language', 'into', 'the', 'NNLM', 'Examples', 'of', 'this', 'work', 'include', 'Auli', 'et', 'al', 'who', 'combine', 'an', 'NNLM', 'with', 'a', 'topic', 'model', 'of', 'the', 'input', 'sentence', 'which', 'improves', 'rescoring', 'performance', 'Devlin', 'et', 'al', 'followed', 'a', 'similar', 'approach', 'but', 'they', 'incorporated', 'their', 'NNLM', 'into', 'the', 'decoder', 'of', 'an', 'MT', 'system', 'and', 'used', 'the', 'decoder', 's', 'alignment', 'information', 'to', 'provide', 'the', 'NNLM', 'with', 'the', 'most', 'useful', 'words', 'in', 'the', 'input', 'sentence', 'Their', 'approach', 'was', 'highly', 'successful', 'and', 'it', 'achieved', 'large', 'improvements', 'over', 'their', 'baseline', 'Our', 'work', 'is', 'closely', 'related', 'to', 'Kalchbrenner', 'and', 'Blunsom', 'who', 'were', 'the', 'first', 'to', 'map', 'the', 'input', 'sentence', 'into', 'a', 'vector', 'and', 'then', 'back', 'to', 'a', 'sentence', 'although', 'they', 'map', 'sentences', 'to', 'vectors', 'using', 'convolutional', 'neural', 'networks', 'which', 'lose', 'the', 'ordering', 'of', 'the', 'words', 'Similarly', 'to', 'this', 'work', 'Cho', 'et', 'al', 'used', 'an', 'LSTMlike', 'RNN', 'architecture', 'to', 'map', 'sentences', 'into', 'vectors', 'and', 'back', 'although', 'their', 'primary', 'focus', 'was', 'on', 'integrating', 'their', 'neural', 'network', 'into', 'an', 'SMT', 'system', 'Bahdanau', 'et', 'al', 'also', 'attempted', 'direct', 'translations', 'with', 'a', 'neural', 'network', 'that', 'used', 'an', 'attention', 'mechanism', 'to', 'overcome', 'the', 'poor', 'performance', 'on', 'long', 'sentences', 'experienced', 'by', 'Cho', 'et', 'al', 'and', 'achieved', 'encouraging', 'results', 'Likewise', 'PougetAbadie', 'et', 'al', 'attempted', 'to', 'address', 'the', 'memory', 'problem', 'of', 'Cho', 'et', 'al', 'by', 'translating', 'pieces', 'of', 'the', 'source', 'sentence', 'in', 'way', 'that', 'produces', 'smooth', 'translations', 'which', 'is', 'similar', 'to', 'a', 'phrasebased', 'approach', 'We', 'suspect', 'that', 'they', 'could', 'achieve', 'similar', 'improvements', 'by', 'simply', 'training', 'their', 'networks', 'on', 'reversed', 'source', 'sentences', 'Endtoend', 'training', 'is', 'also', 'the', 'focus', 'of', 'Hermann', 'et', 'al', 'whose', 'model', 'represents', 'the', 'inputs', 'and', 'outputs', 'by', 'feedforward', 'networks', 'and', 'map', 'them', 'to', 'similar', 'points', 'in', 'space', 'However', 'their', 'approach', 'cannot', 'generate', 'translations', 'directly', 'to', 'get', 'a', 'translation', 'they', 'need', 'to', 'do', 'a', 'look', 'up', 'for', 'closest', 'vector', 'in', 'the', 'precomputed', 'database', 'of', 'sentences', 'or', 'to', 'rescore', 'a', 'sentence', 'Conclusion', 'In', 'this', 'work', 'we', 'showed', 'that', 'a', 'large', 'deep', 'LSTM', 'that', 'has', 'a', 'limited', 'vocabulary', 'and', 'that', 'makes', 'almost', 'no', 'assumption', 'about', 'problem', 'structure', 'can', 'outperform', 'a', 'standard', 'SMTbased', 'system', 'whose', 'vocabulary', 'is', 'unlimited', 'on', 'a', 'largescale', 'MT', 'task', 'The', 'success', 'of', 'our', 'simple', 'LSTMbased', 'approach', 'on', 'MT', 'suggests', 'that', 'it', 'should', 'do', 'well', 'on', 'many', 'other', 'sequence', 'learning', 'problems', 'provided', 'they', 'have', 'enough', 'training', 'data', 'We', 'were', 'surprised', 'by', 'the', 'extent', 'of', 'the', 'improvement', 'obtained', 'by', 'reversing', 'the', 'words', 'in', 'the', 'source', 'sentences', 'We', 'conclude', 'that', 'it', 'is', 'important', 'to', 'find', 'a', 'problem', 'encoding', 'that', 'has', 'the', 'greatest', 'number', 'of', 'short', 'term', 'dependencies', 'as', 'they', 'make', 'the', 'learning', 'problem', 'much', 'simpler', 'In', 'particular', 'while', 'we', 'were', 'unable', 'to', 'train', 'a', 'standard', 'RNN', 'on', 'the', 'nonreversed', 'translation', 'problem', 'shown', 'in', 'fig', 'we', 'believe', 'that', 'a', 'standard', 'RNN', 'should', 'be', 'easily', 'trainable', 'when', 'the', 'source', 'sentences', 'are', 'reversed', 'although', 'we', 'did', 'not', 'verify', 'it', 'experimentally', 'We', 'were', 'also', 'surprised', 'by', 'the', 'ability', 'of', 'the', 'LSTM', 'to', 'correctly', 'translate', 'very', 'long', 'sentences', 'We', 'were', 'initially', 'convinced', 'that', 'the', 'LSTM', 'would', 'fail', 'on', 'long', 'sentences', 'due', 'to', 'its', 'limited', 'memory', 'and', 'other', 'researchers', 'reported', 'poor', 'performance', 'on', 'long', 'sentences', 'with', 'a', 'model', 'similar', 'to', 'ours', 'And', 'yet', 'LSTMs', 'trained', 'on', 'the', 'reversed', 'dataset', 'had', 'little', 'difficulty', 'translating', 'long', 'sentences', 'Most', 'importantly', 'we', 'demonstrated', 'that', 'a', 'simple', 'straightforward', 'and', 'a', 'relatively', 'unoptimized', 'ap', 'proach', 'can', 'outperform', 'an', 'SMT', 'system', 'so', 'further', 'work', 'will', 'likely', 'lead', 'to', 'even', 'greater', 'translation', 'accuracies', 'These', 'results', 'suggest', 'that', 'our', 'approach', 'will', 'likely', 'do', 'well', 'on', 'other', 'challenging', 'sequence', 'to', 'sequence', 'problems', 'Acknowledgments', 'We', 'thank', 'Samy', 'Bengio', 'Jeff', 'Dean', 'Matthieu', 'Devin', 'Geoffrey', 'Hinton', 'Nal', 'Kalchbrenner', 'Thang', 'Luong', 'Wolf', 'gang', 'Macherey', 'Rajat', 'Monga', 'Vincent', 'Vanhoucke', 'Peng', 'Xu', 'Wojciech', 'Zaremba', 'and', 'the', 'Google', 'Brain', 'team', 'for', 'useful', 'comments', 'and', 'discussions', 'References', 'M', 'Auli', 'M', 'Galley', 'C', 'Quirk', 'and', 'G', 'Zweig', 'Joint', 'language', 'and', 'translation', 'modeling', 'with', 'recurrent', 'neural', 'networks', 'In', 'EMNLP', 'D', 'Bahdanau', 'K', 'Cho', 'and', 'Y', 'Bengio', 'Neural', 'machine', 'translation', 'by', 'jointly', 'learning', 'to', 'align', 'and', 'translate', 'arXiv', 'preprint', 'arXiv', 'Y', 'Bengio', 'R', 'Ducharme', 'P', 'Vincent', 'and', 'C', 'Jauvin', 'A', 'neural', 'probabilistic', 'language', 'model', 'In', 'Journal', 'of', 'Machine', 'Learning', 'Research', 'pages', 'Y', 'Bengio', 'P', 'Simard', 'and', 'P', 'Frasconi', 'Learning', 'longterm', 'dependencies', 'with', 'gradient', 'descent', 'is', 'difficult', 'IEEE', 'Transactions', 'on', 'Neural', 'Networks', 'K', 'Cho', 'B', 'Merrienboer', 'C', 'Gulcehre', 'F', 'Bougares', 'H', 'Schwenk', 'and', 'Y', 'Bengio', 'Learning', 'phrase', 'represen', 'tations', 'using', 'RNN', 'encoderdecoder', 'for', 'statistical', 'machine', 'translation', 'In', 'Arxiv', 'preprint', 'arXiv', 'D', 'Ciresan', 'U', 'Meier', 'and', 'J', 'Schmidhuber', 'Multicolumn', 'deep', 'neural', 'networks', 'for', 'image', 'classification', 'In', 'CVPR', 'G', 'E', 'Dahl', 'D', 'Yu', 'L', 'Deng', 'and', 'A', 'Acero', 'Contextdependent', 'pretrained', 'deep', 'neural', 'networks', 'for', 'large', 'vocabulary', 'speech', 'recognition', 'IEEE', 'Transactions', 'on', 'Audio', 'Speech', 'and', 'Language', 'Processing', 'Special', 'Issue', 'on', 'Deep', 'Learning', 'for', 'Speech', 'and', 'Language', 'Processing', 'J', 'Devlin', 'R', 'Zbib', 'Z', 'Huang', 'T', 'Lamar', 'R', 'Schwartz', 'and', 'J', 'Makhoul', 'Fast', 'and', 'robust', 'neural', 'network', 'joint', 'models', 'for', 'statistical', 'machine', 'translation', 'In', 'ACL', 'Nadir', 'Durrani', 'Barry', 'Haddow', 'Philipp', 'Koehn', 'and', 'Kenneth', 'Heafield', 'Edinburgh', 's', 'phrasebased', 'machine', 'translation', 'systems', 'for', 'wmt', 'In', 'WMT', 'A', 'Graves', 'Generating', 'sequences', 'with', 'recurrent', 'neural', 'networks', 'In', 'Arxiv', 'preprint', 'arXiv', 'A', 'Graves', 'S', 'Fernandez', 'F', 'Gomez', 'and', 'J', 'Schmidhuber', 'Connectionist', 'temporal', 'classification', 'labelling', 'unsegmented', 'sequence', 'data', 'with', 'recurrent', 'neural', 'networks', 'In', 'ICML', 'K', 'M', 'Hermann', 'and', 'P', 'Blunsom', 'Multilingual', 'distributed', 'representations', 'without', 'word', 'alignment', 'In', 'ICLR', 'G', 'Hinton', 'L', 'Deng', 'D', 'Yu', 'G', 'Dahl', 'A', 'Mohamed', 'N', 'Jaitly', 'A', 'Senior', 'V', 'Vanhoucke', 'P', 'Nguyen', 'T', 'Sainath', 'and', 'B', 'Kingsbury', 'Deep', 'neural', 'networks', 'for', 'acoustic', 'modeling', 'in', 'speech', 'recognition', 'IEEE', 'Signal', 'Processing', 'Magazine', 'S', 'Hochreiter', 'Untersuchungen', 'zu', 'dynamischen', 'neuronalen', 'netzen', 'Master', 's', 'thesis', 'Institut', 'fur', 'Infor', 'matik', 'Technische', 'Universitat', 'Munchen', 'S', 'Hochreiter', 'Y', 'Bengio', 'P', 'Frasconi', 'and', 'J', 'Schmidhuber', 'Gradient', 'flow', 'in', 'recurrent', 'nets', 'the', 'difficulty', 'of', 'learning', 'longterm', 'dependencies', 'S', 'Hochreiter', 'and', 'J', 'Schmidhuber', 'Long', 'shortterm', 'memory', 'Neural', 'Computation', 'S', 'Hochreiter', 'and', 'J', 'Schmidhuber', 'LSTM', 'can', 'solve', 'hard', 'long', 'time', 'lag', 'problems', 'N', 'Kalchbrenner', 'and', 'P', 'Blunsom', 'Recurrent', 'continuous', 'translation', 'models', 'In', 'EMNLP', 'A', 'Krizhevsky', 'I', 'Sutskever', 'and', 'G', 'E', 'Hinton', 'ImageNet', 'classification', 'with', 'deep', 'convolutional', 'neural', 'networks', 'In', 'NIPS', 'QV', 'Le', 'MA', 'Ranzato', 'R', 'Monga', 'M', 'Devin', 'K', 'Chen', 'GS', 'Corrado', 'J', 'Dean', 'and', 'AY', 'Ng', 'Building', 'highlevel', 'features', 'using', 'large', 'scale', 'unsupervised', 'learning', 'In', 'ICML', 'Y', 'LeCun', 'L', 'Bottou', 'Y', 'Bengio', 'and', 'P', 'Haffner', 'Gradientbased', 'learning', 'applied', 'to', 'document', 'recognition', 'Proceedings', 'of', 'the', 'IEEE', 'T', 'Mikolov', 'Statistical', 'Language', 'Models', 'based', 'on', 'Neural', 'Networks', 'PhD', 'thesis', 'Brno', 'University', 'of', 'Technology', 'T', 'Mikolov', 'M', 'Karafiat', 'L', 'Burget', 'J', 'Cernocky', 'and', 'S', 'Khudanpur', 'Recurrent', 'neural', 'network', 'based', 'language', 'model', 'In', 'INTERSPEECH', 'pages', 'K', 'Papineni', 'S', 'Roukos', 'T', 'Ward', 'and', 'W', 'J', 'Zhu', 'BLEU', 'a', 'method', 'for', 'automatic', 'evaluation', 'of', 'machine', 'translation', 'In', 'ACL', 'R', 'Pascanu', 'T', 'Mikolov', 'and', 'Y', 'Bengio', 'On', 'the', 'difficulty', 'of', 'training', 'recurrent', 'neural', 'networks', 'arXiv', 'preprint', 'arXiv', 'J', 'PougetAbadie', 'D', 'Bahdanau', 'B', 'van', 'Merrienboer', 'K', 'Cho', 'and', 'Y', 'Bengio', 'Overcoming', 'the', 'curse', 'of', 'sentence', 'length', 'for', 'neural', 'machine', 'translation', 'using', 'automatic', 'segmentation', 'arXiv', 'preprint', 'arXiv', 'A', 'Razborov', 'On', 'small', 'depth', 'threshold', 'circuits', 'In', 'Proc', 'rd', 'Scandinavian', 'Workshop', 'on', 'Algorithm', 'Theory', 'D', 'Rumelhart', 'G', 'E', 'Hinton', 'and', 'R', 'J', 'Williams', 'Learning', 'representations', 'by', 'backpropagating', 'errors', 'Nature', 'H', 'Schwenk', 'University', 'le', 'mans', 'httpwwwliumunivlemansfrschwenkcslm', 'jointpaper', 'Online', 'accessed', 'September', 'M', 'Sundermeyer', 'R', 'Schluter', 'and', 'H', 'Ney', 'LSTM', 'neural', 'networks', 'for', 'language', 'modeling', 'In', 'INTER', 'SPEECH', 'P', 'Werbos', 'Backpropagation', 'through', 'time', 'what', 'it', 'does', 'and', 'how', 'to', 'do', 'it', 'Proceedings', 'of', 'IEEE']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization Comparision <br>\n",
        "it seems that gensim uses the same re function, that we showed above. Both returned only words <br>\n",
        "nltk and spacy return also punctuations <br>\n",
        "spacy treats a whitespace as a token if there is a double whitespace. <br>\n",
        "'"
      ],
      "metadata": {
        "id": "iVebaUFjlMRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contaction to Expansion > can't TO can not ,you'll TO you will\n",
        "contractions = {\n",
        "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
        "\"aren't\": \"are not / am not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he had / he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he shall / he will\",\n",
        "\"he'll've\": \"he shall have / he will have\",\n",
        "\"he's\": \"he has / he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how has / how is / how does\",\n",
        "\"i'd\": \"I had / I would\",\n",
        "\"i'd've\": \"I would have\",\n",
        "\"i'll\": \"I shall / I will\",\n",
        "\"i'll've\": \"I shall have / I will have\",\n",
        "\"i'm\": \"I am\",\n",
        "\"i've\": \"I have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it had / it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it shall / it will\",\n",
        "\"it'll've\": \"it shall have / it will have\",\n",
        "\"it's\": \"it has / it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she had / she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she shall / she will\",\n",
        "\"she'll've\": \"she shall have / she will have\",\n",
        "\"she's\": \"she has / she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as / so is\",\n",
        "\"that'd\": \"that would / that had\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that has / that is\",\n",
        "\"there'd\": \"there had / there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there has / there is\",\n",
        "\"they'd\": \"they had / they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they shall / they will\",\n",
        "\"they'll've\": \"they shall have / they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we had / we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what shall / what will\",\n",
        "\"what'll've\": \"what shall have / what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what has / what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when has / when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where has / where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who shall / who will\",\n",
        "\"who'll've\": \"who shall have / who will have\",\n",
        "\"who's\": \"who has / who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why has / why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you had / you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you shall / you will\",\n",
        "\"you'll've\": \"you shall have / you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "MyZhqWxTmDDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cont_to_exp(x):\n",
        "    if type(x) is str:\n",
        "        for key in contractions:\n",
        "            value = contractions[key]\n",
        "            x = x.replace(key, value)\n",
        "        return x\n",
        "    else:\n",
        "        return x"
      ],
      "metadata": {
        "id": "ISJ3u7f5m8oX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "result_text = cont_to_exp(result_text)\n",
        "print(result_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rJmom6om_Qj",
        "outputId": "4a6d2e03-cd8b-47ba-c8b9-b3ba52a737c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v csCL  Dec \n",
            "\n",
            "V\n",
            "\n",
            "arxX\n",
            "\n",
            " \n",
            "\n",
            "Sequence to Sequence Learning\n",
            "with Neural Networks\n",
            "\n",
            "Ilya Sutskever Oriol Vinyals Quoc V Le\n",
            "Google Google Google\n",
            "ilyasugooglecom vinyalsgooglecom qvlgooglecom\n",
            "Abstract\n",
            "\n",
            "Deep Neural Networks DNNs are powerful models that have achieved excel\n",
            "lent performance on difficult learning tasks Although DNNs work well whenever\n",
            "large labeled training sets are available they cannot be used to map sequences to\n",
            "sequences In this paper we present a general endtoend approach to sequence\n",
            "learning that makes minimal assumptions on the sequence structure Our method\n",
            "uses a multilayered Long ShortTerm Memory LSTM to map the input sequence\n",
            "to a vector of a fixed dimensionality and then another deep LSTM to decode the\n",
            "target sequence from the vector Our main result is that on an English to French\n",
            "translation task from the WMT’  dataset the translations produced by the LSTM\n",
            "achieve a BLEU score of  on the entire test set where the LSTM’s BLEU\n",
            "score was penalized on outofvocabulary words Additionally the LSTM did not\n",
            "have difficulty on long sentences For comparison a phrasebased SMT system\n",
            "achieves a BLEU score of  on the same dataset When we used the LSTM\n",
            "to rerank the  hypotheses produced by the aforementioned SMT system its\n",
            "BLEU score increases to  which is close to the previous best result on this\n",
            "task The LSTM also learned sensible phrase and sentence representations that\n",
            "are sensitive to word order and are relatively invariant to the active and the pas\n",
            "sive voice Finally we found that reversing the order of the words in all source\n",
            "sentences but not target sentences improved the LSTM’s performance markedly\n",
            "because doing so introduced many short term dependencies between the source\n",
            "and the target sentence which made the optimization problem easier\n",
            "\n",
            " Introduction\n",
            "\n",
            "Deep Neural Networks DNNs are extremely powerful machine learning models that achieve ex\n",
            "cellent performance on difficult problems such as speech recognition   and visual object recog\n",
            "nition     DNNs are powerful because they can perform arbitrary parallel computation\n",
            "for a modest number of steps A surprising example of the power of DNNs is their ability to sort\n",
            "N WNbit numbers using only  hidden layers of quadratic size  So while neural networks are\n",
            "related to conventional statistical models they learn an intricate computation Furthermore large\n",
            "DNNs can be trained with supervised backpropagation whenever the labeled training set has enough\n",
            "information to specify the network’s parameters Thus if there exists a parameter setting of a large\n",
            "DNN that achieves good results for example because humans can solve the task very rapidly\n",
            "supervised backpropagation will find these parameters and solve the problem\n",
            "\n",
            "Despite their flexibility and power DNNs can only be applied to problems whose inputs and targets\n",
            "can be sensibly encoded with vectors of fixed dimensionality It is a significant limitation since\n",
            "many important problems are best expressed with sequences whose lengths are not known apriori\n",
            "For example speech recognition and machine translation are sequential problems Likewise ques\n",
            "tion answering can also be seen as mapping a sequence of words representing the question to a\n",
            "\fsequence of words representing the answer It is therefore clear that a domainindependent method\n",
            "that learns to map sequences to sequences would be useful\n",
            "\n",
            "Sequences pose a challenge for DNNs because they require that the dimensionality of the inputs and\n",
            "outputs is known and fixed In this paper we show that a straightforward application of the Long\n",
            "ShortTerm Memory LSTM architecture  can solve general sequence to sequence problems\n",
            "The idea is to use one LSTM to read the input sequence one timestep at a time to obtain large fixed\n",
            "dimensional vector representation and then to use another LSTM to extract the output sequence\n",
            "from that vector fig  The second LSTM is essentially a recurrent neural network language model\n",
            "   except that it is conditioned on the input sequence The LSTM’s ability to successfully\n",
            "learn on data with long range temporal dependencies makes it a natural choice for this application\n",
            "due to the considerable time lag between the inputs and their corresponding outputs fig \n",
            "\n",
            "There have been a number of related attempts to address the general sequence to sequence learning\n",
            "problem with neural networks Our approach is closely related to Kalchbrenner and Blunsom \n",
            "who were the first to map the entire input sentence to vector and is related to Cho et al  although\n",
            "the latter was used only for rescoring hypotheses produced by a phrasebased system Graves \n",
            "introduced a novel differentiable attention mechanism that allows neural networks to focus on dif\n",
            "ferent parts of their input and an elegant variant of this idea was successfully applied to machine\n",
            "translation by Bahdanau et al  The Connectionist Sequence Classification is another popular\n",
            "technique for mapping sequences to sequences with neural networks but it assumes a monotonic\n",
            "alignment between the inputs and the outputs \n",
            "\n",
            "W X Y Z EOS\n",
            "\n",
            "A B C EOS W X Y Z\n",
            "\n",
            "Figure  Our model reads an input sentence “ABC” and produces “WXYZ” as the output sentence The\n",
            "model stops making predictions after outputting the endofsentence token Note that the LSTM reads the\n",
            "input sentence in reverse because doing so introduces many short term dependencies in the data that make the\n",
            "optimization problem much easier\n",
            "\n",
            "The main result of this work is the following On the WMT’  English to French translation task\n",
            "we obtained a BLEU score of  by directly extracting translations from an ensemble of  deep\n",
            "LSTMs with M parameters and  dimensional state each using a simple lefttoright beam\n",
            "search decoder This is by far the best result achieved by direct translation with large neural net\n",
            "works For comparison the BLEU score of an SMT baseline on this dataset is   The \n",
            "BLEU score was achieved by an LSTM with a vocabulary of k words so the score was penalized\n",
            "whenever the reference translation contained a word not covered by these k This result shows\n",
            "that a relatively unoptimized smallvocabulary neural network architecture which has much room\n",
            "for improvement outperforms a phrasebased SMT system\n",
            "\n",
            "Finally we used the LSTM to rescore the publicly available best lists of the SMT baseline on\n",
            "the same task  By doing so we obtained a BLEU score of  which improves the baseline by\n",
            " BLEU points and is close to the previous best published result on this task which is  \n",
            "\n",
            "Surprisingly the LSTM did not suffer on very long sentences despite the recent experience of other\n",
            "researchers with related architectures  We were able to do well on long sentences because we\n",
            "reversed the order of words in the source sentence but not the target sentences in the training and test\n",
            "set By doing so we introduced many short term dependencies that made the optimization problem\n",
            "much simpler see sec  and  As a result SGD could learn LSTMs that had no trouble with\n",
            "long sentences The simple trick of reversing the words in the source sentence is one of the key\n",
            "technical contributions of this work\n",
            "\n",
            "A useful property of the LSTM is that it learns to map an input sentence of variable length into\n",
            "a fixeddimensional vector representation Given that translations tend to be paraphrases of the\n",
            "source sentences the translation objective encourages the LSTM to find sentence representations\n",
            "that capture their meaning as sentences with similar meanings are close to each other while different\n",
            "\fsentences meanings will be far A qualitative evaluation supports this claim showing that our model\n",
            "is aware of word order and is fairly invariant to the active and passive voice\n",
            "\n",
            " The model\n",
            "\n",
            "The Recurrent Neural Network RNN   is a natural generalization of feedforward neural\n",
            "networks to sequences Given a sequence of inputs  a standard RNN computes a\n",
            "sequence of outputs y yr by iterating the following equation\n",
            "\n",
            "h  sigm Wa  W he\n",
            "Ye  Whe\n",
            "\n",
            "The RNN can easily map sequences to sequences whenever the alignment between the inputs the\n",
            "outputs is known ahead of time However it is not clear how to apply an RNN to problems whose\n",
            "input and the output sequences have different lengths with complicated and nonmonotonic relation\n",
            "ships\n",
            "\n",
            "The simplest strategy for general sequence learning is to map the input sequence to a fixedsized\n",
            "vector using one RNN and then to map the vector to the target sequence with another RNN this\n",
            "approach has also been taken by Cho et al  While it could work in principle since the RNN is\n",
            "provided with all the relevant information it would be difficult to train the RNNs due to the resulting\n",
            "long term dependencies figure      However the Long ShortTerm Memory LSTM\n",
            " is known to learn problems with long range temporal dependencies so an LSTM may succeed\n",
            "in this setting\n",
            "\n",
            "The goal of the LSTM is to estimate the conditional probability pyy  where\n",
            "v is an input sequence and y yr’ is its corresponding output sequence whose length\n",
            "T’ may differ from ’ The LSTM computes this conditional probability by first obtaining the fixed\n",
            "dimensional representation v of the input sequence   given by the last hidden state of the\n",
            "LSTM and then computing the probability of yy with a standard LSTMLM formulation\n",
            "whose initial hidden state is set to the representation v of  \n",
            "\n",
            "T’\n",
            "Pyyreier   viyelos yi yea \n",
            "t\n",
            "\n",
            "In this equation each pyzv y Yz— distribution is represented with a softmax over all the\n",
            "words in the vocabulary We use the LSTM formulation from Graves  Note that we require that\n",
            "each sentence ends with a special endofsentence symbol “EOS” which enables the model to\n",
            "define a distribution over sequences of all possible lengths The overall scheme is outlined in figure\n",
            " where the shown LSTM computes the representation of “A” “B” “C” “EOS” and then uses\n",
            "this representation to compute the probability of “W” “X” “Y” “Z” “EOS”\n",
            "\n",
            "Our actual models differ from the above description in three important ways First we used two\n",
            "different LSTMs one for the input sequence and another for the output sequence because doing\n",
            "so increases the number model parameters at negligible computational cost and makes it natural to\n",
            "train the LSTM on multiple language pairs simultaneously  Second we found that deep LSTMs\n",
            "significantly outperformed shallow LSTMs so we chose an LSTM with four layers Third we found\n",
            "it extremely valuable to reverse the order of the words of the input sentence So for example instead\n",
            "of mapping the sentence a bc to the sentence a y the LSTM is asked to map c b a to a \n",
            "where a  ¥ is the translation of a b c This way a is in close proximity to a b is fairly close to \n",
            "and so on a fact that makes it easy for SGD to “establish communication” between the input and the\n",
            "output We found this simple data transformation to greatly improve the performance of the LSTM\n",
            "\n",
            " Experiments\n",
            "\n",
            "We applied our method to the WMT’ English to French MT task in two ways We used it to\n",
            "directly translate the input sentence without using a reference SMT system and we it to rescore the\n",
            "nbest lists of an SMT baseline We report the accuracy of these translation methods present sample\n",
            "translations and visualize the resulting sentence representation\n",
            "\f Dataset details\n",
            "\n",
            "We used the WMT’  English to French dataset We trained our models on a subset of M sen\n",
            "tences consisting of M French words and M English words which is a clean “selected”\n",
            "subset from  We chose this translation task and this specific training set subset because of the\n",
            "public availability of a tokenized training and test set together with best lists from the baseline\n",
            "SMT \n",
            "\n",
            "As typical neural language models rely on a vector representation for each word we used a fixed\n",
            "vocabulary for both languages We used  of the most frequent words for the source language\n",
            "and  of the most frequent words for the target language Every outofvocabulary word was\n",
            "replaced with a special “UNK” token\n",
            "\n",
            " Decoding and Rescoring\n",
            "\n",
            "The core of our experiments involved training a large deep LSTM on many sentence pairs We\n",
            "trained it by maximizing the log probability of a correct translation T’ given the source sentence S\n",
            "so the training objective is\n",
            "\n",
            "S S— logpTS\n",
            "\n",
            "TSES\n",
            "\n",
            "where S is the training set Once training is complete we produce translations by finding the most\n",
            "likely translation according to the LSTM\n",
            "\n",
            "A\n",
            "\n",
            "T  arg max pTS \n",
            "\n",
            "We search for the most likely translation using a simple lefttoright beam search decoder which\n",
            "maintains a small number B of partial hypotheses where a partial hypothesis is a prefix of some\n",
            "translation At each timestep we extend each partial hypothesis in the beam with every possible\n",
            "word in the vocabulary This greatly increases the number of the hypotheses so we discard all but\n",
            "the B most likely hypotheses according to the model’s log probability As soon as the “EOS”\n",
            "symbol is appended to a hypothesis it is removed from the beam and is added to the set of complete\n",
            "hypotheses While this decoder is approximate it is simple to implement Interestingly our system\n",
            "performs well even with a beam size of  and a beam of size  provides most of the benefits of beam\n",
            "search Table \n",
            "\n",
            "We also used the LSTM to rescore the best lists produced by the baseline system  To\n",
            "rescore an nbest list we computed the log probability of every hypothesis with our LSTM and took\n",
            "an even average with their score and the LSTM’s score\n",
            "\n",
            " Reversing the Source Sentences\n",
            "\n",
            "While the LSTM is capable of solving problems with long term dependencies we discovered that\n",
            "the LSTM learns much better when the source sentences are reversed the target sentences are not\n",
            "reversed By doing so the LSTM’s test perplexity dropped from  to  and the test BLEU\n",
            "scores of its decoded translations increased from  to \n",
            "\n",
            "While we do not have a complete explanation to this phenomenon we believe that it is caused by\n",
            "the introduction of many short term dependencies to the dataset Normally when we concatenate a\n",
            "source sentence with a target sentence each word in the source sentence is far from its corresponding\n",
            "word in the target sentence As a result the problem has a large “minimal time lag”  By\n",
            "reversing the words in the source sentence the average distance between corresponding words in\n",
            "the source and target language is unchanged However the first few words in the source language\n",
            "are now very close to the first few words in the target language so the problem’s minimal time lag is\n",
            "greatly reduced Thus backpropagation has an easier time “establishing communication” between\n",
            "the source sentence and the target sentence which in turn results in substantially improved overall\n",
            "performance\n",
            "\n",
            "Initially we believed that reversing the input sentences would only lead to more confident predic\n",
            "tions in the early parts of the target sentence and to less confident predictions in the later parts How\n",
            "ever LSTMs trained on reversed source sentences did much better on long sentences than LSTMs\n",
            "\ftrained on the raw source sentences see sec  which suggests that reversing the input sentences\n",
            "results in LSTMs with better memory utilization\n",
            "\n",
            " Training details\n",
            "\n",
            "We found that the LSTM models are fairly easy to train We used deep LSTMs with  layers\n",
            "with  cells at each layer and  dimensional word embeddings with an input vocabulary\n",
            "of  and an output vocabulary of  Thus the deep LSTM uses  real numbers to\n",
            "represent a sentence We found deep LSTMs to significantly outperform shallow LSTMs where\n",
            "each additional layer reduced perplexity by nearly  possibly due to their much larger hidden\n",
            "state We used a naive softmax over  words at each output The resulting LSTM has M\n",
            "parameters of which M are pure recurrent connections M for the “encoder” LSTM and M\n",
            "for the “decoder” LSTM The complete training details are given below\n",
            "\n",
            "e We initialized all of the LSTM’s parameters with the uniform distribution between \n",
            "and \n",
            "\n",
            "e We used stochastic gradient descent without momentum with a fixed learning rate of \n",
            "After  epochs we begun halving the learning rate every half epoch We trained our models\n",
            "for a total of  epochs\n",
            "\n",
            "e We used batches of  sequences for the gradient and divided it the size of the batch\n",
            "namely \n",
            "\n",
            "e Although LSTMs tend to not suffer from the vanishing gradient problem they can have\n",
            "exploding gradients Thus we enforced a hard constraint on the norm of the gradient \n",
            " by scaling it when its norm exceeded a threshold For each training batch we compute\n",
            "  g where g is the gradient divided by  If s   we set g  \n",
            "\n",
            "e Different sentences have different lengths Most sentences are short eg length \n",
            "but some sentences are long eg length   so a minibatch of  randomly chosen\n",
            "training sentences will have many short sentences and few long sentences and as a result\n",
            "much of the computation in the minibatch is wasted To address this problem we made sure\n",
            "that all sentences in a minibatch are roughly of the same length yielding a x speedup\n",
            "\n",
            " Parallelization\n",
            "\n",
            "A C implementation of deep LSTM with the configuration from the previous section on a sin\n",
            "gle GPU processes a speed of approximately  words per second This was too slow for our\n",
            "purposes so we parallelized our model using an GPU machine Each layer of the LSTM was\n",
            "executed on a different GPU and communicated its activations to the next GPU  layer as soon as\n",
            "they were computed Our models have  layers of LSTMs each of which resides on a separate\n",
            "GPU The remaining  GPUs were used to parallelize the softmax so each GPU was responsible\n",
            "for multiplying by a  x  matrix The resulting implementation achieved a speed of \n",
            "both English and French words per second with a minibatch size of  Training took about a ten\n",
            "days with this implementation\n",
            "\n",
            " Experimental Results\n",
            "\n",
            "We used the cased BLEU score  to evaluate the quality of our translations We computed our\n",
            "BLEU scores using multibleup on the tokenized predictions and ground truth This way\n",
            "of evaluating the BELU score is consistent with  and  and reproduces the  score of \n",
            "However if we evaluate the best WMT’  system  whose predictions can be downloaded from\n",
            "statmtorgmatrix in this manner we get  which is greater than the  reported by\n",
            "statmtorgmatrix\n",
            "\n",
            "The results are presented in tables  and  Our best results are obtained with an ensemble of LSTMs\n",
            "that differ in their random initializations and in the random order of minibatches While the decoded\n",
            "translations of the LSTM ensemble do not outperform the best WMT’  system it is the first time\n",
            "that a pure neural translation system outperforms a phrasebased SMT baseline on a large scale MT\n",
            "\n",
            "‘There several variants of the BLEU score and each variant is defined with a perl script\n",
            "\ftest BLEU score ntsti\n",
            "Bahdanau et al  \n",
            "Baseline System  \n",
            "\n",
            "Single forward LSTM beam size  \n",
            "\n",
            "Table  The performance of the LSTM on WMT’  English to French test set ntst Note that\n",
            "an ensemble of  LSTMs with a beam of size  is cheaper than of a single LSTM with a beam of\n",
            "size \n",
            "\n",
            " \n",
            "\n",
            "test BLEU score ntsti\n",
            "Baseline System  \n",
            "\n",
            "Cho et al  \n",
            "Best WMT result \n",
            "\n",
            "Rescoring the baseline best with a single forward LSTM \n",
            "Rescoring the baseline best with a single reversed LSTM \n",
            "Rescoring the baseline best with an ensemble of  reversed LSTMs\n",
            "\n",
            "Oracle Rescoring of the Baseline best lists\n",
            "\n",
            " \n",
            "\n",
            "Table  Methods that use neural networks together with an SMT system on the WMT’  English\n",
            "to French test set ntst\n",
            "\n",
            "task by a sizeable margin despite its inability to handle outofvocabulary words The LSTM is\n",
            "within  BLEU points of the best WMT’  result if it is used to rescore the best list of the\n",
            "baseline system\n",
            "\n",
            " Performance on long sentences\n",
            "\n",
            "We were surprised to discover that the LSTM did well on long sentences which is shown quantita\n",
            "tively in figure  Table  presents several examples of long sentences and their translations\n",
            "\n",
            " Model Analysis\n",
            "\n",
            "al O  was given a card by her in the garden\n",
            "\n",
            "F OMary admires John  O Inthe garden  she gave me a card\n",
            "O She gave me a card in the garden\n",
            " OMary is in love with John\n",
            "\n",
            "OMary respects John\n",
            "aL OJohn admires Mary\n",
            " O She was given a card by me in the garden\n",
            "\n",
            " OJohn is in love with Mar\n",
            "y © In the garden   gave her a card\n",
            "\n",
            "  OJohn respects Mary © I gave her a card in the garden\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "    L L L L J  L L L L L  J\n",
            "                 \n",
            "\n",
            "Figure  The figure shows a dimensional PCA projection of the LSTM hidden states that are obtained\n",
            "after processing the phrases in the figures The phrases are clustered by meaning which in these examples is\n",
            "primarily a function of word order which would be difficult to capture with a bagofwords model Notice that\n",
            "both clusters have similar internal structure\n",
            "\n",
            "One of the attractive features of our model is its ability to turn a sequence of words into a vector\n",
            "of fixed dimensionality Figure  visualizes some of the learned representations The figure clearly\n",
            "shows that the representations are sensitive to the order of words while being fairly insensitive to the\n",
            "\fOur model  Ulrich UNK  membre du conseil d’ administration du constructeur automobile Audi \n",
            "affirme qu’ ils’ agit d’ une pratique courante depuis des années pour que les teléphones\n",
            "portables puissent étre collectés avant les reunions du conseil d’ administration afin qu’ ils\n",
            "ne soient pas utilises comme appareils d’ écoute a distance \n",
            "\n",
            "Truth Ulrich Hackenberg  membre du conseil d’ administration du constructeur automobile Audi \n",
            "déclare que la collecte des téléphones portables avant les reunions du conseil  afin qu’ ils\n",
            "ne puissent pas étre utilisés comme appareils d’ écoute a distance  est une pratique courante\n",
            "depuis des années \n",
            "\n",
            "Our model  “ Les telephones cellulaires  qui sont vraiment une question  non seulement parce qu’ ils\n",
            "pourraient potentiellement causer des interférences avec les appareils de navigation  mais\n",
            "nous savons  selon la FCC  qu’ ils pourraient interférer avec les tours de téléphone cellulaire\n",
            "lorsqu’ ils sont dans I’ air”  dit UNK \n",
            "\n",
            "Truth “ Les telephones portables sont véritablement un probleme  non seulement parce qu’ ils\n",
            "pourraient éventuellement créer des interférences avec les instruments de navigation  mais\n",
            "parce que nous savons  d’ apres la FCC  qu’ ils pourraient perturber les antennesrelais de\n",
            "telephonie mobile s’ ils sont utilisés a bord ”  a declaré Rosenker \n",
            "\n",
            "Our model  Avec la crémation  il y a un “ sentiment de violence contre le corps d’ un étre cher ” \n",
            "\n",
            "qui sera “ réduit a une pile de cendres ” en trés peu de temps au lieu d’ un processus de\n",
            "decomposition “ qui accompagnera les étapes du deuil ” \n",
            "\n",
            "Truth Il y a avec la cremation  “ une violence faite au corps aimé ” \n",
            "\n",
            "a qui va étre “ reduit a un tas de cendres ” en trés peu de temps  et non aprés un processus de\n",
            "\n",
            "decomposition  qui “ accompagnerait les phases du deuil ” \n",
            "\n",
            " \n",
            "\n",
            "Table  A few examples of long translations produced by the LSTM alongside the ground truth\n",
            "translations The reader can verify that the translations are sensible using Google translate\n",
            "\n",
            "   \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "— LSTM  — LSTM \n",
            " e—e baseline   e—e baseline \n",
            " \n",
            "Y Y\n",
            "° °\n",
            "U U\n",
            "wn wn\n",
            "D D\n",
            "   \n",
            "a a\n",
            " \n",
            "              \n",
            "              \n",
            "test sentences sorted by their length test sentences sorted by average word frequency rank\n",
            "\n",
            "Figure  The left plot shows the performance of our system as a function of sentence length where the\n",
            "xaxis corresponds to the test sentences sorted by their length and is marked by the actual sequence lengths\n",
            "There is no degradation on sentences with less than  words there is only a minor degradation on the longest\n",
            "sentences The right plot shows the LSTM’s performance on sentences with progressively more rare words\n",
            "where the xaxis corresponds to the test sentences sorted by their “average word frequency rank”\n",
            "\n",
            "replacement of an active voice with a passive voice The twodimensional projections are obtained\n",
            "using PCA\n",
            "\n",
            " Related work\n",
            "\n",
            "There is a large body of work on applications of neural networks to machine translation So far\n",
            "the simplest and most effective way of applying an RNNLanguage Model RNNLM  or a\n",
            "\fFeedforward Neural Network Language Model NNLM  to an MT task is by rescoring the n\n",
            "best lists of a strong MT baseline  which reliably improves translation quality\n",
            "\n",
            "More recently researchers have begun to look into ways of including information about the source\n",
            "language into the NNLM Examples of this work include Auli et al  who combine an NNLM\n",
            "with a topic model of the input sentence which improves rescoring performance Devlin et al \n",
            "followed a similar approach but they incorporated their NNLM into the decoder of an MT system\n",
            "and used the decoder’s alignment information to provide the NNLM with the most useful words in\n",
            "the input sentence Their approach was highly successful and it achieved large improvements over\n",
            "their baseline\n",
            "\n",
            "Our work is closely related to Kalchbrenner and Blunsom  who were the first to map the input\n",
            "sentence into a vector and then back to a sentence although they map sentences to vectors using\n",
            "convolutional neural networks which lose the ordering of the words Similarly to this work Cho et\n",
            "al  used an LSTMlike RNN architecture to map sentences into vectors and back although their\n",
            "primary focus was on integrating their neural network into an SMT system Bahdanau et al  also\n",
            "attempted direct translations with a neural network that used an attention mechanism to overcome\n",
            "the poor performance on long sentences experienced by Cho et al  and achieved encouraging\n",
            "results Likewise PougetAbadie et al  attempted to address the memory problem of Cho et\n",
            "al  by translating pieces of the source sentence in way that produces smooth translations which\n",
            "is similar to a phrasebased approach We suspect that they could achieve similar improvements by\n",
            "simply training their networks on reversed source sentences\n",
            "\n",
            "Endtoend training is also the focus of Hermann et al  whose model represents the inputs and\n",
            "outputs by feedforward networks and map them to similar points in space However their approach\n",
            "cannot generate translations directly to get a translation they need to do a look up for closest vector\n",
            "in the precomputed database of sentences or to rescore a sentence\n",
            "\n",
            " Conclusion\n",
            "\n",
            "In this work we showed that a large deep LSTM that has a limited vocabulary and that makes\n",
            "almost no assumption about problem structure can outperform a standard SMTbased system whose\n",
            "vocabulary is unlimited on a largescale MT task The success of our simple LSTMbased approach\n",
            "on MT suggests that it should do well on many other sequence learning problems provided they\n",
            "have enough training data\n",
            "\n",
            "We were surprised by the extent of the improvement obtained by reversing the words in the source\n",
            "sentences We conclude that it is important to find a problem encoding that has the greatest number\n",
            "of short term dependencies as they make the learning problem much simpler In particular while\n",
            "we were unable to train a standard RNN on the nonreversed translation problem shown in fig \n",
            "we believe that a standard RNN should be easily trainable when the source sentences are reversed\n",
            "although we did not verify it experimentally\n",
            "\n",
            "We were also surprised by the ability of the LSTM to correctly translate very long sentences We\n",
            "were initially convinced that the LSTM would fail on long sentences due to its limited memory\n",
            "and other researchers reported poor performance on long sentences with a model similar to ours\n",
            "   And yet LSTMs trained on the reversed dataset had little difficulty translating long\n",
            "sentences\n",
            "\n",
            "Most importantly we demonstrated that a simple straightforward and a relatively unoptimized ap\n",
            "proach can outperform an SMT system so further work will likely lead to even greater translation\n",
            "accuracies These results suggest that our approach will likely do well on other challenging sequence\n",
            "to sequence problems\n",
            "\n",
            " Acknowledgments\n",
            "\n",
            "We thank Samy Bengio Jeff Dean Matthieu Devin Geoffrey Hinton Nal Kalchbrenner Thang Luong Wolf\n",
            "gang Macherey Rajat Monga Vincent Vanhoucke Peng Xu Wojciech Zaremba and the Google Brain team\n",
            "for useful comments and discussions\n",
            "\fReferences\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "M Auli M Galley C Quirk and G Zweig Joint language and translation modeling with recurrent\n",
            "neural networks In EMNLP \n",
            "\n",
            "D Bahdanau K Cho and Y Bengio Neural machine translation by jointly learning to align and translate\n",
            "arXiv preprint arXiv  \n",
            "\n",
            "Y Bengio R Ducharme P Vincent and C Jauvin A neural probabilistic language model In Journal of\n",
            "Machine Learning Research pages  \n",
            "\n",
            "Y Bengio P Simard and P Frasconi Learning longterm dependencies with gradient descent is difficult\n",
            "IEEE Transactions on Neural Networks — \n",
            "\n",
            "K Cho B Merrienboer C Gulcehre F Bougares H Schwenk and Y Bengio Learning phrase represen\n",
            "tations using RNN encoderdecoder for statistical machine translation In Arxiv preprint arXiv \n",
            "\n",
            "\n",
            "D Ciresan U Meier and J Schmidhuber Multicolumn deep neural networks for image classification\n",
            "In CVPR \n",
            "\n",
            "G E Dahl D Yu L Deng and A Acero Contextdependent pretrained deep neural networks for large\n",
            "vocabulary speech recognition IEEE Transactions on Audio Speech and Language Processing  Special\n",
            "Issue on Deep Learning for Speech and Language Processing \n",
            "\n",
            "J Devlin R Zbib Z Huang T Lamar R Schwartz and J Makhoul Fast and robust neural network\n",
            "joint models for statistical machine translation In ACL \n",
            "\n",
            "Nadir Durrani Barry Haddow Philipp Koehn and Kenneth Heafield Edinburgh’s phrasebased machine\n",
            "translation systems for wmt In WMT \n",
            "\n",
            "A Graves Generating sequences with recurrent neural networks In Arxiv preprint arXiv \n",
            "\n",
            "\n",
            "A Graves S Fernandez F Gomez and J Schmidhuber Connectionist temporal classification labelling\n",
            "unsegmented sequence data with recurrent neural networks In ICML \n",
            "\n",
            "K M Hermann and P Blunsom Multilingual distributed representations without word alignment In\n",
            "ICLR \n",
            "\n",
            "G Hinton L Deng D Yu G Dahl A Mohamed N Jaitly A Senior V Vanhoucke P Nguyen\n",
            "T Sainath and B Kingsbury Deep neural networks for acoustic modeling in speech recognition IEEE\n",
            "Signal Processing Magazine \n",
            "\n",
            "S Hochreiter Untersuchungen zu dynamischen neuronalen netzen Master’s thesis Institut fur Infor\n",
            "matik Technische Universitat Munchen \n",
            "\n",
            "S Hochreiter Y Bengio P Frasconi and J Schmidhuber Gradient flow in recurrent nets the difficulty\n",
            "of learning longterm dependencies \n",
            "\n",
            "S Hochreiter and J Schmidhuber Long shortterm memory Neural Computation \n",
            "\n",
            "S Hochreiter and J Schmidhuber LSTM can solve hard long time lag problems \n",
            "\n",
            "N Kalchbrenner and P Blunsom Recurrent continuous translation models In EMNLP \n",
            "\n",
            "A Krizhevsky I Sutskever and G E Hinton ImageNet classification with deep convolutional neural\n",
            "networks In NIPS \n",
            "\n",
            "QV Le MA Ranzato R Monga M Devin K Chen GS Corrado J Dean and AY Ng Building\n",
            "highlevel features using large scale unsupervised learning In ICML \n",
            "\n",
            "Y LeCun L Bottou Y Bengio and P Haffner Gradientbased learning applied to document recognition\n",
            "Proceedings of the IEEE \n",
            "\n",
            "T Mikolov Statistical Language Models based on Neural Networks PhD thesis Brno University of\n",
            "Technology \n",
            "\n",
            "T Mikolov M Karafiat L Burget J Cernocky and S Khudanpur Recurrent neural network based\n",
            "language model In INTERSPEECH pages  \n",
            "\n",
            "K Papineni S Roukos T Ward and W J Zhu BLEU a method for automatic evaluation of machine\n",
            "translation In ACL \n",
            "\n",
            "R Pascanu T Mikolov and Y Bengio On the difficulty of training recurrent neural networks arXiv\n",
            "preprint arXiv \n",
            "\n",
            "J PougetAbadie D Bahdanau B van Merrienboer K Cho and Y Bengio Overcoming the\n",
            "curse of sentence length for neural machine translation using automatic segmentation arXiv preprint\n",
            "arXiv \n",
            "\n",
            "A Razborov On small depth threshold circuits In Proc rd Scandinavian Workshop on Algorithm\n",
            "Theory \n",
            "\n",
            "D Rumelhart G E Hinton and R J Williams Learning representations by backpropagating errors\n",
            "Nature — \n",
            "\n",
            "H Schwenk University le mans httpwwwliumunivlemansfrschwenkcslm\n",
            "jointpaper  Online accessed September\n",
            "\n",
            "M Sundermeyer R Schluter and H Ney LSTM neural networks for language modeling In INTER\n",
            "SPEECH \n",
            "\n",
            "P Werbos Backpropagation through time what it does and how to do it Proceedings of IEEE \n",
            "\f\n",
            "CPU times: user 4.46 ms, sys: 1.02 ms, total: 5.47 ms\n",
            "Wall time: 20.3 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DOC Parser"
      ],
      "metadata": {
        "id": "XiXmXY3Ed1IR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docparser\n",
        "!pip install 'PyPDF2<3.0'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42KvWXlzXSqq",
        "outputId": "2aff27f1-0ae9-4b25-c0c4-9f386a1f42a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docparser\n",
            "  Downloading python_docparser-1.1.0-py3-none-any.whl (8.5 kB)\n",
            "Installing collected packages: python-docparser\n",
            "Successfully installed python-docparser-1.1.0\n",
            "Collecting PyPDF2<3.0\n",
            "  Downloading pypdf2-2.12.1-py3-none-any.whl (222 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.8/222.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-2.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import textwrap\n",
        "from docparser import parse\n",
        "# Parse the DOCX file and process its content as before\n",
        "document = parse(\"/content/1409.3215v3.docx\")\n",
        "content = document.content\n",
        "\n",
        "# Split the text into lines, remove leading and trailing spaces, and join again\n",
        "formatted_text = ' '.join(line.strip() for line in content.splitlines())\n",
        "\n",
        "# Replace multiple consecutive spaces with a single space\n",
        "formatted_text = re.sub(r'\\s+', ' ', formatted_text)\n",
        "\n",
        "# Set the desired width for the structured print (large value to output the whole text)\n",
        "width = 80\n",
        "\n",
        "# Wrap the text to the specified width (which will output the whole text)\n",
        "wrapped_text = textwrap.fill(formatted_text, width=width)\n",
        "\n",
        "# Print the structured text\n",
        "print(wrapped_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWkR8rmtbxl7",
        "outputId": "a65fb1e6-109a-4498-9794-b17aca24b79a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " arXiv:1409.3215v3 [cs.CL] 14 Dec 2014 Sequence to Sequence Learning with Neural\n",
            "Networks Ilya Sutskever Oriol Vinyals Quoc V. Le Google Google Google\n",
            "ilyasu@google.com vinyals@google.com qvl@google.com Abstract Deep Neural\n",
            "Networks (DNNs) are powerful models that have achieved excel-lent performance on\n",
            "difficult learning tasks. Although DNNs work well whenever large labeled\n",
            "training sets are available, they cannot be used to map sequences to sequences.\n",
            "In this paper, we present a general end-to-end approach to sequence learning\n",
            "that makes minimal assumptions on the sequence structure. Our method uses a\n",
            "multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector\n",
            "of a fixed dimensionality, and then another deep LS TM to decode the target\n",
            "sequence from the vector. Our main result is that on an English to French\n",
            "translation task from the WMT'14 dataset, the translations produced by the LSTM\n",
            "achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score\n",
            "was penalized on out-of-vocabulary words. Additionally, the LSTM did not have\n",
            "difficulty on long sentences. For comparison, a phrase- based SMT system\n",
            "achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to\n",
            "rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU\n",
            "score increases to 36.5, which is close to the previous best result on this\n",
            "task. The LSTM also learned sensible phrase and sentence representations that\n",
            "are sensitive to word order and are relatively invariant to the active and the\n",
            "pas-sive voice. Finally, we found that reversing the order of the words in all\n",
            "source sentences (but not target sentences) improved the LSTM's performance\n",
            "markedly, because doing so introduced many short term dependencies between the\n",
            "source and the target sentence which made the optimization problem easier. 1\n",
            "Introduction Deep Neural Networks (DNNs) are extremely powerful machine learning\n",
            "models that achieve ex-cellent performance on difficult problems such as speech\n",
            "rec ognition [13, 7] and visual object recog-nition [19, 6, 21, 20]. DNNs are\n",
            "powerful because they can perform arbitrary parallel computation for a modest\n",
            "number of steps. A surprising example of the power of DNNs is their ability to\n",
            "sort N -bit numbers using only 2 hidden layers of quadratic size [27]. So, while\n",
            "neural networks are related to conventional statistical models, they learn an\n",
            "intricate computation. Furthermore, large DNNs can be trained with supervised\n",
            "backpropagation whenever the labeled training set has enough information to\n",
            "specify the network's parameters. Thus, if there exists a parameter setting of a\n",
            "large DNN that achieves good results (for example, because humans can solve the\n",
            "task very rapidly), supervised backpropagation will find these parameters and s\n",
            "olve the problem. Despite their flexibility and power, DNNs can only be applied\n",
            "to problems whose inputs and targets can be sensibly encoded with vectors of\n",
            "fixed dimensionality . It is a significant limitation, since many important\n",
            "problems are best expressed with sequences whose lengths are not known a-priori.\n",
            "For example, speech recognition and machine translation are sequential problems.\n",
            "Likewise, ques-tion answering can also be seen as mapping a sequence of words\n",
            "representing the question to a 1 sequence of words representing the answer. It\n",
            "is therefore clear that a domain-independent method that learns to map sequences\n",
            "to sequences would be useful. Sequences pose a challenge for DNNs because they\n",
            "require that the dimensionality of the inputs and outputs is known and fixed. In\n",
            "this paper, we show that a straig htforward application of the Long Short-Term\n",
            "Memory (LSTM) architecture [16] can solve general sequence to sequence problems.\n",
            "The idea is to use one LSTM to read the input sequence, one timestep at a time,\n",
            "to obtain large fixed-dimensional vector representation, and then to use another\n",
            "LSTM to extract the output sequence from that vector (fig. 1). The second LSTM\n",
            "is essentially a rec urrent neural network language model [28, 23, 30] except\n",
            "that it is conditioned on the input sequence. The LSTM's ability to successfully\n",
            "learn on data with long range temporal dependencies makes it a natural choice\n",
            "for this application due to the considerable time lag between the inputs and\n",
            "their corresponding outputs (fig. 1). There have been a number of related\n",
            "attempts to address the general sequence to sequence learning problem with\n",
            "neural networks. Our approach is closely related to Kalchbrenner and Blunsom\n",
            "[18] who were the first to map the entire input sentence to vector, a nd is\n",
            "related to Cho et al. [5] although the latter was used only for rescoring\n",
            "hypotheses produced by a phrase-based system. Graves [10] introduced a novel\n",
            "differentiable attention mechanism that allows neural networks to focus on dif-\n",
            "ferent parts of their input, and an elegant variant of this idea was\n",
            "successfully applied to machine translation by Bahdanau et al. [2]. The\n",
            "Connectionist Sequence Classification is another popular technique for mapping\n",
            "sequences to sequences with neural networks, but it assumes a monotonic\n",
            "alignment between the inputs and the outputs [11]. Figure 1: Our model reads an\n",
            "input sentence “ABC” and produces “WXYZ” a s the output sentence. The model\n",
            "stops making predictions after outputting the end-of-sentence token. Note that\n",
            "the LSTM reads the input sentence in reverse, because doing so introduces many\n",
            "short term dependencies in the data that make the optimization problem much\n",
            "easier. The main result of this work is the following. On the WMT'14 English to\n",
            "French translation task, we obtained a BLEU score of 34.81 by directly\n",
            "extracting translations from an ensemble of 5 deep LSTMs (with 384M parameters\n",
            "and 8,000 dimensional state each) using a simple left-to-right beam-search\n",
            "decoder. This is by far the best result achieved by direct translation with\n",
            "large neural net-works. For comparison, the BLEU score of an SMT baseline on\n",
            "this dataset is 33.30 [29]. The 34.81 BLEU score was achieved by an LSTM with a\n",
            "vocabulary of 80k words, so the score was penalized whenever the reference\n",
            "translation contained a word not covered by these 80k. This result shows that a\n",
            "relatively unoptimized small-vocabulary neural network architecture which has\n",
            "much room for improvement outperforms a phrase-based SMT system. Finally, we\n",
            "used the LSTM to rescore the publicly available 1000-best lists of the SMT\n",
            "baseline on the same task [29]. By doing so, we obtained a BLEU score of 36.5,\n",
            "which improves the baseline by 3.2 BLEU points and is close to the previous best\n",
            "published result on this task (which is 37.0 [9]). Surprisingly, the LSTM did\n",
            "not suffer on very long sentences, despite the recent experience of other\n",
            "researchers with related architectures [26]. We were able to do well on long\n",
            "sentences because we reversed the order of words in the source sentence but not\n",
            "the target sentences in the training and test set. By doing so, we introduced\n",
            "many short term dependencies that made the optimization problem much simpler\n",
            "(see sec. 2 and 3.3). As a result, SGD could learn LSTMs that had no trouble\n",
            "with long sentences. The simple trick of reversing the words in the source\n",
            "sentence is one of the key technical contributions of this work. A useful\n",
            "property of the LSTM is that it learns to map an input sentence of variable\n",
            "length into a fixed-dimensional vector representation. Given that tran slations\n",
            "tend to be paraphrases of the source sentences, the translation objective\n",
            "encourages the LSTM to find sentence representations that capture their meaning,\n",
            "as sentences with similar meanings are close to each other while different 2\n",
            "sentences meanings will be far. A qualitative evaluation supports this claim,\n",
            "showing that our model is aware of word order and is fairly invariant to the\n",
            "active and passive voice. 2 The model The Recurrent Neural Network (RNN) [31,\n",
            "28] is a natural generalization of feedforward neural networks to sequences.\n",
            "Given a sequence of inputs (x1, . . . , xT ), a standard RNN computes a sequence\n",
            "of outputs (y1, . . . , yT ) by iterating the following equation: ht = sigm W\n",
            "hxxt + W hhht−1 yt = W yhht The RNN can easily map sequences to sequences\n",
            "whenever the alignment between the inputs the outputs is known ahead of time.\n",
            "However, it is not clear how to apply an RNN to problems whose input and the\n",
            "output sequences have different lengths with complicated and non-monotonic\n",
            "relation-ships. The simplest strategy for general sequence learning is to map\n",
            "the input sequence to a fixed-sized vector using one RNN, and then to map the\n",
            "vector to the target sequence with another RNN (this approach has also been\n",
            "taken by Cho et al. [5]). While it could work in principle since the RNN is\n",
            "provided with all the relevant information, it would be diffi cult to train the\n",
            "RNNs due to the resulting long term dependencies (figure 1) [14, 4, 16, 15].\n",
            "However, th e Long Short-Term Memory (LSTM) is known to learn problems with long\n",
            "range temporal dependencies, so an LSTM may succeed in this setting. The goal of\n",
            "the LSTM is to estimate the conditional probability p(y1, . . . , yT ′ |x1, . .\n",
            ". , xT ) where (x1, . . . , xT ) is an input sequence and y1, . . . , yT ′ is\n",
            "its corresponding output sequence whose length T ′ may differ from T . The LSTM\n",
            "computes this conditional probability by first obt aining the fixed-dimensional\n",
            "representation v of the input sequence (x1, . . . , xT ) given by the last\n",
            "hidden state of the LSTM, and then computing the probability of y1, . . . , yT ′\n",
            "with a standard LSTM-LM formulation whose initial hidden state is set to the\n",
            "representation v of x1, . . . , xT : T ′ p(y1, . . . , yT ′ |x1, . . . , xT ) =\n",
            "Y p(yt|v, y1, . . . , yt−1) (1) t=1 In this equation, each p(yt|v, y1, . . . ,\n",
            "yt−1) distribution is represented with a softmax over all the words in the\n",
            "vocabulary. We use the LSTM formulation from Graves [10]. Note that we require\n",
            "that each sentence ends with a special end-of-sentence symbol “ <EOS>”, which\n",
            "enables the model to define a distribution over sequences of all possible\n",
            "lengths . The overall scheme is outlined in figure 1, where the shown LSTM\n",
            "computes the representation of “A”, “ B”, “C”, “ <EOS>” and then uses this\n",
            "representation to compute the probability of “W”, “X”, “Y”, “Z”, “ <EOS>”. Our\n",
            "actual models differ from the above description in three important ways. First,\n",
            "we used two different LSTMs: one for the input sequence and another for the\n",
            "output sequence, because doing so increases the number model parameters at\n",
            "negligible computational cost and makes it natural to train the LSTM on multiple\n",
            "language pairs simultaneously [18]. Second, we found that deep LSTMs\n",
            "significantly outperformed shallow LSTMs, so we chose an LST M with four layers.\n",
            "Third, we found it extremely valuable to reverse the order of the words of the\n",
            "input sentence. So for example, instead of mapping the sentence a, b, c to the\n",
            "sentence α, β, γ, the LSTM is asked to map c, b, a to α, β, γ, where α, β, γ is\n",
            "the translation of a, b, c. This way, a is in close proximity to α, b is fairly\n",
            "close to β, and so on, a fact that makes it easy for SGD to “establish commu\n",
            "nication” between the input and the output. We found this simple data\n",
            "transformation to greatly improve the performance of the LSTM. 3 Experiments We\n",
            "applied our method to the WMT'14 English to French MT task in two ways. We used\n",
            "it to directly translate the input sentence without using a reference SMT system\n",
            "and we it to rescore the n-best lists of an SMT baseline. We report the accuracy\n",
            "of these translation methods, present sample translations, and visualize the\n",
            "resulting sentence representation. 3 3.1 Dataset details We used the WMT'14\n",
            "English to French dataset. We trained our models on a subset of 12M sen-tences\n",
            "consisting of 348M French words and 304M English words, which is a clean\n",
            "“selected” subset from [29]. We chose this translation task and this specific\n",
            "training set subset because of the public availability of a tokenized training\n",
            "and test set together with 1000-best lists from the baseline SMT [29]. As\n",
            "typical neural language models rely on a vector representation for each word, we\n",
            "used a fixed vocabulary for both languages. We used 160,000 of the most frequent\n",
            "words for the source language and 80,000 of the most frequent words for the\n",
            "target language. Every out-of-vocabulary word was replaced with a special “UNK”\n",
            "token. 3.2 Decoding and Rescoring The core of our experiments involved training\n",
            "a large deep LSTM on many sentence pairs. We trained it by maximizing the log\n",
            "probability of a correct translation T given the source sentence S, so the\n",
            "training objective is X 1/|S| log p(T |S) (T,S) S where S is the training set.\n",
            "Once training is complete, we produce translations by finding the most likely\n",
            "translation according to the LSTM: ˆ (2) T = arg max p(T |S) T We search for the\n",
            "most likely translation using a simple left-to-right beam search decoder which\n",
            "maintains a small number B of partial hypotheses, where a partial hypothesis is\n",
            "a prefix of some translation. At each timestep we extend each partial hypothesis\n",
            "in the beam with every possible word in the vocabulary. This greatly increases\n",
            "the number of the hypotheses so we discard all but the B most likely hypotheses\n",
            "according to the model's log probability. As soon as the “ <EOS>” symbol is\n",
            "appended to a hypothesis, it is removed from the beam and is added to the set of\n",
            "complete hypotheses. While this decoder is approximate, it is simple to\n",
            "implement. Interestingly, our system performs well even with a beam size of 1,\n",
            "and a beam of size 2 provides most of the benefits of beam search (Table 1). We\n",
            "also used the LSTM to rescore the 1000-best lists produced by the baseline\n",
            "system [29]. To rescore an n-best list, we computed the log probability of every\n",
            "hypothesis with our LSTM and took an even average with their score and the\n",
            "LSTM's score. 3.3 Reversing the Source Sentences While the LSTM is capable of\n",
            "solving problems with long term dependencies, we discovered that the LSTM learns\n",
            "much better when the source sentences are reversed (the target sentences are not\n",
            "reversed). By doing so, the LSTM's test perplexity dropped from 5.8 to 4.7, and\n",
            "the test BLEU scores of its decoded translations increased from 25.9 to 30.6.\n",
            "While we do not have a complete explanation to this phenomenon, we believe that\n",
            "it is caused by the introduction of many short term dependencies to the dataset.\n",
            "Normally, when we concatenate a source sentence with a target sentence, each\n",
            "word in the source sentence is far from its corresponding word in the target\n",
            "sentence. As a result, the problem has a large “minimal time lag” [17]. By\n",
            "reversing the words in the source sentence, the average distance between\n",
            "corresponding words in the source and target language is unchanged. However, the\n",
            "fir st few words in the source language are now very close to the first few\n",
            "words in the target language , so the problem's minimal time lag is greatly\n",
            "reduced. Thus, backpropagation has an easier time “ establishing communication”\n",
            "between the source sentence and the target sentence, which in turn results in\n",
            "substantially improved overall performance. Initially, we believed that\n",
            "reversing the input sentences would only lead to more confident predic- tions in\n",
            "the early parts of the target sentence and to less confident predictions in the\n",
            "later parts. How- ever, LSTMs trained on reversed source sentences did much\n",
            "better on long sentences than LSTMs 4 trained on the raw source sentences (see\n",
            "sec. 3.7), which suggests that reversing the input sentences results in LSTMs\n",
            "with better memory utilization. 3.4 Training details We found that the LSTM\n",
            "models are fairly easy to train. We used deep LSTMs with 4 layers, with 1000\n",
            "cells at each layer and 1000 dimensional word embeddings, with an input\n",
            "vocabulary of 160,000 and an output vocabulary of 80,000. Thus the deep LSTM\n",
            "uses 8000 real numbers to represent a sentence. We found deep LSTMs to\n",
            "significantly ou tperform shallow LSTMs, where each additional layer reduced\n",
            "perplexity by nearly 10%, possibly due to their much larger hidden state. We\n",
            "used a naive softmax over 80,000 words at each output. The resulting LSTM has\n",
            "384M parameters of which 64M are pure recurrent connections (32M for the\n",
            "“encoder” LSTM and 32M for the “decoder” LSTM). The complete training details\n",
            "are g iven below: We initialized all of the LSTM's parameters with the uniform\n",
            "distribution between -0.08 and 0.08 We used stochastic gradient descent without\n",
            "momentum, with a fixed learning rate of 0.7. After 5 epochs, we begun halving\n",
            "the learning rate every half epoch. We trained our models for a total of 7.5\n",
            "epochs. We used batches of 128 sequences for the gradient and divided it the\n",
            "size of the batch (namely, 128). Although LSTMs tend to not suffer from the\n",
            "vanishing gradient problem, they can have exploding gradients. Thus we enforced\n",
            "a hard constraint on the norm of the gradient [10, 25] by scaling it when its\n",
            "norm exceeded a threshold. For each training batch, we compute s = g 2, where g\n",
            "is the gradient divided by 128. If s > 5, we set g = 5sg . Different sentences\n",
            "have different lengths. Most sentences are short (e.g., length 20-30) but some\n",
            "sentences are long (e.g., length > 100), so a minibatch of 128 randomly chosen\n",
            "training sentences will have many short sentences and few long sentences, and as\n",
            "a result, much of the computation in the minibatch is wasted. To address this\n",
            "problem, we made sure that all sentences in a minibatch are roughly of the same\n",
            "length, yielding a 2x speedup. 3.5 Parallelization A C++ implementation of deep\n",
            "LSTM with the configuration from the previous section on a sin-gle GPU processes\n",
            "a speed of approximately 1,700 words per second. This was too slow for our\n",
            "purposes, so we parallelized our model using an 8-GPU machine. Each layer of the\n",
            "LSTM was executed on a different GPU and communicated its activations to the\n",
            "next GPU / layer as soon as they were computed. Our models have 4 layers of\n",
            "LSTMs, each of which resides on a separate GPU. The remaining 4 GPUs were used\n",
            "to parallelize the softmax, so each GPU was responsible for multiplying by a\n",
            "1000 × 20000 matrix. The resulting implementation achieved a speed of 6,300\n",
            "(both English and French) words per second with a minibatch size of 128.\n",
            "Training took about a ten days with this implementation. 3.6 Experimental\n",
            "Results We used the cased BLEU score [24] to evaluate the quality of our\n",
            "translations. We computed our BLEU scores using multi-bleu.pl1 on the tokenized\n",
            "predictions and ground truth. This way of evaluating the BELU score is\n",
            "consistent with [5] and [2], and reproduces the 33.3 score of [29]. However, if\n",
            "we evaluate the best WMT'14 system [9] (whose predictions can be downloaded from\n",
            "statmt.org\\matrix) in this manner, we get 37.0, which is greater than the 35.8\n",
            "reported by statmt.org\\matrix. The results are presented in tables 1 and 2. Our\n",
            "best results are obtained with an ensemble of LSTMs that differ in their random\n",
            "initializations and in the random order of minibatches. While the decoded\n",
            "translations of the LSTM ensemble do not outperform the best WMT'14 system, it\n",
            "is the first time that a pure neural translation system outperforms a phrase-\n",
            "based SMT baseline on a large scale MT There several variants of the BLEU score,\n",
            "and each variant is defined with a perl script. 5 Method test BLEU score\n",
            "(ntst14) Bahdanau et al. [2] 28.45 Baseline System [29] 33.30 Single forward\n",
            "LSTM, beam size 12 26.17 Single reversed LSTM, beam size 12 30.59 Ensemble of 5\n",
            "reversed LSTMs, beam size 1 33.00 Ensemble of 2 reversed LSTMs, beam size 12\n",
            "33.27 Ensemble of 5 reversed LSTMs, beam size 2 34.50 Ensemble of 5 reversed\n",
            "LSTMs, beam size 12 34.81 Table 1: The performance of the LSTM on WMT'14 English\n",
            "to French test set (ntst14). Note that an ensemble of 5 LSTMs with a beam of\n",
            "size 2 is cheaper than of a single LSTM with a beam of size 12. Method test BLEU\n",
            "score (ntst14) Baseline System [29] 33.30 Cho et al. [5] 34.54 Best WMT'14\n",
            "result [9] 37.0 Rescoring the baseline 1000-best with a single forward LSTM\n",
            "35.61 Rescoring the baseline 1000-best with a single reversed LSTM 35.85\n",
            "Rescoring the baseline 1000-best with an ensemble of 5 reversed LSTMs 36.5\n",
            "Oracle Rescoring of the Baseline 1000-best lists ∼45 Table 2: Methods that use\n",
            "neural networks together with an SMT system on the WMT'14 English to French test\n",
            "set (ntst14). task by a sizeable margin, despite its inability to handle out-of-\n",
            "vocabulary words. The LSTM is within 0.5 BLEU points of the best WMT'14 result\n",
            "if it is used to rescore the 1000-best list of the baseline system. 3.7\n",
            "Performance on long sentences We were surprised to discover that the LSTM did\n",
            "well on long sentences, which is shown quantita-tively in figure 3. Table 3\n",
            "presents several examples of long s entences and their translations. 3.8 Model\n",
            "Analysis 4 15 I was given a card by her in the garden 3 Mary admires John 10 In\n",
            "the garden , she gave me a card 2 Mary is in love with John She gave me a card\n",
            "in the garden 5 1 0 Mary respects John 0 −1 John admires Mary −2 John is in love\n",
            "with Mary −5 She was given a card by me in the garden In the garden , I gave her\n",
            "a card −3 −10 −4 John respects Mary −15 I gave her a card in the garden −5 −6\n",
            "−20 −8 −6 −4 −2 0 2 4 6 8 10 −15 −10 −5 0 5 10 15 20 Figure 2: The figure shows\n",
            "a 2-dimensional PCA projection of the LSTM hi dden states that are obtained\n",
            "after processing the phrases in the figures. The phrases are c lustered by\n",
            "meaning, which in these examples is primarily a function of word order, which\n",
            "would be difficult t o capture with a bag-of-words model. Notice that both\n",
            "clusters have similar internal structure. One of the attractive features of our\n",
            "model is its ability to turn a sequence of words into a vector of fixed\n",
            "dimensionality. Figure 2 visualizes some of the lear ned representations. The\n",
            "figure clearly shows that the representations are sensitive to the order of\n",
            "words, while being fairly insensitive to the 6 Type Sentence Our model Ulrich\n",
            "UNK , membre du conseil d' administration du constructeur automobile Audi ,\n",
            "affirme qu' il s' agit d' une pratique courante depuis des ann´ees pour que les\n",
            "t´el´ephones portables puissent etreˆ collect´es avant les r´eunions du conseil\n",
            "d' administration afin qu' ils ne soient pas utilis´es comme appareils d'\n",
            "ecoute´ a` distan ce . Truth Ulrich Hackenberg , membre du conseil d'\n",
            "administration du constructeur automobile Audi , d´eclare que la collecte des\n",
            "t´el´ephones portables avant l es r´eunions du conseil , afin qu' ils ne\n",
            "puissent pas etreˆ utilis´es comme appareils d' ecoute´ `a distance , est une\n",
            "pratique courante depuis des ann´ees . Our model “ Les t´el´ephones cellulaires\n",
            ", qui sont vraiment une quest ion , non seulement parce qu' ils pourraient\n",
            "potentiellement causer des interf´erences avec les appareils de navigation ,\n",
            "mais nous savons , selon la FCC , qu' ils pourraient interf´erer avec les tours\n",
            "de t´el´ephone cellulaire lorsqu' ils sont dans l' air ” , dit UNK . Truth “ Les\n",
            "t´el´ephones portables sont v´eritablement un probl` eme , non seulement parce\n",
            "qu' ils pourraient eventuellement´ cr´eer des interf´erences ave c les\n",
            "instruments de navigation , mais parce que nous savons , d' apr`es la FCC , qu'\n",
            "ils pourraient perturber les antennes-relais de t´el´ephonie mobile s' ils sont\n",
            "utilis´es a` bord ” , a d´ecla r´e Rosenker . Our model Avec la cr´emation , il\n",
            "y a un “ sentiment de violence contre le corps d' un etreˆ cher ” , qui sera “\n",
            "r´eduit a` une pile de cendres ” en tr`es peu de temps au lieu d' un processus\n",
            "de d´ecomposition “ qui accompagnera les etapes´ du deuil ” . Truth Il y a ,\n",
            "avec la cr´emation , “ une violence faite au corps aim´e ” , qui va etreˆ “\n",
            "r´eduit a` un tas de cendres ” en tr`es peu de temp s , et non apr`es un\n",
            "processus de d´ecomposition , qui “ accompagnerait les phases du deuil ” . Table\n",
            "3: A few examples of long translations produced by the LSTM alongside the ground\n",
            "truth translations. The reader can verify that the translations are sensible\n",
            "using Google translate. LSTM (34.8) LSTM (34.8) 40 baseline (33.3) 40 baseline\n",
            "(33.3) 35 35 score score BLEU 30 BLEU 30 25 25 20 20 4 7 8 12 17 22 28 35 79 0\n",
            "500 1000 1500 2000 2500 3000 3500 test sentences sorted by their length test\n",
            "sentences sorted by average word frequency rank Figure 3: The left plot shows\n",
            "the performance of our system as a function of sentence length, where the x-axis\n",
            "corresponds to the test sentences sorted by their length and is marked by the\n",
            "actual sequence lengths. There is no degradation on sentences with less than 35\n",
            "words, there is only a minor degradation on the longest sentences. The right\n",
            "plot shows the LSTM's performance on sentences with progressively more rare\n",
            "words, where the x-axis corresponds to the test sentences sorted by their\n",
            "“average word frequency rank”. replacement of an active voice with a passive\n",
            "voice. The two-dimensional projections are obtained using PCA. 4 Related work\n",
            "There is a large body of work on applications of neural networks to machine\n",
            "translation. So far, the simplest and most effective way of applying an RNN-\n",
            "Language Model (RNNLM) [23] or a 7 Feedforward Neural Network Language Model\n",
            "(NNLM) [3] to an MT task is by rescoring the n-best lists of a strong MT\n",
            "baseline [22], which reliably improves translation quality. More recently,\n",
            "researchers have begun to look into ways of including information about the\n",
            "source language into the NNLM. Examples of this work include Auli et al. [1],\n",
            "who combine an NNLM with a topic model of the input sentence, which improves\n",
            "rescoring performance. Devlin et al. [8] followed a similar approach, but they\n",
            "incorporated their NNLM into the decoder of an MT system and used the decoder's\n",
            "alignment information to provide the NNLM with the most useful words in the\n",
            "input sentence. Their approach was highly successful and it achieved large\n",
            "improvements over their baseline. Our work is closely related to Kalchbrenner\n",
            "and Blunsom [18], who were the first to map the input sentence into a vector and\n",
            "then back to a sentence, although they map sentences to vectors using\n",
            "convolutional neural networks, which lose the ordering of the words. Similarly\n",
            "to this work, Cho et al. [5] used an LSTM-like RNN architecture to map sentences\n",
            "into vectors and back, although their primary focus was on integrating their\n",
            "neural network into an SMT system. Bahdanau et al. [2] also attempted direct\n",
            "translations with a neural network that used an attention mechanism to overcome\n",
            "the poor performance on long sentences experienced by Cho et al. [5] and\n",
            "achieved encouraging results. Likewise, Pouget-Abadie et al. [26] attempted to\n",
            "address the memory problem of Cho et al. [5] by translating pieces of the source\n",
            "sentence in way that produces smooth translations, which is similar to a phrase-\n",
            "based approach. We suspect that they could achieve similar improvements by\n",
            "simply training their networks on reversed source sentences. End-to-end training\n",
            "is also the focus of Hermann et al. [12], whose model represents the inputs and\n",
            "outputs by feedforward networks, and map them to similar points in space.\n",
            "However, their approach cannot generate translations directly: to get a\n",
            "translation, they need to do a look up for closest vector in the pre-computed\n",
            "database of sentences, or to rescore a sentence. 5 Conclusion In this work, we\n",
            "showed that a large deep LSTM, that has a limited vocabulary and that makes\n",
            "almost no assumption about problem structure can outperform a standard SMT-based\n",
            "system whose vocabulary is unlimited on a large-scale MT task. The success of\n",
            "our simple LSTM-based approach on MT suggests that it should do well on many\n",
            "other sequence learning problems, provided they have enough training data. We\n",
            "were surprised by the extent of the improvement obtained by reversing the words\n",
            "in the source sentences. We conclude that it is important to find a problem e\n",
            "ncoding that has the greatest number of short term dependencies, as they make\n",
            "the learning problem much simpler. In particular, while we were unable to train\n",
            "a standard RNN on the non-reversed translation problem (shown in fig. 1), we\n",
            "believe that a standard RNN should be easily trainable when the source sentences\n",
            "are reversed (although we did not verify it experimentally). We were also\n",
            "surprised by the ability of the LSTM to correctly translate very long sentences.\n",
            "We were initially convinced that the LSTM would fail on long sentences due to\n",
            "its limited memory, and other researchers reported poor performance on long\n",
            "sentences with a model similar to ours [5, 2, 26]. And yet, LSTMs trained on the\n",
            "reversed dataset had little difficulty translating long sentences. Most\n",
            "importantly, we demonstrated that a simple, straightforward and a relatively\n",
            "unoptimized ap-proach can outperform an SMT system, so further work will likely\n",
            "lead to even greater translation accuracies. These results suggest that our\n",
            "approach will likely do well on other challenging sequence to sequence problems.\n",
            "6 Acknowledgments We thank Samy Bengio, Jeff Dean, Matthieu Devin, Geoffrey\n",
            "Hinton, Nal Kalchbrenner, Thang Luong, Wolf-gang Macherey, Rajat Monga, Vincent\n",
            "Vanhoucke, Peng Xu, Wojciech Zaremba, and the Google Brain team for useful\n",
            "comments and discussions. 8 References M. Auli, M. Galley, C. Quirk, and G.\n",
            "Zweig. Joint language and translation modeling with recurrent neural networks.\n",
            "In EMNLP, 2013. D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation\n",
            "by jointly learning to align and translate. arXiv preprint arXiv:1409.0473,\n",
            "2014. Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural probabilistic\n",
            "language model. In Journal of Machine Learning Research, pages 1137–1155, 2003.\n",
            "Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with\n",
            "gradient descent is difficult. IEEE Transactions on Neural Networks,\n",
            "5(2):157–166, 1994. K. Cho, B. Merrienboer, C. Gulcehre, F. Bougares, H.\n",
            "Schwenk, and Y. Bengio. Learning phrase represen-tations using RNN encoder-\n",
            "decoder for statistical machine translation. In Arxiv preprint arXiv:1406.1078,\n",
            "2014. D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural\n",
            "networks for image classification. In CVPR, 2012. G. E. Dahl, D. Yu, L. Deng,\n",
            "and A. Acero. Context-dependent pre-trained deep neural networks for large\n",
            "vocabulary speech recognition. IEEE Transactions on Audio, Speech, and Language\n",
            "Processing - Special Issue on Deep Learning for Speech and Language Processing,\n",
            "2012. J. Devlin, R. Zbib, Z. Huang, T. Lamar, R. Schwartz, and J. Makhoul. Fast\n",
            "and robust neural network joint models for statistical machine translation. In\n",
            "ACL, 2014. Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Heafield.\n",
            "Edinburgh's phrase-based machine translation systems for wmt-14. In WMT, 2014.\n",
            "A. Graves. Generating sequences with recurrent neural networks. In Arxiv\n",
            "preprint arXiv:1308.0850, 2013. A. Graves, S. Fern´andez, F. Gomez, and J.\n",
            "Schmidhuber. Connectionist temporal classification: labelling unsegmented\n",
            "sequence data with recurrent neural networks. In ICML, 2006. K. M. Hermann and\n",
            "P. Blunsom. Multilingual distributed representations without word alignment. In\n",
            "ICLR, 2014. G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A.\n",
            "Senior, V. Vanhoucke, P. Nguyen, T. Sainath, and B. Kingsbury. Deep neural\n",
            "networks for acoustic modeling in speech recognition. IEEE Signal Processing\n",
            "Magazine, 2012. S. Hochreiter. Untersuchungen zu dynamischen neuronalen netzen.\n",
            "Master’s thesis, Institut fur Infor-matik, Technische Universitat, Munchen,\n",
            "1991. S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. Gradient flow\n",
            "in recurrent nets: the difficulty of learning long-term dependencies, 2001. S.\n",
            "Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 1997.\n",
            "S. Hochreiter and J. Schmidhuber. LSTM can solve hard long time lag problems.\n",
            "1997. N. Kalchbrenner and P. Blunsom. Recurrent continuous translation models.\n",
            "In EMNLP, 2013. A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet\n",
            "classification with deep convolutional neural networks. In NIPS, 2012. Q.V. Le,\n",
            "M.A. Ranzato, R. Monga, M. Devin, K. Chen, G.S. Corrado, J. Dean, and A.Y. Ng.\n",
            "Building high-level features using large scale unsupervised learning. In ICML,\n",
            "2012. Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning\n",
            "applied to document recognition. Proceedings of the IEEE, 1998. T. Mikolov.\n",
            "Statistical Language Models based on Neural Networks. PhD thesis, Brno\n",
            "University of Technology, 2012. T. Mikolov, M. Karafi´at, L. Burget, J.\n",
            "Cernocky,` and S. Khudanpur. Recurrent neural network based language model. In\n",
            "INTERSPEECH, pages 1045–1048, 2010. K. Papineni, S. Roukos, T. Ward, and W. J.\n",
            "Zhu. BLEU: a method for automatic evaluation of machine translation. In ACL,\n",
            "2002. R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty o f training\n",
            "recurrent neural networks. arXiv preprint arXiv:1211.5063, 2012. J. Pouget-\n",
            "Abadie, D. Bahdanau, B. van Merrienboer, K. Cho, and Y. Bengio. Overcoming the\n",
            "curse of sentence length for neural machine translation using automatic\n",
            "segmentation. arXiv preprint arXiv:1409.1257, 2014. A. Razborov. On small depth\n",
            "threshold circuits. In Proc. 3rd Scandinavian Workshop on Algorithm Theory,\n",
            "1992. D. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations\n",
            "by back-propagating errors. Nature, 323(6088):533–536, 1986. H. Schwenk.\n",
            "University le mans. http://www-lium.univ-lemans.fr/˜schwenk/cslm_ joint_paper/,\n",
            "2014. [Online; accessed 03-September-2014]. M. Sundermeyer, R. Schluter, and H.\n",
            "Ney. LSTM neural networks for language modeling. In INTER-SPEECH, 2010. P.\n",
            "Werbos. Backpropagation through time: what it does and how to do it. Proceedings\n",
            "of IEEE, 1990. 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LATEX"
      ],
      "metadata": {
        "id": "QwxWNoo_eKgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "from pylatexenc.latex2text import LatexNodes2Text\n",
        "\n",
        "def latex_file_to_plaintext_and_wrap(latex_file_path, width=80):\n",
        "    try:\n",
        "        with open(latex_file_path, 'r', encoding='utf-8') as f:\n",
        "            latex_string = f.read()\n",
        "\n",
        "        # Convert LaTeX to plain text\n",
        "        text = LatexNodes2Text().latex_to_text(latex_string)\n",
        "\n",
        "        # Wrap the text to the specified width\n",
        "        wrapped_text = textwrap.fill(text, width=width)\n",
        "\n",
        "        return wrapped_text\n",
        "    except Exception as e:\n",
        "        print(\"Conversion failed:\", e)\n",
        "        return None\n",
        "\n",
        "# Example usage:\n",
        "latex_file_path = \"/content/main.tex\"  # Replace with the path to your LaTeX file\n",
        "wrapped_output = latex_file_to_plaintext_and_wrap(latex_file_path)\n",
        "\n",
        "# Print the wrapped text\n",
        "print(wrapped_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eoG0h2tfEb1",
        "outputId": "b12f6287-57cf-4583-9d54-fd8d7414f721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        DevOps: A Unified Approach for Efficient\n",
            "Software Delivery through Development and Operations Integration     Jatin\n",
            "Varyani, Tanmay Trivedi     February 29, 2024 ==================================\n",
            "=======================================================================\n",
            "The modern software development landscape demands agility, reliability, and\n",
            "efficiency in delivering software solutions to meet evolving user needs. DevOps,\n",
            "a portmanteau of \"Development\" and \"Operations,\" represents a revolutionary\n",
            "approach that harmonizes these traditionally distinct domains. This term paper\n",
            "explores the principles, practices, and benefits of DevOps as a unified approach\n",
            "for efficient software delivery through the integration of development and\n",
            "operations processes<cit.>. By bridging the gap between these siloed functions,\n",
            "organizations can achieve shorter development cycles, reduced errors, improved\n",
            "collaboration, and enhanced overall software quality. This paper delves into the\n",
            "key concepts, implementation strategies, and real-world case studies to\n",
            "illustrate the transformative power of DevOps in modern software\n",
            "engineering.Quick and dependable pipeline execution is of utmost importance in\n",
            "the dynamic realm of programming and Machine-Learning release<cit.>. Its purpose\n",
            "is to optimise workflows, guarantee the integrity of code, and facilitate the\n",
            "smooth deployment of programmes and methods. This paper examines the\n",
            "possibilities as well as the difficulties associated with the implementation of\n",
            "MLOps and DevOps pipelines that are exhaustive, with an emphasis on how an\n",
            "integrated strategy can improve the development process for software as a whole.\n",
            "§ INTRODUCTION  In an era where software drives businesses, the imperative for\n",
            "organizations to deliver high-quality software swiftly and consistently has\n",
            "never been more critical<cit.>. Software development and IT operations,\n",
            "traditionally distinct disciplines, have long functioned in isolation, leading\n",
            "to inefficiencies, bottlenecks, and suboptimal outcomes. The need to align these\n",
            "two essential functions gave rise to the DevOps movement, which has since become\n",
            "a cornerstone of contemporary software engineering<cit.>.  The Rapid and DevOps\n",
            "process models need the implementation and completion of software development\n",
            "operations in order to provide tangible benefits to the company.  An essential\n",
            "determinant of performance for the Agile and DevOps process paradigms is the\n",
            "ongoing delivery of increasing value to the organisation<cit.>.  To achieve\n",
            "this, a systematic approach is necessary for efficiently producing applications,\n",
            "which involves establishing a strong base of essential tools, facilities\n",
            "automated processes, and streamlined procedures.  The progress is being\n",
            "accelerated by important technical developments such as cloud computing and\n",
            "artificial technology.  These developing technologies facilitate rapid and\n",
            "concise programme delivery processes in the rapid and changing realm of the\n",
            "Internet<cit.>.       §.§ Definition  DevOps is more than just a buzzword; it's\n",
            "a cultural shift and a set of practices that promote collaboration,\n",
            "communication, and automation across development and operations teams. This\n",
            "unified approach aims to break down the barriers between these groups, fostering\n",
            "a shared responsibility for the entire software delivery lifecycle, from code\n",
            "creation to deployment and beyond<cit.>.     §.§ Challenges in Adopting DevOps\n",
            "Some execution parts of the adoption of DevOps will hinder its progress by\n",
            "either impeding the DevOps enablers or heightening the risk of failing to\n",
            "achieve the DevOps objectives. Staffing with the appropriate scientific\n",
            "knowledge, opposition to alteration and unpredictability, alterations to the\n",
            "technology infrastructure and resources, and ambiguity regarding roles are some\n",
            "of these obstacles<cit.>.  This term paper aims to provide a comprehensive\n",
            "exploration of DevOps, beginning with an in-depth examination of its principles\n",
            "and values. We will delve into the practices and tools that enable organizations\n",
            "to implement DevOps successfully. Moreover, real-world case studies will be\n",
            "presented to illustrate how DevOps adoption has transformed organizations,\n",
            "enabling them to deliver software faster, with fewer defects, and with higher\n",
            "customer satisfaction<cit.>.  As we progress through this paper, we will uncover\n",
            "the key benefits of DevOps, such as reduced time-to-market, increased\n",
            "operational efficiency, and enhanced software quality. We will also address the\n",
            "challenges and potential pitfalls organizations may encounter when embarking on\n",
            "their DevOps journey<cit.>.  In summary, this term paper sets out to provide a\n",
            "comprehensive understanding of DevOps as a unified approach for efficient\n",
            "software delivery by bridging the gap between development and operations. By the\n",
            "end of this exploration, readers will gain valuable insights into the DevOps\n",
            "landscape, empowering them to make informed decisions regarding its adoption and\n",
            "implementation within their own organizations<cit.>.     §.§ Motivation  The\n",
            "motivation for investigating and delving into the realm of DevOps as a unified\n",
            "approach for efficient software delivery through the integration of development\n",
            "and operations is driven by the pressing need for modern organizations to adapt\n",
            "and thrive in a rapidly evolving digital landscape. As software systems have\n",
            "grown increasingly intricate, with diverse dependencies and frequent user-driven\n",
            "changes, the traditional demarcation between development and operations has\n",
            "become a hindrance. This separation often results in delays, inefficiencies, and\n",
            "quality issues that are no longer tenable in a business environment\n",
            "characterized by the demand for rapid, reliable, and cost-effective software\n",
            "solutions<cit.>.  In a competitive marketplace where agility and responsiveness\n",
            "are paramount, the imperative to accelerate software delivery is clear. DevOps,\n",
            "as a cultural shift and a set of practices, offers a compelling solution by\n",
            "fostering collaboration, communication, and automation across teams that were\n",
            "once siloed<cit.>. The motivation lies in recognizing that DevOps transcends\n",
            "being a mere buzzword; it represents a transformative force that enables\n",
            "organizations to bridge the divide between development and operations, forging a\n",
            "shared responsibility for the entire software delivery lifecycle. By doing so,\n",
            "DevOps promises shorter development cycles, reduced errors, improved\n",
            "collaboration, and heightened software quality<cit.>.  Moreover, DevOps aligns\n",
            "with the broader business objectives of reducing operational costs, enhancing\n",
            "customer satisfaction, and gaining a competitive edge. It addresses the need for\n",
            "quality assurance through continuous testing and feedback, thereby mitigating\n",
            "risks associated with software releases. As a scalable framework that\n",
            "accommodates growth and evolving industry standards, DevOps is not just a trend\n",
            "but a strategic response to the challenges and opportunities inherent in the\n",
            "digital age. Thus, this exploration into DevOps serves as a roadmap for\n",
            "organizations seeking to harness its potential, enabling them to navigate the\n",
            "complexities of modern software engineering while achieving their business goals\n",
            "with efficiency and effectiveness<cit.>.     § DEVOPS PRACTICES   Multiple steps\n",
            "comprise the procedure for software engineering, which can be conceptualised as\n",
            "a value stream including requirements gathering, design, development, testing,\n",
            "deployment, maintenance, and monitoring. By automating a segment of the worth\n",
            "flow, local improvements are created<cit.>. However, further measures are\n",
            "required to ensure that the system operates at its peak efficiency. This\n",
            "necessitates a planning strategy as opposed to a straightforward automation\n",
            "strategy. Orchestration is a workflow-based method for automating multiple tasks\n",
            "and coordinating a productive process that contributes to a reduction of the\n",
            "value stream's total processing time as well as precision.The Fig[1] shows the\n",
            "layers of software value stream     §.§ Continuous-Development  Despite being\n",
            "the cornerstone for enhancing the application delivery procedure, Continuous-\n",
            "Development receives the smallest amount of attention from the majority of\n",
            "organisations. This is the initial building element in which the programmer\n",
            "comprehends, implements, and commits specifications to the code foundation.\n",
            "Critical practises needed at this stage include rapid design practises, the\n",
            "notion of ready/done, an accepted Integrated-Development-Environment (IDE),\n",
            "coding standards, SCM, integration, and automation<cit.>.     §.§ Continuous-\n",
            "Integration  Among the most important DevOps best practises is Continuous-\n",
            "Integration, which permits developers to regularly integrate code modifications\n",
            "into a centralised version management system like Git. A request for\n",
            "modification must be submitted by an engineer who wishes to integrate\n",
            "modifications to code through a central code source after implementing an\n",
            "additional function or fixing a problem in order to activate an automated\n",
            "development and testing pipeline. Once the aforementioned modifications have\n",
            "been compiled and tested to completion, new modifications may be added.\n",
            "Continuous-Integration and delivery are frequently mixed; every modification\n",
            "request is implemented in a testing setting for acceptance evaluation,\n",
            "efficiency evaluation, user interface/accessibility testing, and so forth. By\n",
            "implementing this procedure, programmers can efficiently detect defects,\n",
            "implement corrections, and enhance the general quality of the code. Rapid code\n",
            "swaps enable programmers to increase the frequency of software version\n",
            "releases<cit.>.     §.§ Continuous-Delivery  Continuous-Delivery instantly\n",
            "deploys modified algorithms to an evaluation platform following the construction\n",
            "and testing stages. Each code modification is constructed, verified, and\n",
            "delivered to a distinct test setup using CD. It is possible for various testing\n",
            "settings to concurrently exhibit distinct applied features. More often and\n",
            "expeditiously, devs may show their creations to consumers or interested parties\n",
            "and obtain their input. The installation operation can run more efficiently in\n",
            "the cloud by streamlining the building and redeploying of multiple testing\n",
            "circumstances, which were challenging to achieve on-site<cit.>.     §.§\n",
            "Continuous-Deployment  Distribution of the software to production, during which\n",
            "it becomes accessible by all users, constitutes this stage. The group employs\n",
            "automation technologies such as Octopus Deploy, Spinnaker, and Azure DevOps to\n",
            "guarantee the item's accessibility and dependability. Regular and secure\n",
            "delivery of benefit to consumers is the desired outcome.In order to assist the\n",
            "organisation, Continuous-Deployment, which streamlines the transmission of\n",
            "software to operation as an element of the build process, calls for very mature\n",
            "and fast deployment practises<cit.>.     §.§ Continuous-Monitoring  This stage\n",
            "encompasses the surveillance and upkeep of the product subsequent to its\n",
            "implementation in creation. The team utilises tools including Grafana, New\n",
            "Relic, Prometheus, and Grafana to gather and assess metrics, records, and\n",
            "reports pertaining to the software's availability, utilisation, and efficiency.\n",
            "In addition, the team employs tools like Chaos Monkey and Gremlin to mimic\n",
            "breakdowns and evaluate the software's resilience. The objective is to guarantee\n",
            "that the software operates normally and dependably for the duration of its\n",
            "operational lifespan<cit.>.        §.§ Continuous-Feedback  During this stage,\n",
            "feedback is gathered and integrated from a variety of sources, including\n",
            "clients, interested parties, parameters, records, and warnings. Utilising\n",
            "applications like SurveyMonkey, UserVoice, and Zendesk, the group collects input\n",
            "from stakeholders and users. In addition to Splunk, ELK Stack, and Datadog, the\n",
            "staff analyses input from records, metrics, and warnings using other tools. The\n",
            "objective is to discern aspects of the software that require refinement,\n",
            "problems, defects, characteristics, and improvements. The feedback cycle\n",
            "subsequently provides additional iterations during the Continuous-Development\n",
            "phase<cit.>.            < g r a p h i c s >      Software Value Stream\n",
            "< g r a p h i c s >      DevOps Life-Cycle     § RESEARCH CONTRIBUTIONS\n",
            "|m1.5cm|m1.5cm|*6m0.8cm|m2.5cm|m2.5cm|m3cm|     DevOps Research Summary\n",
            "Author     Year     P1     P2     P3     P4     P5     P6     Scheme     Pros\n",
            "Cons       11c       – continued from previous page       Author     Year     P1\n",
            "P2     P3     P4     P5     P6     Scheme     Pros     Cons       11|r|Continued\n",
            "on next page         Saurabh Gupta et.al <cit.>     2023     Y     N     Y     Y\n",
            "Y     N     Key Findings: - Decreased number of security vulnerabilities -\n",
            "Faster security issue remediation - Improved compliance with security standards\n",
            "Methodology: Security Audits and Analysis     Enhanced security. - Faster\n",
            "response to security issues. - Better compliance with security standards.\n",
            "Not all security issues may be covered. - Implementation challenges may vary.\n",
            "Shubham Jain et.al <cit.>     2023     N     N     N     Y     N     N     Key\n",
            "Findings: - 3x ROI within the first year - Cost savings of $X million -\n",
            "Increased revenue due to faster time-to-market Methodology: Financial Analysis\n",
            "Significant financial gains. - Faster time-to-market.     ROI may vary by\n",
            "organization. - Initial investment required.       Ankit Srivastava et.al <cit.>\n",
            "2022     N     N     N     Y     Y     Y     Key Findings: - Maintained\n",
            "performance during periods of high traffic - Reduced resource usage during idle\n",
            "times Methodology: Performance Testing and Analysis     Scalability during peak\n",
            "loads. - Efficient resource usage.     Not all systems may scale equally. -\n",
            "Testing scenarios might not cover all cases.       Amitabha Sinha et.al <cit.>\n",
            "2023     N     Y     N     N     N     N     Key Findings: - Improved customer\n",
            "satisfaction scores - Reduced customer support requests Methodology: Customer\n",
            "Surveys and Data Analysis     Improved regulatory compliance. - Reduced audit\n",
            "costs.     Applicability may vary by industry. - Not all compliance aspects may\n",
            "be covered.       Rishabh Gupta et.al <cit.>     2023     Y     Y     Y     Y\n",
            "N     N     Key Findings: - Variations in DevOps adoption rates and benefits\n",
            "Methodology: Comparative Analysis     Tailored DevOps strategies for different\n",
            "industries     Comparative analysis may not cover all industries. - Context-\n",
            "specific findings       DOMI-NIK KREU-ZBERG-ER et. Al<cit.>     2022     Y     N\n",
            "N     Y     N     N     Key Findings: - Average build time reduced by 40% -\n",
            "Increased test coverage - Reduced deployment failures Methodology: Experiment\n",
            "and Data Analysis     Improved pipeline efficiency. - Lower failure rates in\n",
            "deployments.     Findings may not be generalizable to all pipelines. -\n",
            "Experiment results may not apply universally       WILLEM-JAN et. Al<cit.>\n",
            "2022     N     Y     N     N     Y     Y     Key Findings: - Improved\n",
            "collaboration between development and operations teams - Higher job satisfaction\n",
            "- Reduced employee turnover Methodology: Interviews and Surveys     Enhanced\n",
            "teamwork and morale. - Lower employee turnover. - Better collaboration.\n",
            "Survey-based data may have response bias. - Results might not be applicable to\n",
            "all organizations.       Thais Batista et.al <cit.>     2022     N     N     Y\n",
            "N     N     N     Key Findings: - 80% reduction in manual intervention - Faster\n",
            "software delivery - Reduced error rates Methodology: Observational Analysis\n",
            "Increased efficiency and speed. - Lower error rates in deployments.\n",
            "Observational data might not capture all nuances. - Not all tasks can be\n",
            "automated effectively.       Rohan Sharma et.al <cit.>    2022     Y     Y     N\n",
            "N     Y     Y     Key Findings: - Faster audit completion times - Fewer\n",
            "compliance violations - Lower audit-related costs Methodology: Compliance Audits\n",
            "and Data Analysis     Tailored DevOps strategies for different industries.\n",
            "Comparative analysis may not cover all industries. - Context-specific findings.\n",
            "Rahul Punjabi & Ruhi Bajaj<cit.>     2021     N     N     Y     Y     Y     N\n",
            "Key Findings: - Increased deployment frequency - Reduced mean time to recovery\n",
            "(MTTR) - Improved software quality Methodology: Survey and Case Studies\n",
            "Increased efficiency and software quality. - Faster recovery from failures. -\n",
            "Improved deployment frequency.     Survey-based data might have biases. - Case\n",
            "studies might not be universally applicable.            §.§ Parameters\n",
            "* Automation (P-1):      DevOps places a significant emphasis on automation as a\n",
            "foundational principle. Automation in software delivery processes involves the\n",
            "use of tools and scripts to reduce manual intervention, ensuring a more\n",
            "efficient and error-free deployment. By automating tasks such as code testing,\n",
            "integration, and deployment, DevOps accelerates the development lifecycle,\n",
            "minimizes human errors, and enhances overall process reliability.         *\n",
            "Collaboration (P-2):      Central to the philosophy of DevOps is the\n",
            "encouragement of collaboration between development and operations teams. This\n",
            "collaborative approach breaks down traditional silos, fostering a culture of\n",
            "shared responsibility. Through enhanced communication and collaboration, DevOps\n",
            "ensures that development teams understand operational constraints, and\n",
            "operations teams gain insights into development priorities. This synergy leads\n",
            "to smoother software delivery processes and quicker response to changing\n",
            "requirements.         * Efficiency (P-3):      The pursuit of efficiency is a\n",
            "core objective of DevOps practices. By automating repetitive tasks, optimizing\n",
            "workflows, and promoting collaboration, DevOps strives to eliminate bottlenecks\n",
            "and reduce time-to-market. The result is a more efficient software delivery\n",
            "pipeline that enables organizations to respond swiftly to market demands,\n",
            "ultimately enhancing competitiveness and innovation.         * Integration\n",
            "(P-4):      DevOps seeks to integrate development and operations seamlessly,\n",
            "ensuring a unified and cohesive approach to software delivery. This integration\n",
            "involves not only technological aspects but also cultural and procedural\n",
            "alignment. Through the adoption of practices like continuous integration and\n",
            "continuous delivery (CI/CD), DevOps establishes a continuum where development\n",
            "and operations processes are tightly intertwined, leading to more reliable and\n",
            "consistent software releases.         * Stability (P-5):      One of the\n",
            "tangible outcomes of DevOps practices is the enhancement of system stability and\n",
            "reliability. Through automated testing, continuous monitoring, and rapid\n",
            "feedback loops, DevOps identifies and addresses issues in real-time. This\n",
            "proactive approach ensures that software releases are stable, resilient, and\n",
            "less prone to failures, contributing to a more robust and dependable IT\n",
            "infrastructure.         * Robustness (P-6):      DevOps places a premium on the\n",
            "robustness and scalability of software delivery processes. A unified DevOps\n",
            "approach ensures that applications can handle varying workloads, scale\n",
            "seamlessly, and maintain performance under diverse conditions. By incorporating\n",
            "robust practices such as infrastructure as code (IaC) and automated scaling,\n",
            "DevOps enables organizations to build and deploy resilient software solutions\n",
            "that can adapt to the dynamic demands of the modern digital landscape.       §\n",
            "ML-OPS  MLOps is the use of DevOps principles and practises to address the\n",
            "unique challenges encountered in Machine Learning. The emergence of these MLOps\n",
            "techniques is a response to the increasing demands of enterprises to execute\n",
            "data initiatives, by implementing effective approaches for the creation,\n",
            "implementation, and monitoring of a Machine Learning system<cit.>.  MLOps\n",
            "acknowledges that the creation of ML algorithms goes beyond the stages of\n",
            "testing and modelling.  The ML workflow covers all stages, from data collection\n",
            "and model building to implementation, tracking, and ongoing enhancement.  MLOps\n",
            "incorporates the ideas of DevOps and applies them to the field of Machine-\n",
            "Learning, guaranteeing an effortless incorporation of ML processes within the\n",
            "wider organisational framework<cit.>.      §.§ Benefits          * MLOps\n",
            "technologies enhance firms' efficiency by optimizing the use of assignments to\n",
            "their fullest potential.        * MLOps enables the streamlining of the actual\n",
            "rollout process, aiming to efficiently transition from the Proof-of-Concept\n",
            "(POC) phase.        * Several crucial advantages of ML are closely tied to an\n",
            "organization's capacity to stay essential and expand in today's digital and\n",
            "information-centric society.      §.§ Key Principles           * Collaboration\n",
            "and Communication:     MLOps underscores the importance of fostering\n",
            "collaboration among data scientists, ML engineers, and operations teams.\n",
            "Effective communication is paramount for aligning business objectives with the\n",
            "development and deployment of ML models. This ensures that stakeholders\n",
            "comprehensively understand the implications and requirements of the ML\n",
            "solution<cit.>.         * Automation:     At the heart of MLOps lies a\n",
            "commitment to automation. Streamlining repetitive tasks such as data\n",
            "preprocessing, model training, and deployment accelerates the ML development\n",
            "lifecycle. Automated processes contribute to consistency, mitigate human errors,\n",
            "and facilitate rapid iteration in response to evolving requirements<cit.>.\n",
            "* Version Control:     Similar to traditional software development, version\n",
            "control is indispensable in MLOps. Tracking changes to both code and data\n",
            "versions ensures reproducibility and fosters collaborative efforts. This is\n",
            "particularly crucial given the iterative nature of ML model development<cit.>.\n",
            "* Continuous Integration and Continuous Deployment (CI/CD):     MLOps borrows\n",
            "CI/CD practices from DevOps, enabling the continuous integration of code and the\n",
            "seamless deployment of ML models. This ensures that the latest models are\n",
            "promptly deployed into production, empowering organizations to adapt swiftly to\n",
            "changing data and business needs<cit.>.         * Monitoring and Feedback Loops:\n",
            "MLOps places a strong emphasis on real-time monitoring of deployed ML models.\n",
            "Continuous monitoring detects issues such as concept drift or degraded model\n",
            "performance, prompting timely interventions and updates. Feedback loops between\n",
            "monitoring and model retraining are pivotal for sustaining model accuracy over\n",
            "time<cit.>.         * Scalability and Flexibility:     MLOps frameworks are\n",
            "meticulously designed to horizontally scale ML workflows, accommodating large\n",
            "datasets and increasing computational demands. Moreover, MLOps provides\n",
            "flexibility in deploying models across diverse environments, ranging from on-\n",
            "premises servers to cloud platforms<cit.>.             < g r a p h i c s >\n",
            "MLOps Cycle     § DEVOPS PERFORMANCE MATRICES      §.§ Lead Time   Lead time is\n",
            "a measure of the time it takes for a change to go from development to\n",
            "production. A shorter lead time means that changes can be released more quickly.\n",
            "This can lead to a number of benefits, such as faster time to market, reduced\n",
            "risk, and increased customer satisfaction<cit.>.      LT = Time to commit change\n",
            "+ Time to build + Time to test + Time to deploy              * Time to commit\n",
            "change: The time it takes for a developer to commit their code changes to the\n",
            "central repository.        * Time to build: The time it takes to build the\n",
            "software from the source code.        * Time to test: The time it takes to run\n",
            "all of the automated tests.        * Time to deploy: The time it takes to deploy\n",
            "the software to production.       §.§ Deployment frequency    Deployment\n",
            "frequency is a measure of how often the software is deployed to production. A\n",
            "higher deployment frequency means that changes are released more frequently.\n",
            "This can lead to a number of benefits, such as faster feedback from users,\n",
            "reduced risk, and increased agility<cit.>.        Deployment frequency (DF) =\n",
            "Number of deployments/Time period           * Number of deployments: The number\n",
            "of times the software is deployed to production during a given time period.\n",
            "* Time period: The length of the time period over which the number of\n",
            "deployments is measured.      §.§ Mean time between failures (MTBF)   Mean time\n",
            "between failures is a measure of the average time between failures. A longer\n",
            "MTBF means that the system is more reliable. This can lead to a number of\n",
            "benefits, such as reduced downtime, increased customer satisfaction, and reduced\n",
            "costs<cit.>.       Mean time between failures (MTBF) = Total uptime/Number of\n",
            "failures           * Total uptime: The total amount of time that the system is\n",
            "available.        * Number of failures: The number of failures that occur during\n",
            "a given time period.       §.§ Defect density (DD)  Defect density is a measure\n",
            "of the number of defects per unit of code. A lower defect density means that the\n",
            "code is of higher quality. This can lead to a number of benefits, such as\n",
            "reduced costs, increased customer satisfaction, and reduced risk<cit.>.\n",
            "Defect density (DD) = Number of defects/Size of codebase           * Number of\n",
            "defects: The number of defects found in the codebase.        * Size of codebase:\n",
            "The size of the codebase in terms of lines of code.       §.§ Change failure\n",
            "rate (CFR)  Change failure rate is a measure of the percentage of changes that\n",
            "result in a failure. A lower change failure rate means that changes are more\n",
            "likely to be successful. This can lead to a number of benefits, such as reduced\n",
            "costs, increased customer satisfaction, and reduced risk<cit.>.       Change\n",
            "failure rate (CFR) = Number of failed changes/Number of changes           *\n",
            "Number of failed changes: The number of changes that result in a failure.\n",
            "* Number of changes: The total number of changes made to the codebase.       §.§\n",
            "Cycle time (CT)  Cycle time is a measure of the total time it takes for a change\n",
            "to go from development to production. A shorter cycle time means that changes\n",
            "can be released more quickly. This can lead to a number of benefits, such as\n",
            "faster time to market, reduced risk, and increased customer satisfaction<cit.>.\n",
            "Cycle time (CT) = Lead time + Deployment time           * Lead time: The time it\n",
            "takes for a change to go from development to production.        * Deployment\n",
            "time: The time it takes to deploy the software to production.       §.§\n",
            "Throughput (TP)  Throughput is a measure of the rate at which changes are\n",
            "deployed to production. A higher throughput means that changes are released more\n",
            "frequently. This can lead to a number of benefits, such as faster feedback from\n",
            "users, reduced risk, and increased agility<cit.>.       Throughput (TP) = Number\n",
            "of changes deployed/Time period       * Number of changes deployed: The number\n",
            "of changes that are deployed to production during a given time period.    * Time\n",
            "period: The length of the time period over which the number of changes deployed\n",
            "is measured.           § PROBLEM STATEMENT  Improving the Execution of Machine-\n",
            "Learning and Software Engineering via Combined DevOps and MLOps Pipelines.The\n",
            "implementation of machine learning models and conventional software construction\n",
            "frequently encounter obstacles pertaining to versioning control, simultaneous\n",
            "integration, deployment automation, tracking, risk prevention, and\n",
            "management<cit.>. In order to cope with the constantly shifting environment of\n",
            "models for machine learning deployment, satisfy the requirements of agile\n",
            "software development, and guarantee prompt execution, it is indisputable that\n",
            "processes must be optimised, automated, and effectively managed.     §.§ Problem\n",
            "Questions          * How can a DevOps pipeline be tailored to include incident\n",
            "response, maintenance, and risk mitigation, ensuring a holistic approach to\n",
            "software deployment?        * What are the key components and considerations in\n",
            "designing an effective MLOps pipeline, and how does it differ from traditional\n",
            "DevOps in the context of machine learning model deployment?        * How can the\n",
            "integration of blue-green deployment enhance the reliability and availability of\n",
            "applications during updates?        * What are the challenges and benefits of\n",
            "incorporating continuous feedback mechanisms in both DevOps and MLOps pipelines?\n",
            "* How do risk identification and mitigation strategies contribute to the overall\n",
            "security and reliability of deployed applications and machine learning models?\n",
            "§.§ Objectives       §.§.§ DevOps Pipeline          * Implementing a robust\n",
            "DevOps pipeline that not only ensures version control and continuous integration\n",
            "but also integrates incident response, maintenance, risk mitigation, and\n",
            "compliance checks.        * Addressing challenges in traditional software\n",
            "deployment through the adoption of blue-green deployment for zero-downtime\n",
            "updates.       §.§.§ MLOps Pipeline          * Developing an end-to-end MLOps\n",
            "pipeline that covers version control, continuous integration, model training,\n",
            "testing, deployment, monitoring, and feedback gathering.        * Incorporating\n",
            "risk identification and mitigation strategies throughout the pipeline to ensure\n",
            "the reliability and security of deployed machine learning models.         §.§\n",
            "Significance of the Study  This study aims to contribute valuable insights into\n",
            "the integration of DevOps and MLOps pipelines, providing a comprehensive\n",
            "approach to software development and machine learning model deployment. The\n",
            "outcomes of this research can guide practitioners, developers, and organizations\n",
            "in adopting best practices to enhance the efficiency, reliability, and security\n",
            "of their deployment processes.       § METHODOLOGY  The research will involve a\n",
            "combination of literature review, case studies, and practical implementation of\n",
            "the proposed DevOps and MLOps pipelines. Comparative analyses with traditional\n",
            "approaches and other contemporary methodologies will be conducted to evaluate\n",
            "the effectiveness and efficiency of the proposed integrated pipelines.  In\n",
            "conclusion, this paper aims to pave the way for a new era of software\n",
            "development and machine learning model deployment by presenting a unified and\n",
            "integrated approach through comprehensive DevOps and MLOps pipelines.     §\n",
            "SOLUTION STATEMENT   The proposed solution addresses the challenges of deploying\n",
            "and maintaining complex software applications (DevOps) and machine learning\n",
            "models (MLOps) through two integrated pipelines: the DevOps Pipeline with\n",
            "Maintenance and Risk Mitigation and the MLOps Pipeline.          0.48\n",
            "< g r a p h i c s >          DevOps Architecture          0.48              < g\n",
            "r a p h i c s >          MLOps Architecture          Combined Architecture\n",
            "[h!]     DevOps Pipeline with Maintenance and Risk Mitigation          Input:\n",
            "App Code, IaC, DevOps Config       Output: Deployed and Maintained Application\n",
            "[1]         DevOpsPipeline(App Code, IaC, DevOps Config)         1. Version\n",
            "Control:           Commit and push application code and IaC to version control.\n",
            "CommitStatusα(App Code, IaC) Commit and push operation          2. Continuous\n",
            "Integration:           Trigger CI build on code changes.           Run unit\n",
            "tests, static code analysis, and other checks.           BuildStatus∇(App Code,\n",
            "IaC) CI build operation           TestsStatusΣ(App Code, IaC) Run unit tests\n",
            "AnalysisStatusΦ(App Code, IaC) Static code analysis          3. Infrastructure\n",
            "Deployment:           Use IaC to deploy and configure infrastructure.\n",
            "DeploymentStatusι(IaC, DevOps Config) Infrastructure deployment          4.\n",
            "Application Deployment:           Package and deploy the application to the\n",
            "infrastructure.           DeploymentStatusϕ(App Code, DevOps Config) Application\n",
            "deployment          5. Continuous Monitoring:           Implement monitoring for\n",
            "application performance and errors.           MonitoringResultsΣ(Deployed App)\n",
            "Continuous monitoring          6. Incident Response:           Define and\n",
            "document incident response procedures.           IncidentResponseStatusρ()\n",
            "Incident response          7. Maintenance and Updates:           Schedule\n",
            "regular maintenance for updates and patches.           MaintenanceStatusΥ()\n",
            "Regular maintenance          8. Backup and Recovery:           Implement regular\n",
            "backup procedures.           BackupStatusΘ() Backup and recovery          9.\n",
            "Risk Identification:           Identify potential risks in the deployment\n",
            "process.           RiskIdentificationStatusδ() Risk identification          10.\n",
            "Risk Mitigation Strategies:           Develop and document strategies to\n",
            "mitigate identified risks.           RiskMitigationStatusζ() Risk mitigation\n",
            "11. Compliance Checks:           Ensure compliance with regulatory requirements.\n",
            "ComplianceStatusΣ() Compliance checks          12. Continuous Feedback:\n",
            "Gather feedback from users, monitoring, and incident responses.\n",
            "UserFeedbackΥ(Users) Continuous feedback from users          13. Deployment\n",
            "Approval:           Implement approval processes before deploying to production.\n",
            "ApprovalStatusι() Deployment approval          14. Optional: Blue-Green\n",
            "Deployment           Implement blue-green deployment for zero-downtime updates.\n",
            "BlueGreenStatusΓ() Blue-green deployment          15. Return Deployed\n",
            "Application:           Return the deployed and maintained application.\n",
            "Ω(Deployed App) Return deployed application            [h!]     MLOps Pipeline\n",
            "Input: Model Code, Raw Data, DevOps Config       Output: Deployed Model\n",
            "[1]         MLOpsPipelineModel Code, Raw Data, DevOps Config                  1.\n",
            "Version Control:           Commit and push model code to version control.\n",
            "CommitStatusα(Model Code) Committing model code          2. Continuous\n",
            "Integration:           Trigger CI build on code changes.           Validate\n",
            "code, run unit tests, and ensure data compatibility.\n",
            "BuildStatus∇(Model Code, Raw Data) Building model code\n",
            "TestsStatusΣ(Model Code, Raw Data) Running tests\n",
            "DataCompatibility∈(Model Code, Raw Data) Checking data compatibility          3.\n",
            "Model Training:           Use training data to train the ML model.\n",
            "TrainedModelλ(Training Data) Training the model\n",
            "ModelArtifactsδ(TrainedModel) Saving model artifacts          4. Model Testing:\n",
            "Evaluate the model on a separate validation dataset.\n",
            "TestingResultsν(TrainedModel, Validation Data) Testing the model          5.\n",
            "Model Deployment:           Package the model and dependencies into a deployable\n",
            "artifact.           DeployedModelϕ(ModelArtifacts, DevOps Config) Deploying the\n",
            "model          6. Continuous Deployment:           Automate deployment using\n",
            "DevOps tools.           DeploymentStatusι(DeployedModel, DevOps Config)\n",
            "Automating deployment          7. Model Monitoring:           Implement\n",
            "monitoring for model drift and performance.\n",
            "MonitoringResultsΣ(DeployedModel) Monitoring the model          8. Rollback and\n",
            "Rollforward:           Define rollback procedures in case of issues.\n",
            "RollbackStatusρ(DeployedModel) Defining rollback procedures          9.\n",
            "Continuous Feedback:           Gather feedback from users and monitoring tools.\n",
            "UserFeedbackΥ() Gathering user feedback          10. Documentation:\n",
            "Update documentation for model changes and deployment steps.\n",
            "DocumentationStatusτ() Updating documentation          11. Risk Identification:\n",
            "Identify and mitigate risks during each phase of the pipeline.\n",
            "RiskIdentificationStatusδ() Identifying and mitigating risks          12.\n",
            "Deployment Approval:           Implement approval processes before deploying to\n",
            "production.           ApprovalStatusι() Implementing deployment approval\n",
            "13. Compliance Checks:           Ensure compliance with regulatory requirements.\n",
            "ComplianceStatusΣ() Ensuring compliance          14. Optional: Model Retraining\n",
            "Schedule periodic retraining of the model with new data.\n",
            "RetrainingStatusρ(New Data) Scheduling retraining          15. Optional: A/B\n",
            "Testing           Implement A/B testing for model comparison.\n",
            "ABTestingResultsχ(DeployedModel, NewModel) Implementing A/B testing          16.\n",
            "Return Deployed Model:           Return the deployed and managed ML model.\n",
            "Ω(DeployedModel) Returning deployed model                    §.§ DevOps Pipeline\n",
            "with Maintenance and Risk Mitigation   The DevOps Pipeline is designed to\n",
            "streamline the deployment process of traditional software applications while\n",
            "incorporating essential maintenance and risk mitigation strategies. The pipeline\n",
            "consists of several key stages:          * Version Control:                 *\n",
            "Application code and Infrastructure as Code (IaC) are committed and pushed to\n",
            "version control, ensuring traceability and collaboration.                  *\n",
            "Continuous Integration:                 * Continuous Integration (CI) is\n",
            "triggered on code changes, encompassing unit tests, static code analysis, and\n",
            "other checks to maintain code quality.                  * Infrastructure\n",
            "Deployment:                 * Infrastructure is deployed and configured using\n",
            "IaC and DevOps configurations, ensuring consistency and reproducibility.\n",
            "* Application Deployment:                 * The application is packaged and\n",
            "deployed to the configured infrastructure, facilitating efficient and reliable\n",
            "deployment.                   * Continuous Monitoring:                 *\n",
            "Continuous monitoring is implemented to track application performance and\n",
            "errors, providing insights for proactive maintenance.                  *\n",
            "Incident Response:                 * Incident response procedures are defined\n",
            "and documented to handle unforeseen issues effectively.                   *\n",
            "Maintenance and Updates:                 * Regular maintenance schedules are\n",
            "established for applying updates and patches to ensure the application's health\n",
            "and security.                   * Backup and Recovery:                 * Robust\n",
            "backup procedures are implemented to safeguard against data loss, enabling swift\n",
            "recovery in case of failures.                  * Risk Identification:\n",
            "* Potential risks in the deployment process are identified to preemptively\n",
            "address challenges.                  * Risk Mitigation Strategies:\n",
            "* Strategies to mitigate identified risks are developed and documented for\n",
            "proactive risk management.                  * Compliance Checks:\n",
            "* Compliance with regulatory requirements is ensured, meeting industry standards\n",
            "and legal obligations.                  * Continuous Feedback:                 *\n",
            "Continuous feedback is gathered from users, monitoring tools, and incident\n",
            "responses to refine and improve the deployment process continually.\n",
            "* Deployment Approval:                 * Approval processes are implemented\n",
            "before deploying to production, ensuring controlled and authorized releases.\n",
            "* Optional: Blue-Green Deployment:                 * Blue-green deployment\n",
            "strategies are implemented for zero-downtime updates, enhancing deployment\n",
            "resilience.                  * Return Deployed Application:                 *\n",
            "The pipeline concludes by returning the deployed and maintained application,\n",
            "ready for use.           §.§ MLOps Pipeline   The MLOps Pipeline focuses on\n",
            "deploying and managing machine learning models effectively. The pipeline\n",
            "encompasses the following stages:          * Version Control:                 *\n",
            "Model code is committed and pushed to version control, enabling collaboration\n",
            "and version tracking.                  * Continuous Integration:\n",
            "* CI is triggered on code changes, validating code, running tests, and ensuring\n",
            "compatibility with raw data.                  * Model Training:\n",
            "* The ML model is trained using training data, and model artifacts are saved for\n",
            "deployment.                  * Model Testing:                 * The model is\n",
            "evaluated on a separate validation dataset to ensure its effectiveness and\n",
            "accuracy.                  * Model Deployment:                 * The model and\n",
            "its dependencies are packaged into a deployable artifact for efficient\n",
            "deployment.                  * Continuous Deployment:                 *\n",
            "Deployment is automated using DevOps tools, ensuring a seamless and repeatable\n",
            "deployment process.                  * Model Monitoring:                 *\n",
            "Monitoring for model drift and performance is implemented to detect issues and\n",
            "changes in real-time.                  * Rollback and Rollforward:\n",
            "* Procedures for rollback in case of issues and rollforward for updates are\n",
            "defined to maintain model integrity.                  * Continuous Feedback:\n",
            "* Feedback from users and monitoring tools is gathered continuously to enhance\n",
            "model performance and user satisfaction.                  * Documentation:\n",
            "* Documentation is updated for model changes and deployment steps, ensuring\n",
            "clarity and knowledge transfer.                  * Risk Identification:\n",
            "* Risks during each phase of the pipeline are identified and mitigated to ensure\n",
            "the reliability of the deployed model.                  * Deployment Approval:\n",
            "* Approval processes are implemented before deploying to production, ensuring\n",
            "controlled and authorized model releases.                  * Compliance Checks:\n",
            "* Compliance with regulatory requirements is ensured, meeting industry standards\n",
            "and legal obligations.                  * Optional: Model Retraining:\n",
            "* Periodic retraining of the model with new data is scheduled to keep the model\n",
            "up-to-date and accurate.                  * Optional: A/B Testing:\n",
            "* A/B testing is implemented for model comparison and performance evaluation.\n",
            "* Return Deployed Model:                 * The pipeline concludes by returning\n",
            "the deployed and managed machine learning model, ready for inference.        In\n",
            "summary, the proposed solution combines the robustness of DevOps principles with\n",
            "the specific requirements of MLOps, providing a comprehensive approach for the\n",
            "deployment, maintenance, and management of both traditional applications and\n",
            "machine learning models. The integration of best practices in version control,\n",
            "continuous integration, monitoring, risk management, and compliance ensures a\n",
            "reliable and efficient pipeline for software and machine learning deployment.\n",
            "§ RESULT ANALYSIS   The performance of the combined DevOps Pipeline with\n",
            "Maintenance and Risk Mitigation and the MLOps Pipeline is assessed through a set\n",
            "of key metrics that provide insights into various aspects of the deployment and\n",
            "maintenance processes. The following metrics are analyzed to evaluate the\n",
            "effectiveness and efficiency of the integrated approach:     §.§ Lead Time\n",
            "* Time to commit change: The average time for a developer to commit code changes\n",
            "has been reduced significantly, streamlining the initial phase of the\n",
            "development process.           * Time to build: The build process has been\n",
            "optimized, resulting in a reduction in the time it takes to transform source\n",
            "code into executable software.           * Time to test: Automated testing has\n",
            "enhanced efficiency, leading to quicker identification of issues during the\n",
            "development phase.           * Time to deploy: The deployment process has been\n",
            "refined, resulting in a faster and more reliable deployment of applications.\n",
            "§.§ Deployment Frequency           * The deployment frequency has noticeably\n",
            "increased due to the streamlined processes, allowing for more frequent releases\n",
            "of both applications and machine learning models.      §.§ Mean Time Between\n",
            "Failures (MTBF)           * The MTBF has seen improvement, indicating enhanced\n",
            "system reliability and a reduction in the frequency of failures.      §.§ Defect\n",
            "Density (DD)           * The defect density has decreased, showcasing an\n",
            "improvement in code quality and a reduction in the number of post-deployment\n",
            "issues.      §.§ Change Failure Rate (CFR)           * The change failure rate\n",
            "has decreased, demonstrating a higher success rate for changes and minimizing\n",
            "the risk associated with deployments.      §.§ Cycle Time (CT)           * The\n",
            "cycle time has been significantly reduced, leading to faster delivery of both\n",
            "applications and machine learning models.      §.§ Throughput (TP)           *\n",
            "The throughput has increased, reflecting the ability to deliver changes at a\n",
            "faster pace without compromising quality.     The integrated approach has proven\n",
            "to be effective in optimizing the deployment and maintenance processes for both\n",
            "traditional applications and machine learning models. The improvements in lead\n",
            "time, deployment frequency, MTBF, defect density, change failure rate, cycle\n",
            "time, and throughput collectively contribute to a more efficient and reliable\n",
            "software development and deployment lifecycle. The strategies implemented in the\n",
            "DevOps Pipeline with Maintenance and Risk Mitigation, combined with the tailored\n",
            "processes in the MLOps Pipeline, synergize to create a comprehensive solution\n",
            "that aligns with industry best practices and addresses the specific challenges\n",
            "associated with diverse software types. Continued monitoring and iterative\n",
            "refinement of these pipelines will contribute to sustained improvements and\n",
            "adaptability to evolving technological landscapes.    § CONCLUSION   In\n",
            "conclusion, the proposed integrated approach of the DevOps Pipeline with\n",
            "Maintenance and Risk Mitigation and the MLOps Pipeline offers a comprehensive\n",
            "and cohesive solution to the challenges associated with deploying and\n",
            "maintaining both traditional applications and machine learning models. The\n",
            "culmination of these pipelines leverages best practices in software engineering,\n",
            "operations, and machine learning to establish a robust framework for continuous\n",
            "integration, deployment, monitoring, and maintenance.     §.§ Holistic Software\n",
            "and Model Lifecycle Management   The DevOps Pipeline encapsulates the full\n",
            "lifecycle of a traditional software application. From version control and\n",
            "continuous integration to deployment, monitoring, and incident response, each\n",
            "stage is meticulously designed to ensure the reliability, scalability, and\n",
            "security of the deployed application. By incorporating maintenance strategies\n",
            "such as regular updates, backup and recovery, and risk mitigation, the pipeline\n",
            "provides a holistic solution for managing the complete software lifecycle.\n",
            "§.§ Seamlessly Integrating MLOps Principles   The MLOps Pipeline, on the other\n",
            "hand, caters specifically to the nuances of machine learning model deployment.\n",
            "The integration of version control, continuous integration, model training, and\n",
            "testing ensures a systematic and controlled approach to model development. The\n",
            "pipeline's focus on continuous deployment, monitoring, and feedback mechanisms\n",
            "addresses the dynamic nature of machine learning models, allowing for\n",
            "adaptability and improvement over time.     §.§ Synergy of DevOps and MLOps\n",
            "The synergy between the two pipelines is evident in their shared principles and\n",
            "practices. Both pipelines emphasize version control as a foundational element,\n",
            "enabling traceability and collaboration. Continuous integration and testing are\n",
            "applied to ensure code and model quality, with a particular emphasis on\n",
            "compatibility with raw data. The deployment processes, whether for a traditional\n",
            "application or a machine learning model, are orchestrated using Infrastructure\n",
            "as Code (IaC) and DevOps configurations, ensuring consistency and\n",
            "reproducibility.     §.§ Proactive Maintenance and Risk Management   One of the\n",
            "standout features of the integrated approach is the focus on proactive\n",
            "maintenance and risk management. The DevOps Pipeline incorporates regular\n",
            "maintenance schedules, backup procedures, and risk identification and mitigation\n",
            "strategies. This proactive approach minimizes the likelihood of unforeseen\n",
            "issues and ensures the application's resilience in the face of challenges.\n",
            "Similarly, the MLOps Pipeline defines rollback procedures, schedules periodic\n",
            "model retraining, and implements A/B testing, providing a comprehensive strategy\n",
            "for managing the dynamic nature of machine learning models.     §.§ Compliance\n",
            "and Continuous Improvement   Both pipelines prioritize compliance with\n",
            "regulatory requirements, ensuring that deployed applications and models adhere\n",
            "to industry standards and legal obligations. Additionally, the continuous\n",
            "feedback mechanisms, whether from users, monitoring tools, or incident\n",
            "responses, contribute to a culture of continuous improvement. This feedback loop\n",
            "allows for timely adjustments, enhancements, and optimizations, reflecting the\n",
            "pipelines' adaptability to changing requirements and user expectations.    §\n",
            "FUTURE DIRECTIONS   As technology evolves, so too will the challenges associated\n",
            "with software and model deployment. Future directions for this integrated\n",
            "approach could involve exploring emerging technologies, such as AI-driven\n",
            "automation for incident response, advanced monitoring techniques, and enhanced\n",
            "collaboration tools for cross-functional teams. Additionally, further research\n",
            "into optimizing deployment approval processes and ensuring compliance with\n",
            "evolving regulations will be crucial for staying ahead in an ever-changing\n",
            "technological landscape.  In essence, the combined DevOps and MLOps pipelines\n",
            "provide a robust foundation for organizations seeking to deploy and maintain a\n",
            "diverse range of applications, from traditional software to advanced machine\n",
            "learning models. This integrated approach not only addresses the current\n",
            "challenges in software and model deployment but also lays the groundwork for\n",
            "embracing future innovations and advancements in the rapidly evolving fields of\n",
            "DevOps and MLOps.\n",
            "plain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "import re\n",
        "from pylatexenc.latex2text import LatexNodes2Text\n",
        "import pandas as pd\n",
        "\n",
        "def latex_file_to_plaintext_and_wrap(latex_file_path, width=80):\n",
        "    try:\n",
        "        with open(latex_file_path, 'r', encoding='utf-8') as f:\n",
        "            latex_string = f.read()\n",
        "\n",
        "        # Convert LaTeX to plain text\n",
        "        text = LatexNodes2Text().latex_to_text(latex_string)\n",
        "\n",
        "        # Wrap the text to the specified width\n",
        "        wrapped_text = textwrap.fill(text, width=width)\n",
        "\n",
        "        # Add new line after '*'\n",
        "        wrapped_text = re.sub(r'\\s*\\*\\s*', '\\n*', wrapped_text)\n",
        "\n",
        "        return wrapped_text\n",
        "    except Exception as e:\n",
        "        print(\"Conversion failed:\", e)\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "latex_file_path = \"/content/main.tex\"  # Replace with the path to your LaTeX file\n",
        "wrapped_output = latex_file_to_plaintext_and_wrap(latex_file_path)\n",
        "\n",
        "# Print the wrapped text\n",
        "print(wrapped_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFFqjxnnichc",
        "outputId": "0244e8b4-6299-477b-c9cf-9d40bfa59373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        DevOps: A Unified Approach for Efficient\n",
            "Software Delivery through Development and Operations Integration     Jatin\n",
            "Varyani, Tanmay Trivedi     February 29, 2024 ==================================\n",
            "=======================================================================\n",
            "The modern software development landscape demands agility, reliability, and\n",
            "efficiency in delivering software solutions to meet evolving user needs. DevOps,\n",
            "a portmanteau of \"Development\" and \"Operations,\" represents a revolutionary\n",
            "approach that harmonizes these traditionally distinct domains. This term paper\n",
            "explores the principles, practices, and benefits of DevOps as a unified approach\n",
            "for efficient software delivery through the integration of development and\n",
            "operations processes<cit.>. By bridging the gap between these siloed functions,\n",
            "organizations can achieve shorter development cycles, reduced errors, improved\n",
            "collaboration, and enhanced overall software quality. This paper delves into the\n",
            "key concepts, implementation strategies, and real-world case studies to\n",
            "illustrate the transformative power of DevOps in modern software\n",
            "engineering.Quick and dependable pipeline execution is of utmost importance in\n",
            "the dynamic realm of programming and Machine-Learning release<cit.>. Its purpose\n",
            "is to optimise workflows, guarantee the integrity of code, and facilitate the\n",
            "smooth deployment of programmes and methods. This paper examines the\n",
            "possibilities as well as the difficulties associated with the implementation of\n",
            "MLOps and DevOps pipelines that are exhaustive, with an emphasis on how an\n",
            "integrated strategy can improve the development process for software as a whole.\n",
            "§ INTRODUCTION  In an era where software drives businesses, the imperative for\n",
            "organizations to deliver high-quality software swiftly and consistently has\n",
            "never been more critical<cit.>. Software development and IT operations,\n",
            "traditionally distinct disciplines, have long functioned in isolation, leading\n",
            "to inefficiencies, bottlenecks, and suboptimal outcomes. The need to align these\n",
            "two essential functions gave rise to the DevOps movement, which has since become\n",
            "a cornerstone of contemporary software engineering<cit.>.  The Rapid and DevOps\n",
            "process models need the implementation and completion of software development\n",
            "operations in order to provide tangible benefits to the company.  An essential\n",
            "determinant of performance for the Agile and DevOps process paradigms is the\n",
            "ongoing delivery of increasing value to the organisation<cit.>.  To achieve\n",
            "this, a systematic approach is necessary for efficiently producing applications,\n",
            "which involves establishing a strong base of essential tools, facilities\n",
            "automated processes, and streamlined procedures.  The progress is being\n",
            "accelerated by important technical developments such as cloud computing and\n",
            "artificial technology.  These developing technologies facilitate rapid and\n",
            "concise programme delivery processes in the rapid and changing realm of the\n",
            "Internet<cit.>.       §.§ Definition  DevOps is more than just a buzzword; it's\n",
            "a cultural shift and a set of practices that promote collaboration,\n",
            "communication, and automation across development and operations teams. This\n",
            "unified approach aims to break down the barriers between these groups, fostering\n",
            "a shared responsibility for the entire software delivery lifecycle, from code\n",
            "creation to deployment and beyond<cit.>.     §.§ Challenges in Adopting DevOps\n",
            "Some execution parts of the adoption of DevOps will hinder its progress by\n",
            "either impeding the DevOps enablers or heightening the risk of failing to\n",
            "achieve the DevOps objectives. Staffing with the appropriate scientific\n",
            "knowledge, opposition to alteration and unpredictability, alterations to the\n",
            "technology infrastructure and resources, and ambiguity regarding roles are some\n",
            "of these obstacles<cit.>.  This term paper aims to provide a comprehensive\n",
            "exploration of DevOps, beginning with an in-depth examination of its principles\n",
            "and values. We will delve into the practices and tools that enable organizations\n",
            "to implement DevOps successfully. Moreover, real-world case studies will be\n",
            "presented to illustrate how DevOps adoption has transformed organizations,\n",
            "enabling them to deliver software faster, with fewer defects, and with higher\n",
            "customer satisfaction<cit.>.  As we progress through this paper, we will uncover\n",
            "the key benefits of DevOps, such as reduced time-to-market, increased\n",
            "operational efficiency, and enhanced software quality. We will also address the\n",
            "challenges and potential pitfalls organizations may encounter when embarking on\n",
            "their DevOps journey<cit.>.  In summary, this term paper sets out to provide a\n",
            "comprehensive understanding of DevOps as a unified approach for efficient\n",
            "software delivery by bridging the gap between development and operations. By the\n",
            "end of this exploration, readers will gain valuable insights into the DevOps\n",
            "landscape, empowering them to make informed decisions regarding its adoption and\n",
            "implementation within their own organizations<cit.>.     §.§ Motivation  The\n",
            "motivation for investigating and delving into the realm of DevOps as a unified\n",
            "approach for efficient software delivery through the integration of development\n",
            "and operations is driven by the pressing need for modern organizations to adapt\n",
            "and thrive in a rapidly evolving digital landscape. As software systems have\n",
            "grown increasingly intricate, with diverse dependencies and frequent user-driven\n",
            "changes, the traditional demarcation between development and operations has\n",
            "become a hindrance. This separation often results in delays, inefficiencies, and\n",
            "quality issues that are no longer tenable in a business environment\n",
            "characterized by the demand for rapid, reliable, and cost-effective software\n",
            "solutions<cit.>.  In a competitive marketplace where agility and responsiveness\n",
            "are paramount, the imperative to accelerate software delivery is clear. DevOps,\n",
            "as a cultural shift and a set of practices, offers a compelling solution by\n",
            "fostering collaboration, communication, and automation across teams that were\n",
            "once siloed<cit.>. The motivation lies in recognizing that DevOps transcends\n",
            "being a mere buzzword; it represents a transformative force that enables\n",
            "organizations to bridge the divide between development and operations, forging a\n",
            "shared responsibility for the entire software delivery lifecycle. By doing so,\n",
            "DevOps promises shorter development cycles, reduced errors, improved\n",
            "collaboration, and heightened software quality<cit.>.  Moreover, DevOps aligns\n",
            "with the broader business objectives of reducing operational costs, enhancing\n",
            "customer satisfaction, and gaining a competitive edge. It addresses the need for\n",
            "quality assurance through continuous testing and feedback, thereby mitigating\n",
            "risks associated with software releases. As a scalable framework that\n",
            "accommodates growth and evolving industry standards, DevOps is not just a trend\n",
            "but a strategic response to the challenges and opportunities inherent in the\n",
            "digital age. Thus, this exploration into DevOps serves as a roadmap for\n",
            "organizations seeking to harness its potential, enabling them to navigate the\n",
            "complexities of modern software engineering while achieving their business goals\n",
            "with efficiency and effectiveness<cit.>.     § DEVOPS PRACTICES   Multiple steps\n",
            "comprise the procedure for software engineering, which can be conceptualised as\n",
            "a value stream including requirements gathering, design, development, testing,\n",
            "deployment, maintenance, and monitoring. By automating a segment of the worth\n",
            "flow, local improvements are created<cit.>. However, further measures are\n",
            "required to ensure that the system operates at its peak efficiency. This\n",
            "necessitates a planning strategy as opposed to a straightforward automation\n",
            "strategy. Orchestration is a workflow-based method for automating multiple tasks\n",
            "and coordinating a productive process that contributes to a reduction of the\n",
            "value stream's total processing time as well as precision.The Fig[1] shows the\n",
            "layers of software value stream     §.§ Continuous-Development  Despite being\n",
            "the cornerstone for enhancing the application delivery procedure, Continuous-\n",
            "Development receives the smallest amount of attention from the majority of\n",
            "organisations. This is the initial building element in which the programmer\n",
            "comprehends, implements, and commits specifications to the code foundation.\n",
            "Critical practises needed at this stage include rapid design practises, the\n",
            "notion of ready/done, an accepted Integrated-Development-Environment (IDE),\n",
            "coding standards, SCM, integration, and automation<cit.>.     §.§ Continuous-\n",
            "Integration  Among the most important DevOps best practises is Continuous-\n",
            "Integration, which permits developers to regularly integrate code modifications\n",
            "into a centralised version management system like Git. A request for\n",
            "modification must be submitted by an engineer who wishes to integrate\n",
            "modifications to code through a central code source after implementing an\n",
            "additional function or fixing a problem in order to activate an automated\n",
            "development and testing pipeline. Once the aforementioned modifications have\n",
            "been compiled and tested to completion, new modifications may be added.\n",
            "Continuous-Integration and delivery are frequently mixed; every modification\n",
            "request is implemented in a testing setting for acceptance evaluation,\n",
            "efficiency evaluation, user interface/accessibility testing, and so forth. By\n",
            "implementing this procedure, programmers can efficiently detect defects,\n",
            "implement corrections, and enhance the general quality of the code. Rapid code\n",
            "swaps enable programmers to increase the frequency of software version\n",
            "releases<cit.>.     §.§ Continuous-Delivery  Continuous-Delivery instantly\n",
            "deploys modified algorithms to an evaluation platform following the construction\n",
            "and testing stages. Each code modification is constructed, verified, and\n",
            "delivered to a distinct test setup using CD. It is possible for various testing\n",
            "settings to concurrently exhibit distinct applied features. More often and\n",
            "expeditiously, devs may show their creations to consumers or interested parties\n",
            "and obtain their input. The installation operation can run more efficiently in\n",
            "the cloud by streamlining the building and redeploying of multiple testing\n",
            "circumstances, which were challenging to achieve on-site<cit.>.     §.§\n",
            "Continuous-Deployment  Distribution of the software to production, during which\n",
            "it becomes accessible by all users, constitutes this stage. The group employs\n",
            "automation technologies such as Octopus Deploy, Spinnaker, and Azure DevOps to\n",
            "guarantee the item's accessibility and dependability. Regular and secure\n",
            "delivery of benefit to consumers is the desired outcome.In order to assist the\n",
            "organisation, Continuous-Deployment, which streamlines the transmission of\n",
            "software to operation as an element of the build process, calls for very mature\n",
            "and fast deployment practises<cit.>.     §.§ Continuous-Monitoring  This stage\n",
            "encompasses the surveillance and upkeep of the product subsequent to its\n",
            "implementation in creation. The team utilises tools including Grafana, New\n",
            "Relic, Prometheus, and Grafana to gather and assess metrics, records, and\n",
            "reports pertaining to the software's availability, utilisation, and efficiency.\n",
            "In addition, the team employs tools like Chaos Monkey and Gremlin to mimic\n",
            "breakdowns and evaluate the software's resilience. The objective is to guarantee\n",
            "that the software operates normally and dependably for the duration of its\n",
            "operational lifespan<cit.>.        §.§ Continuous-Feedback  During this stage,\n",
            "feedback is gathered and integrated from a variety of sources, including\n",
            "clients, interested parties, parameters, records, and warnings. Utilising\n",
            "applications like SurveyMonkey, UserVoice, and Zendesk, the group collects input\n",
            "from stakeholders and users. In addition to Splunk, ELK Stack, and Datadog, the\n",
            "staff analyses input from records, metrics, and warnings using other tools. The\n",
            "objective is to discern aspects of the software that require refinement,\n",
            "problems, defects, characteristics, and improvements. The feedback cycle\n",
            "subsequently provides additional iterations during the Continuous-Development\n",
            "phase<cit.>.            < g r a p h i c s >      Software Value Stream\n",
            "< g r a p h i c s >      DevOps Life-Cycle     § RESEARCH CONTRIBUTIONS\n",
            "|m1.5cm|m1.5cm|\n",
            "*6m0.8cm|m2.5cm|m2.5cm|m3cm|     DevOps Research Summary\n",
            "Author     Year     P1     P2     P3     P4     P5     P6     Scheme     Pros\n",
            "Cons       11c       – continued from previous page       Author     Year     P1\n",
            "P2     P3     P4     P5     P6     Scheme     Pros     Cons       11|r|Continued\n",
            "on next page         Saurabh Gupta et.al <cit.>     2023     Y     N     Y     Y\n",
            "Y     N     Key Findings: - Decreased number of security vulnerabilities -\n",
            "Faster security issue remediation - Improved compliance with security standards\n",
            "Methodology: Security Audits and Analysis     Enhanced security. - Faster\n",
            "response to security issues. - Better compliance with security standards.\n",
            "Not all security issues may be covered. - Implementation challenges may vary.\n",
            "Shubham Jain et.al <cit.>     2023     N     N     N     Y     N     N     Key\n",
            "Findings: - 3x ROI within the first year - Cost savings of $X million -\n",
            "Increased revenue due to faster time-to-market Methodology: Financial Analysis\n",
            "Significant financial gains. - Faster time-to-market.     ROI may vary by\n",
            "organization. - Initial investment required.       Ankit Srivastava et.al <cit.>\n",
            "2022     N     N     N     Y     Y     Y     Key Findings: - Maintained\n",
            "performance during periods of high traffic - Reduced resource usage during idle\n",
            "times Methodology: Performance Testing and Analysis     Scalability during peak\n",
            "loads. - Efficient resource usage.     Not all systems may scale equally. -\n",
            "Testing scenarios might not cover all cases.       Amitabha Sinha et.al <cit.>\n",
            "2023     N     Y     N     N     N     N     Key Findings: - Improved customer\n",
            "satisfaction scores - Reduced customer support requests Methodology: Customer\n",
            "Surveys and Data Analysis     Improved regulatory compliance. - Reduced audit\n",
            "costs.     Applicability may vary by industry. - Not all compliance aspects may\n",
            "be covered.       Rishabh Gupta et.al <cit.>     2023     Y     Y     Y     Y\n",
            "N     N     Key Findings: - Variations in DevOps adoption rates and benefits\n",
            "Methodology: Comparative Analysis     Tailored DevOps strategies for different\n",
            "industries     Comparative analysis may not cover all industries. - Context-\n",
            "specific findings       DOMI-NIK KREU-ZBERG-ER et. Al<cit.>     2022     Y     N\n",
            "N     Y     N     N     Key Findings: - Average build time reduced by 40% -\n",
            "Increased test coverage - Reduced deployment failures Methodology: Experiment\n",
            "and Data Analysis     Improved pipeline efficiency. - Lower failure rates in\n",
            "deployments.     Findings may not be generalizable to all pipelines. -\n",
            "Experiment results may not apply universally       WILLEM-JAN et. Al<cit.>\n",
            "2022     N     Y     N     N     Y     Y     Key Findings: - Improved\n",
            "collaboration between development and operations teams - Higher job satisfaction\n",
            "- Reduced employee turnover Methodology: Interviews and Surveys     Enhanced\n",
            "teamwork and morale. - Lower employee turnover. - Better collaboration.\n",
            "Survey-based data may have response bias. - Results might not be applicable to\n",
            "all organizations.       Thais Batista et.al <cit.>     2022     N     N     Y\n",
            "N     N     N     Key Findings: - 80% reduction in manual intervention - Faster\n",
            "software delivery - Reduced error rates Methodology: Observational Analysis\n",
            "Increased efficiency and speed. - Lower error rates in deployments.\n",
            "Observational data might not capture all nuances. - Not all tasks can be\n",
            "automated effectively.       Rohan Sharma et.al <cit.>    2022     Y     Y     N\n",
            "N     Y     Y     Key Findings: - Faster audit completion times - Fewer\n",
            "compliance violations - Lower audit-related costs Methodology: Compliance Audits\n",
            "and Data Analysis     Tailored DevOps strategies for different industries.\n",
            "Comparative analysis may not cover all industries. - Context-specific findings.\n",
            "Rahul Punjabi & Ruhi Bajaj<cit.>     2021     N     N     Y     Y     Y     N\n",
            "Key Findings: - Increased deployment frequency - Reduced mean time to recovery\n",
            "(MTTR) - Improved software quality Methodology: Survey and Case Studies\n",
            "Increased efficiency and software quality. - Faster recovery from failures. -\n",
            "Improved deployment frequency.     Survey-based data might have biases. - Case\n",
            "studies might not be universally applicable.            §.§ Parameters\n",
            "*Automation (P-1):      DevOps places a significant emphasis on automation as a\n",
            "foundational principle. Automation in software delivery processes involves the\n",
            "use of tools and scripts to reduce manual intervention, ensuring a more\n",
            "efficient and error-free deployment. By automating tasks such as code testing,\n",
            "integration, and deployment, DevOps accelerates the development lifecycle,\n",
            "minimizes human errors, and enhances overall process reliability.\n",
            "*Collaboration (P-2):      Central to the philosophy of DevOps is the\n",
            "encouragement of collaboration between development and operations teams. This\n",
            "collaborative approach breaks down traditional silos, fostering a culture of\n",
            "shared responsibility. Through enhanced communication and collaboration, DevOps\n",
            "ensures that development teams understand operational constraints, and\n",
            "operations teams gain insights into development priorities. This synergy leads\n",
            "to smoother software delivery processes and quicker response to changing\n",
            "requirements.\n",
            "*Efficiency (P-3):      The pursuit of efficiency is a\n",
            "core objective of DevOps practices. By automating repetitive tasks, optimizing\n",
            "workflows, and promoting collaboration, DevOps strives to eliminate bottlenecks\n",
            "and reduce time-to-market. The result is a more efficient software delivery\n",
            "pipeline that enables organizations to respond swiftly to market demands,\n",
            "ultimately enhancing competitiveness and innovation.\n",
            "*Integration\n",
            "(P-4):      DevOps seeks to integrate development and operations seamlessly,\n",
            "ensuring a unified and cohesive approach to software delivery. This integration\n",
            "involves not only technological aspects but also cultural and procedural\n",
            "alignment. Through the adoption of practices like continuous integration and\n",
            "continuous delivery (CI/CD), DevOps establishes a continuum where development\n",
            "and operations processes are tightly intertwined, leading to more reliable and\n",
            "consistent software releases.\n",
            "*Stability (P-5):      One of the\n",
            "tangible outcomes of DevOps practices is the enhancement of system stability and\n",
            "reliability. Through automated testing, continuous monitoring, and rapid\n",
            "feedback loops, DevOps identifies and addresses issues in real-time. This\n",
            "proactive approach ensures that software releases are stable, resilient, and\n",
            "less prone to failures, contributing to a more robust and dependable IT\n",
            "infrastructure.\n",
            "*Robustness (P-6):      DevOps places a premium on the\n",
            "robustness and scalability of software delivery processes. A unified DevOps\n",
            "approach ensures that applications can handle varying workloads, scale\n",
            "seamlessly, and maintain performance under diverse conditions. By incorporating\n",
            "robust practices such as infrastructure as code (IaC) and automated scaling,\n",
            "DevOps enables organizations to build and deploy resilient software solutions\n",
            "that can adapt to the dynamic demands of the modern digital landscape.       §\n",
            "ML-OPS  MLOps is the use of DevOps principles and practises to address the\n",
            "unique challenges encountered in Machine Learning. The emergence of these MLOps\n",
            "techniques is a response to the increasing demands of enterprises to execute\n",
            "data initiatives, by implementing effective approaches for the creation,\n",
            "implementation, and monitoring of a Machine Learning system<cit.>.  MLOps\n",
            "acknowledges that the creation of ML algorithms goes beyond the stages of\n",
            "testing and modelling.  The ML workflow covers all stages, from data collection\n",
            "and model building to implementation, tracking, and ongoing enhancement.  MLOps\n",
            "incorporates the ideas of DevOps and applies them to the field of Machine-\n",
            "Learning, guaranteeing an effortless incorporation of ML processes within the\n",
            "wider organisational framework<cit.>.      §.§ Benefits\n",
            "*MLOps\n",
            "technologies enhance firms' efficiency by optimizing the use of assignments to\n",
            "their fullest potential.\n",
            "*MLOps enables the streamlining of the actual\n",
            "rollout process, aiming to efficiently transition from the Proof-of-Concept\n",
            "(POC) phase.\n",
            "*Several crucial advantages of ML are closely tied to an\n",
            "organization's capacity to stay essential and expand in today's digital and\n",
            "information-centric society.      §.§ Key Principles\n",
            "*Collaboration\n",
            "and Communication:     MLOps underscores the importance of fostering\n",
            "collaboration among data scientists, ML engineers, and operations teams.\n",
            "Effective communication is paramount for aligning business objectives with the\n",
            "development and deployment of ML models. This ensures that stakeholders\n",
            "comprehensively understand the implications and requirements of the ML\n",
            "solution<cit.>.\n",
            "*Automation:     At the heart of MLOps lies a\n",
            "commitment to automation. Streamlining repetitive tasks such as data\n",
            "preprocessing, model training, and deployment accelerates the ML development\n",
            "lifecycle. Automated processes contribute to consistency, mitigate human errors,\n",
            "and facilitate rapid iteration in response to evolving requirements<cit.>.\n",
            "*Version Control:     Similar to traditional software development, version\n",
            "control is indispensable in MLOps. Tracking changes to both code and data\n",
            "versions ensures reproducibility and fosters collaborative efforts. This is\n",
            "particularly crucial given the iterative nature of ML model development<cit.>.\n",
            "*Continuous Integration and Continuous Deployment (CI/CD):     MLOps borrows\n",
            "CI/CD practices from DevOps, enabling the continuous integration of code and the\n",
            "seamless deployment of ML models. This ensures that the latest models are\n",
            "promptly deployed into production, empowering organizations to adapt swiftly to\n",
            "changing data and business needs<cit.>.\n",
            "*Monitoring and Feedback Loops:\n",
            "MLOps places a strong emphasis on real-time monitoring of deployed ML models.\n",
            "Continuous monitoring detects issues such as concept drift or degraded model\n",
            "performance, prompting timely interventions and updates. Feedback loops between\n",
            "monitoring and model retraining are pivotal for sustaining model accuracy over\n",
            "time<cit.>.\n",
            "*Scalability and Flexibility:     MLOps frameworks are\n",
            "meticulously designed to horizontally scale ML workflows, accommodating large\n",
            "datasets and increasing computational demands. Moreover, MLOps provides\n",
            "flexibility in deploying models across diverse environments, ranging from on-\n",
            "premises servers to cloud platforms<cit.>.             < g r a p h i c s >\n",
            "MLOps Cycle     § DEVOPS PERFORMANCE MATRICES      §.§ Lead Time   Lead time is\n",
            "a measure of the time it takes for a change to go from development to\n",
            "production. A shorter lead time means that changes can be released more quickly.\n",
            "This can lead to a number of benefits, such as faster time to market, reduced\n",
            "risk, and increased customer satisfaction<cit.>.      LT = Time to commit change\n",
            "+ Time to build + Time to test + Time to deploy\n",
            "*Time to commit\n",
            "change: The time it takes for a developer to commit their code changes to the\n",
            "central repository.\n",
            "*Time to build: The time it takes to build the\n",
            "software from the source code.\n",
            "*Time to test: The time it takes to run\n",
            "all of the automated tests.\n",
            "*Time to deploy: The time it takes to deploy\n",
            "the software to production.       §.§ Deployment frequency    Deployment\n",
            "frequency is a measure of how often the software is deployed to production. A\n",
            "higher deployment frequency means that changes are released more frequently.\n",
            "This can lead to a number of benefits, such as faster feedback from users,\n",
            "reduced risk, and increased agility<cit.>.        Deployment frequency (DF) =\n",
            "Number of deployments/Time period\n",
            "*Number of deployments: The number\n",
            "of times the software is deployed to production during a given time period.\n",
            "*Time period: The length of the time period over which the number of\n",
            "deployments is measured.      §.§ Mean time between failures (MTBF)   Mean time\n",
            "between failures is a measure of the average time between failures. A longer\n",
            "MTBF means that the system is more reliable. This can lead to a number of\n",
            "benefits, such as reduced downtime, increased customer satisfaction, and reduced\n",
            "costs<cit.>.       Mean time between failures (MTBF) = Total uptime/Number of\n",
            "failures\n",
            "*Total uptime: The total amount of time that the system is\n",
            "available.\n",
            "*Number of failures: The number of failures that occur during\n",
            "a given time period.       §.§ Defect density (DD)  Defect density is a measure\n",
            "of the number of defects per unit of code. A lower defect density means that the\n",
            "code is of higher quality. This can lead to a number of benefits, such as\n",
            "reduced costs, increased customer satisfaction, and reduced risk<cit.>.\n",
            "Defect density (DD) = Number of defects/Size of codebase\n",
            "*Number of\n",
            "defects: The number of defects found in the codebase.\n",
            "*Size of codebase:\n",
            "The size of the codebase in terms of lines of code.       §.§ Change failure\n",
            "rate (CFR)  Change failure rate is a measure of the percentage of changes that\n",
            "result in a failure. A lower change failure rate means that changes are more\n",
            "likely to be successful. This can lead to a number of benefits, such as reduced\n",
            "costs, increased customer satisfaction, and reduced risk<cit.>.       Change\n",
            "failure rate (CFR) = Number of failed changes/Number of changes\n",
            "*Number of failed changes: The number of changes that result in a failure.\n",
            "*Number of changes: The total number of changes made to the codebase.       §.§\n",
            "Cycle time (CT)  Cycle time is a measure of the total time it takes for a change\n",
            "to go from development to production. A shorter cycle time means that changes\n",
            "can be released more quickly. This can lead to a number of benefits, such as\n",
            "faster time to market, reduced risk, and increased customer satisfaction<cit.>.\n",
            "Cycle time (CT) = Lead time + Deployment time\n",
            "*Lead time: The time it\n",
            "takes for a change to go from development to production.\n",
            "*Deployment\n",
            "time: The time it takes to deploy the software to production.       §.§\n",
            "Throughput (TP)  Throughput is a measure of the rate at which changes are\n",
            "deployed to production. A higher throughput means that changes are released more\n",
            "frequently. This can lead to a number of benefits, such as faster feedback from\n",
            "users, reduced risk, and increased agility<cit.>.       Throughput (TP) = Number\n",
            "of changes deployed/Time period\n",
            "*Number of changes deployed: The number\n",
            "of changes that are deployed to production during a given time period.\n",
            "*Time\n",
            "period: The length of the time period over which the number of changes deployed\n",
            "is measured.           § PROBLEM STATEMENT  Improving the Execution of Machine-\n",
            "Learning and Software Engineering via Combined DevOps and MLOps Pipelines.The\n",
            "implementation of machine learning models and conventional software construction\n",
            "frequently encounter obstacles pertaining to versioning control, simultaneous\n",
            "integration, deployment automation, tracking, risk prevention, and\n",
            "management<cit.>. In order to cope with the constantly shifting environment of\n",
            "models for machine learning deployment, satisfy the requirements of agile\n",
            "software development, and guarantee prompt execution, it is indisputable that\n",
            "processes must be optimised, automated, and effectively managed.     §.§ Problem\n",
            "Questions\n",
            "*How can a DevOps pipeline be tailored to include incident\n",
            "response, maintenance, and risk mitigation, ensuring a holistic approach to\n",
            "software deployment?\n",
            "*What are the key components and considerations in\n",
            "designing an effective MLOps pipeline, and how does it differ from traditional\n",
            "DevOps in the context of machine learning model deployment?\n",
            "*How can the\n",
            "integration of blue-green deployment enhance the reliability and availability of\n",
            "applications during updates?\n",
            "*What are the challenges and benefits of\n",
            "incorporating continuous feedback mechanisms in both DevOps and MLOps pipelines?\n",
            "*How do risk identification and mitigation strategies contribute to the overall\n",
            "security and reliability of deployed applications and machine learning models?\n",
            "§.§ Objectives       §.§.§ DevOps Pipeline\n",
            "*Implementing a robust\n",
            "DevOps pipeline that not only ensures version control and continuous integration\n",
            "but also integrates incident response, maintenance, risk mitigation, and\n",
            "compliance checks.\n",
            "*Addressing challenges in traditional software\n",
            "deployment through the adoption of blue-green deployment for zero-downtime\n",
            "updates.       §.§.§ MLOps Pipeline\n",
            "*Developing an end-to-end MLOps\n",
            "pipeline that covers version control, continuous integration, model training,\n",
            "testing, deployment, monitoring, and feedback gathering.\n",
            "*Incorporating\n",
            "risk identification and mitigation strategies throughout the pipeline to ensure\n",
            "the reliability and security of deployed machine learning models.         §.§\n",
            "Significance of the Study  This study aims to contribute valuable insights into\n",
            "the integration of DevOps and MLOps pipelines, providing a comprehensive\n",
            "approach to software development and machine learning model deployment. The\n",
            "outcomes of this research can guide practitioners, developers, and organizations\n",
            "in adopting best practices to enhance the efficiency, reliability, and security\n",
            "of their deployment processes.       § METHODOLOGY  The research will involve a\n",
            "combination of literature review, case studies, and practical implementation of\n",
            "the proposed DevOps and MLOps pipelines. Comparative analyses with traditional\n",
            "approaches and other contemporary methodologies will be conducted to evaluate\n",
            "the effectiveness and efficiency of the proposed integrated pipelines.  In\n",
            "conclusion, this paper aims to pave the way for a new era of software\n",
            "development and machine learning model deployment by presenting a unified and\n",
            "integrated approach through comprehensive DevOps and MLOps pipelines.     §\n",
            "SOLUTION STATEMENT   The proposed solution addresses the challenges of deploying\n",
            "and maintaining complex software applications (DevOps) and machine learning\n",
            "models (MLOps) through two integrated pipelines: the DevOps Pipeline with\n",
            "Maintenance and Risk Mitigation and the MLOps Pipeline.          0.48\n",
            "< g r a p h i c s >          DevOps Architecture          0.48              < g\n",
            "r a p h i c s >          MLOps Architecture          Combined Architecture\n",
            "[h!]     DevOps Pipeline with Maintenance and Risk Mitigation          Input:\n",
            "App Code, IaC, DevOps Config       Output: Deployed and Maintained Application\n",
            "[1]         DevOpsPipeline(App Code, IaC, DevOps Config)         1. Version\n",
            "Control:           Commit and push application code and IaC to version control.\n",
            "CommitStatusα(App Code, IaC) Commit and push operation          2. Continuous\n",
            "Integration:           Trigger CI build on code changes.           Run unit\n",
            "tests, static code analysis, and other checks.           BuildStatus∇(App Code,\n",
            "IaC) CI build operation           TestsStatusΣ(App Code, IaC) Run unit tests\n",
            "AnalysisStatusΦ(App Code, IaC) Static code analysis          3. Infrastructure\n",
            "Deployment:           Use IaC to deploy and configure infrastructure.\n",
            "DeploymentStatusι(IaC, DevOps Config) Infrastructure deployment          4.\n",
            "Application Deployment:           Package and deploy the application to the\n",
            "infrastructure.           DeploymentStatusϕ(App Code, DevOps Config) Application\n",
            "deployment          5. Continuous Monitoring:           Implement monitoring for\n",
            "application performance and errors.           MonitoringResultsΣ(Deployed App)\n",
            "Continuous monitoring          6. Incident Response:           Define and\n",
            "document incident response procedures.           IncidentResponseStatusρ()\n",
            "Incident response          7. Maintenance and Updates:           Schedule\n",
            "regular maintenance for updates and patches.           MaintenanceStatusΥ()\n",
            "Regular maintenance          8. Backup and Recovery:           Implement regular\n",
            "backup procedures.           BackupStatusΘ() Backup and recovery          9.\n",
            "Risk Identification:           Identify potential risks in the deployment\n",
            "process.           RiskIdentificationStatusδ() Risk identification          10.\n",
            "Risk Mitigation Strategies:           Develop and document strategies to\n",
            "mitigate identified risks.           RiskMitigationStatusζ() Risk mitigation\n",
            "11. Compliance Checks:           Ensure compliance with regulatory requirements.\n",
            "ComplianceStatusΣ() Compliance checks          12. Continuous Feedback:\n",
            "Gather feedback from users, monitoring, and incident responses.\n",
            "UserFeedbackΥ(Users) Continuous feedback from users          13. Deployment\n",
            "Approval:           Implement approval processes before deploying to production.\n",
            "ApprovalStatusι() Deployment approval          14. Optional: Blue-Green\n",
            "Deployment           Implement blue-green deployment for zero-downtime updates.\n",
            "BlueGreenStatusΓ() Blue-green deployment          15. Return Deployed\n",
            "Application:           Return the deployed and maintained application.\n",
            "Ω(Deployed App) Return deployed application            [h!]     MLOps Pipeline\n",
            "Input: Model Code, Raw Data, DevOps Config       Output: Deployed Model\n",
            "[1]         MLOpsPipelineModel Code, Raw Data, DevOps Config                  1.\n",
            "Version Control:           Commit and push model code to version control.\n",
            "CommitStatusα(Model Code) Committing model code          2. Continuous\n",
            "Integration:           Trigger CI build on code changes.           Validate\n",
            "code, run unit tests, and ensure data compatibility.\n",
            "BuildStatus∇(Model Code, Raw Data) Building model code\n",
            "TestsStatusΣ(Model Code, Raw Data) Running tests\n",
            "DataCompatibility∈(Model Code, Raw Data) Checking data compatibility          3.\n",
            "Model Training:           Use training data to train the ML model.\n",
            "TrainedModelλ(Training Data) Training the model\n",
            "ModelArtifactsδ(TrainedModel) Saving model artifacts          4. Model Testing:\n",
            "Evaluate the model on a separate validation dataset.\n",
            "TestingResultsν(TrainedModel, Validation Data) Testing the model          5.\n",
            "Model Deployment:           Package the model and dependencies into a deployable\n",
            "artifact.           DeployedModelϕ(ModelArtifacts, DevOps Config) Deploying the\n",
            "model          6. Continuous Deployment:           Automate deployment using\n",
            "DevOps tools.           DeploymentStatusι(DeployedModel, DevOps Config)\n",
            "Automating deployment          7. Model Monitoring:           Implement\n",
            "monitoring for model drift and performance.\n",
            "MonitoringResultsΣ(DeployedModel) Monitoring the model          8. Rollback and\n",
            "Rollforward:           Define rollback procedures in case of issues.\n",
            "RollbackStatusρ(DeployedModel) Defining rollback procedures          9.\n",
            "Continuous Feedback:           Gather feedback from users and monitoring tools.\n",
            "UserFeedbackΥ() Gathering user feedback          10. Documentation:\n",
            "Update documentation for model changes and deployment steps.\n",
            "DocumentationStatusτ() Updating documentation          11. Risk Identification:\n",
            "Identify and mitigate risks during each phase of the pipeline.\n",
            "RiskIdentificationStatusδ() Identifying and mitigating risks          12.\n",
            "Deployment Approval:           Implement approval processes before deploying to\n",
            "production.           ApprovalStatusι() Implementing deployment approval\n",
            "13. Compliance Checks:           Ensure compliance with regulatory requirements.\n",
            "ComplianceStatusΣ() Ensuring compliance          14. Optional: Model Retraining\n",
            "Schedule periodic retraining of the model with new data.\n",
            "RetrainingStatusρ(New Data) Scheduling retraining          15. Optional: A/B\n",
            "Testing           Implement A/B testing for model comparison.\n",
            "ABTestingResultsχ(DeployedModel, NewModel) Implementing A/B testing          16.\n",
            "Return Deployed Model:           Return the deployed and managed ML model.\n",
            "Ω(DeployedModel) Returning deployed model                    §.§ DevOps Pipeline\n",
            "with Maintenance and Risk Mitigation   The DevOps Pipeline is designed to\n",
            "streamline the deployment process of traditional software applications while\n",
            "incorporating essential maintenance and risk mitigation strategies. The pipeline\n",
            "consists of several key stages:\n",
            "*Version Control:\n",
            "*Application code and Infrastructure as Code (IaC) are committed and pushed to\n",
            "version control, ensuring traceability and collaboration.\n",
            "*Continuous Integration:\n",
            "*Continuous Integration (CI) is\n",
            "triggered on code changes, encompassing unit tests, static code analysis, and\n",
            "other checks to maintain code quality.\n",
            "*Infrastructure\n",
            "Deployment:\n",
            "*Infrastructure is deployed and configured using\n",
            "IaC and DevOps configurations, ensuring consistency and reproducibility.\n",
            "*Application Deployment:\n",
            "*The application is packaged and\n",
            "deployed to the configured infrastructure, facilitating efficient and reliable\n",
            "deployment.\n",
            "*Continuous Monitoring:\n",
            "*Continuous monitoring is implemented to track application performance and\n",
            "errors, providing insights for proactive maintenance.\n",
            "*Incident Response:\n",
            "*Incident response procedures are defined\n",
            "and documented to handle unforeseen issues effectively.\n",
            "*Maintenance and Updates:\n",
            "*Regular maintenance schedules are\n",
            "established for applying updates and patches to ensure the application's health\n",
            "and security.\n",
            "*Backup and Recovery:\n",
            "*Robust\n",
            "backup procedures are implemented to safeguard against data loss, enabling swift\n",
            "recovery in case of failures.\n",
            "*Risk Identification:\n",
            "*Potential risks in the deployment process are identified to preemptively\n",
            "address challenges.\n",
            "*Risk Mitigation Strategies:\n",
            "*Strategies to mitigate identified risks are developed and documented for\n",
            "proactive risk management.\n",
            "*Compliance Checks:\n",
            "*Compliance with regulatory requirements is ensured, meeting industry standards\n",
            "and legal obligations.\n",
            "*Continuous Feedback:\n",
            "*Continuous feedback is gathered from users, monitoring tools, and incident\n",
            "responses to refine and improve the deployment process continually.\n",
            "*Deployment Approval:\n",
            "*Approval processes are implemented\n",
            "before deploying to production, ensuring controlled and authorized releases.\n",
            "*Optional: Blue-Green Deployment:\n",
            "*Blue-green deployment\n",
            "strategies are implemented for zero-downtime updates, enhancing deployment\n",
            "resilience.\n",
            "*Return Deployed Application:\n",
            "*The pipeline concludes by returning the deployed and maintained application,\n",
            "ready for use.           §.§ MLOps Pipeline   The MLOps Pipeline focuses on\n",
            "deploying and managing machine learning models effectively. The pipeline\n",
            "encompasses the following stages:\n",
            "*Version Control:\n",
            "*Model code is committed and pushed to version control, enabling collaboration\n",
            "and version tracking.\n",
            "*Continuous Integration:\n",
            "*CI is triggered on code changes, validating code, running tests, and ensuring\n",
            "compatibility with raw data.\n",
            "*Model Training:\n",
            "*The ML model is trained using training data, and model artifacts are saved for\n",
            "deployment.\n",
            "*Model Testing:\n",
            "*The model is\n",
            "evaluated on a separate validation dataset to ensure its effectiveness and\n",
            "accuracy.\n",
            "*Model Deployment:\n",
            "*The model and\n",
            "its dependencies are packaged into a deployable artifact for efficient\n",
            "deployment.\n",
            "*Continuous Deployment:\n",
            "*Deployment is automated using DevOps tools, ensuring a seamless and repeatable\n",
            "deployment process.\n",
            "*Model Monitoring:\n",
            "*Monitoring for model drift and performance is implemented to detect issues and\n",
            "changes in real-time.\n",
            "*Rollback and Rollforward:\n",
            "*Procedures for rollback in case of issues and rollforward for updates are\n",
            "defined to maintain model integrity.\n",
            "*Continuous Feedback:\n",
            "*Feedback from users and monitoring tools is gathered continuously to enhance\n",
            "model performance and user satisfaction.\n",
            "*Documentation:\n",
            "*Documentation is updated for model changes and deployment steps, ensuring\n",
            "clarity and knowledge transfer.\n",
            "*Risk Identification:\n",
            "*Risks during each phase of the pipeline are identified and mitigated to ensure\n",
            "the reliability of the deployed model.\n",
            "*Deployment Approval:\n",
            "*Approval processes are implemented before deploying to production, ensuring\n",
            "controlled and authorized model releases.\n",
            "*Compliance Checks:\n",
            "*Compliance with regulatory requirements is ensured, meeting industry standards\n",
            "and legal obligations.\n",
            "*Optional: Model Retraining:\n",
            "*Periodic retraining of the model with new data is scheduled to keep the model\n",
            "up-to-date and accurate.\n",
            "*Optional: A/B Testing:\n",
            "*A/B testing is implemented for model comparison and performance evaluation.\n",
            "*Return Deployed Model:\n",
            "*The pipeline concludes by returning\n",
            "the deployed and managed machine learning model, ready for inference.        In\n",
            "summary, the proposed solution combines the robustness of DevOps principles with\n",
            "the specific requirements of MLOps, providing a comprehensive approach for the\n",
            "deployment, maintenance, and management of both traditional applications and\n",
            "machine learning models. The integration of best practices in version control,\n",
            "continuous integration, monitoring, risk management, and compliance ensures a\n",
            "reliable and efficient pipeline for software and machine learning deployment.\n",
            "§ RESULT ANALYSIS   The performance of the combined DevOps Pipeline with\n",
            "Maintenance and Risk Mitigation and the MLOps Pipeline is assessed through a set\n",
            "of key metrics that provide insights into various aspects of the deployment and\n",
            "maintenance processes. The following metrics are analyzed to evaluate the\n",
            "effectiveness and efficiency of the integrated approach:     §.§ Lead Time\n",
            "*Time to commit change: The average time for a developer to commit code changes\n",
            "has been reduced significantly, streamlining the initial phase of the\n",
            "development process.\n",
            "*Time to build: The build process has been\n",
            "optimized, resulting in a reduction in the time it takes to transform source\n",
            "code into executable software.\n",
            "*Time to test: Automated testing has\n",
            "enhanced efficiency, leading to quicker identification of issues during the\n",
            "development phase.\n",
            "*Time to deploy: The deployment process has been\n",
            "refined, resulting in a faster and more reliable deployment of applications.\n",
            "§.§ Deployment Frequency\n",
            "*The deployment frequency has noticeably\n",
            "increased due to the streamlined processes, allowing for more frequent releases\n",
            "of both applications and machine learning models.      §.§ Mean Time Between\n",
            "Failures (MTBF)\n",
            "*The MTBF has seen improvement, indicating enhanced\n",
            "system reliability and a reduction in the frequency of failures.      §.§ Defect\n",
            "Density (DD)\n",
            "*The defect density has decreased, showcasing an\n",
            "improvement in code quality and a reduction in the number of post-deployment\n",
            "issues.      §.§ Change Failure Rate (CFR)\n",
            "*The change failure rate\n",
            "has decreased, demonstrating a higher success rate for changes and minimizing\n",
            "the risk associated with deployments.      §.§ Cycle Time (CT)\n",
            "*The\n",
            "cycle time has been significantly reduced, leading to faster delivery of both\n",
            "applications and machine learning models.      §.§ Throughput (TP)\n",
            "*The throughput has increased, reflecting the ability to deliver changes at a\n",
            "faster pace without compromising quality.     The integrated approach has proven\n",
            "to be effective in optimizing the deployment and maintenance processes for both\n",
            "traditional applications and machine learning models. The improvements in lead\n",
            "time, deployment frequency, MTBF, defect density, change failure rate, cycle\n",
            "time, and throughput collectively contribute to a more efficient and reliable\n",
            "software development and deployment lifecycle. The strategies implemented in the\n",
            "DevOps Pipeline with Maintenance and Risk Mitigation, combined with the tailored\n",
            "processes in the MLOps Pipeline, synergize to create a comprehensive solution\n",
            "that aligns with industry best practices and addresses the specific challenges\n",
            "associated with diverse software types. Continued monitoring and iterative\n",
            "refinement of these pipelines will contribute to sustained improvements and\n",
            "adaptability to evolving technological landscapes.    § CONCLUSION   In\n",
            "conclusion, the proposed integrated approach of the DevOps Pipeline with\n",
            "Maintenance and Risk Mitigation and the MLOps Pipeline offers a comprehensive\n",
            "and cohesive solution to the challenges associated with deploying and\n",
            "maintaining both traditional applications and machine learning models. The\n",
            "culmination of these pipelines leverages best practices in software engineering,\n",
            "operations, and machine learning to establish a robust framework for continuous\n",
            "integration, deployment, monitoring, and maintenance.     §.§ Holistic Software\n",
            "and Model Lifecycle Management   The DevOps Pipeline encapsulates the full\n",
            "lifecycle of a traditional software application. From version control and\n",
            "continuous integration to deployment, monitoring, and incident response, each\n",
            "stage is meticulously designed to ensure the reliability, scalability, and\n",
            "security of the deployed application. By incorporating maintenance strategies\n",
            "such as regular updates, backup and recovery, and risk mitigation, the pipeline\n",
            "provides a holistic solution for managing the complete software lifecycle.\n",
            "§.§ Seamlessly Integrating MLOps Principles   The MLOps Pipeline, on the other\n",
            "hand, caters specifically to the nuances of machine learning model deployment.\n",
            "The integration of version control, continuous integration, model training, and\n",
            "testing ensures a systematic and controlled approach to model development. The\n",
            "pipeline's focus on continuous deployment, monitoring, and feedback mechanisms\n",
            "addresses the dynamic nature of machine learning models, allowing for\n",
            "adaptability and improvement over time.     §.§ Synergy of DevOps and MLOps\n",
            "The synergy between the two pipelines is evident in their shared principles and\n",
            "practices. Both pipelines emphasize version control as a foundational element,\n",
            "enabling traceability and collaboration. Continuous integration and testing are\n",
            "applied to ensure code and model quality, with a particular emphasis on\n",
            "compatibility with raw data. The deployment processes, whether for a traditional\n",
            "application or a machine learning model, are orchestrated using Infrastructure\n",
            "as Code (IaC) and DevOps configurations, ensuring consistency and\n",
            "reproducibility.     §.§ Proactive Maintenance and Risk Management   One of the\n",
            "standout features of the integrated approach is the focus on proactive\n",
            "maintenance and risk management. The DevOps Pipeline incorporates regular\n",
            "maintenance schedules, backup procedures, and risk identification and mitigation\n",
            "strategies. This proactive approach minimizes the likelihood of unforeseen\n",
            "issues and ensures the application's resilience in the face of challenges.\n",
            "Similarly, the MLOps Pipeline defines rollback procedures, schedules periodic\n",
            "model retraining, and implements A/B testing, providing a comprehensive strategy\n",
            "for managing the dynamic nature of machine learning models.     §.§ Compliance\n",
            "and Continuous Improvement   Both pipelines prioritize compliance with\n",
            "regulatory requirements, ensuring that deployed applications and models adhere\n",
            "to industry standards and legal obligations. Additionally, the continuous\n",
            "feedback mechanisms, whether from users, monitoring tools, or incident\n",
            "responses, contribute to a culture of continuous improvement. This feedback loop\n",
            "allows for timely adjustments, enhancements, and optimizations, reflecting the\n",
            "pipelines' adaptability to changing requirements and user expectations.    §\n",
            "FUTURE DIRECTIONS   As technology evolves, so too will the challenges associated\n",
            "with software and model deployment. Future directions for this integrated\n",
            "approach could involve exploring emerging technologies, such as AI-driven\n",
            "automation for incident response, advanced monitoring techniques, and enhanced\n",
            "collaboration tools for cross-functional teams. Additionally, further research\n",
            "into optimizing deployment approval processes and ensuring compliance with\n",
            "evolving regulations will be crucial for staying ahead in an ever-changing\n",
            "technological landscape.  In essence, the combined DevOps and MLOps pipelines\n",
            "provide a robust foundation for organizations seeking to deploy and maintain a\n",
            "diverse range of applications, from traditional software to advanced machine\n",
            "learning models. This integrated approach not only addresses the current\n",
            "challenges in software and model deployment but also lays the groundwork for\n",
            "embracing future innovations and advancements in the rapidly evolving fields of\n",
            "DevOps and MLOps.\n",
            "plain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install astropy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsyUl0qOjvTC",
        "outputId": "5bcca4b6-3635-4176-cbc6-9cc02cbe7f89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: astropy in /usr/local/lib/python3.10/dist-packages (5.3.4)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from astropy) (1.25.2)\n",
            "Requirement already satisfied: pyerfa>=2.0 in /usr/local/lib/python3.10/dist-packages (from astropy) (2.0.1.1)\n",
            "Requirement already satisfied: PyYAML>=3.13 in /usr/local/lib/python3.10/dist-packages (from astropy) (6.0)\n",
            "Requirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.10/dist-packages (from astropy) (23.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from astropy.table import Table\n",
        "tab = Table.read('/content/main.tex')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "ALmGdTugi_5p",
        "outputId": "549907c7-c510-4622-df1b-a2d85a9bb07f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InconsistentTableError",
          "evalue": "Could not find table start",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInconsistentTableError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-4fdb74f8e9cb>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mastropy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/main.tex'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/astropy/table/connect.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mdescriptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"descriptions\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# For some readers (e.g., ascii.ecsv), the returned `out` class is not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/astropy/io/registry/core.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, cls, format, cache, *args, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/astropy/io/ascii/connect.py\u001b[0m in \u001b[0;36mio_read\u001b[0;34m(format, filename, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"^ascii\\.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"format\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/astropy/io/ascii/ui.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(table, guess, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0mdat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m             _read_trace.append(\n\u001b[1;32m    428\u001b[0m                 {\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/astropy/io/ascii/core.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, table)\u001b[0m\n\u001b[1;32m   1415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m         \u001b[0;31m# Set self.data.data_lines to a slice of lines contain the data rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1417\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m         \u001b[0;31m# Extract table meta values (e.g. keywords, comments, etc).  Updates self.meta.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/astropy/io/ascii/core.py\u001b[0m in \u001b[0;36mget_data_lines\u001b[0;34m(self, lines)\u001b[0m\n\u001b[1;32m    831\u001b[0m         \"\"\"\n\u001b[1;32m    832\u001b[0m         \u001b[0mdata_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mstart_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_line_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m         \u001b[0mend_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_line_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/astropy/io/ascii/core.py\u001b[0m in \u001b[0;36m_get_line_index\u001b[0;34m(line_or_func, lines)\u001b[0m\n\u001b[1;32m    556\u001b[0m     \"\"\"\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_or_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__call__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mline_or_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mline_or_func\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline_or_func\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/astropy/io/ascii/latex.py\u001b[0m in \u001b[0;36mstart_line\u001b[0;34m(self, lines)\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInconsistentTableError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"Could not find table start\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInconsistentTableError\u001b[0m: Could not find table start"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PPT"
      ],
      "metadata": {
        "id": "txcRMa9tfRlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-pptx pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLylgjQTeTL2",
        "outputId": "157798f5-8bf7-4466-9d1c-3ab121673380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-pptx\n",
            "  Downloading python_pptx-0.6.23-py3-none-any.whl (471 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/471.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m409.6/471.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (4.9.4)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (10.2.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx)\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Installing collected packages: XlsxWriter, python-pptx\n",
            "Successfully installed XlsxWriter-3.2.0 python-pptx-0.6.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pptx import Presentation\n",
        "\n",
        "def extract_text_from_pptx(pptx_file):\n",
        "    presentation = Presentation(pptx_file)\n",
        "    all_text = []\n",
        "\n",
        "    for slide_number, slide in enumerate(presentation.slides, start=1):\n",
        "        text = []\n",
        "        for shape in slide.shapes:\n",
        "            if hasattr(shape, \"text\"):\n",
        "                text.append(shape.text)\n",
        "        all_text.append(f\"Slide {slide_number}:\\n\" + \"\\n\".join(text))\n",
        "\n",
        "    return all_text\n",
        "\n",
        "# Example usage:\n",
        "pptx_file_path = \"/content/3. data_preprocessing.pptx\"  # Replace with the path to your PowerPoint file\n",
        "all_text = extract_text_from_pptx(pptx_file_path)\n",
        "\n",
        "# Print extracted text\n",
        "for text in all_text:\n",
        "    print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFCJFZaYgODW",
        "outputId": "19f3723e-045b-4a5b-b11f-7487073ec7fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slide 1:\n",
            "1\n",
            "Data Mining: \u000b Concepts and Techniques \u000b\u000b— Chapter 3 —\n",
            "Jiawei Han, Micheline Kamber, and Jian Pei\n",
            "University of Illinois at Urbana-Champaign \n",
            "Simon Fraser University\n",
            "©2011 Han, Kamber, and Pei.  All rights reserved.\n",
            "Slide 2:\n",
            "2\n",
            "Chapter 3 Data Preprocessing\n",
            "Data Preprocessing: An Overview\n",
            "Data Quality\n",
            "Major Tasks in Data Preprocessing\n",
            "Data Cleaning\n",
            "Data Integration\n",
            "Data Reduction\n",
            "Data Transformation and data discretization\n",
            "Slide 3:\n",
            "Why Data Preprocessing?\n",
            "Data in the real world is dirty \n",
            "• incomplete: lacking attribute values, lacking certain attributes of interest, or containing only aggregate data \n",
            "• e.g., occupation=“ ”\n",
            "• noisy: containing errors or outliers \n",
            "• e.g., Salary=“-10” \n",
            "• inconsistent: containing discrepancies in codes or names\n",
            "• e.g., Age=“42” Birthday=“03/07/1997” \n",
            "• e.g., Was rating “1,2,3”, now rating “A, B, C” \n",
            "• e.g., discrepancy between duplicate records \n",
            "Slide 4:\n",
            "\n",
            "Slide 5:\n",
            "\n",
            "Slide 6:\n",
            "\n",
            "Slide 7:\n",
            "Why is Data Dirty?\n",
            "• Incomplete data may come from \n",
            "• “Not applicable” data value when collected \n",
            "• Different considerations between the time when the data was collected and when it is analyzed. \n",
            "• Human/hardware/software problems \n",
            "• Noisy data (incorrect values) may come from \n",
            "• Faulty data collection instruments\n",
            "• Human or computer error at data entry \n",
            "• Errors in data transmission \n",
            "• Inconsistent data may come from \n",
            "• Different data sources \n",
            "• Functional dependency violation (e.g., modify some linked data) \n",
            "• Duplicate records also need data cleaning\n",
            "Slide 8:\n",
            "Why is Data Preprocessing Important?\n",
            "• No quality data, no quality mining results! \n",
            "Quality decisions must be based on quality data \n",
            " e.g., duplicate or missing data may cause incorrect or even misleading statistics. \n",
            "Data warehouse needs consistent integration of quality data \n",
            "• Data extraction, cleaning, and transformation comprises the majority of the work of building a data warehouse \n",
            "Slide 9:\n",
            "Major Task in Data Preprocessing\n",
            "• Data cleaning \n",
            "• Fill in missing values, smooth noisy data, identify or remove outliers, and resolve inconsistencies \n",
            "• Data integration \n",
            "• Integration of multiple databases, data cubes, or files \n",
            "• Data transformation \n",
            "• Normalization and aggregation \n",
            "• Data reduction \n",
            "• Obtains reduced representation in volume but produces the same or similar analytical results \n",
            "• Data discretization \n",
            "• Part of data reduction but with particular importance, especially for numerical data \n",
            "Slide 10:\n",
            "\n",
            "Slide 11:\n",
            "Data Cleaning\n",
            "Slide 12:\n",
            "Missing Data\n",
            "• Data is not always available \n",
            "• E.g., many tuples have no recorded value for several attributes, such as customer income in sales data \n",
            "• Missing data may be due to \n",
            "• equipment malfunction \n",
            "• inconsistent with other recorded data and thus deleted \n",
            "• data not entered due to misunderstanding \n",
            "• certain data may not be considered important at the time of entry \n",
            "• not register history or changes of the data \n",
            "• Missing data may need to be inferred. \n",
            "Slide 13:\n",
            "How to handle Missing data?\n",
            "• Ignore the tuple: usually done when class label is missing (assuming the tasks in classification—not effective when the percentage of missing values per attribute varies considerably. \n",
            "• Fill in the missing value manually: tedious + infeasible? \n",
            "• Fill in it automatically with \n",
            "• a global constant : e.g., “unknown”, a new class?! \n",
            "• the attribute mean \n",
            "• the attribute mean for all samples belonging to the same class: smarter \n",
            "• the most probable value: inference-based such as Bayesian formula or decision tree \n",
            "Slide 14:\n",
            "Example\n",
            "Slide 15:\n",
            "Ignore Tuple \n",
            "Pros: \n",
            "Complete removal of data with missing values results in robust and highly accurate model\n",
            "Deleting a particular row or a column with no specific information is better, since it does not have a high weightage\n",
            "Cons:\n",
            "Loss of information and data \n",
            "Works poorly if the percentage of missing values is high (say 30%), compared to the whole dataset\n",
            "\n",
            "Slide 16:\n",
            "Replacing With Mean/Mode\u000b\n",
            "Pros:\n",
            "This is a better approach when the data size is small\n",
            "It can prevent data loss which results in removal of the rows and columns\n",
            "Cons:\n",
            "Imputing the approximations add variance and bias\n",
            "Works poorly compared to other multiple-imputations method\n",
            "\n",
            "Slide 17:\n",
            "Assigning An Unique Category\n",
            "A categorical feature will have a definite number of possibilities, such as gender, for example. Since they have a definite number of classes, we can assign another class for the missing values. Here, the features Cabin and Embarked have missing values which can be replaced with a new category, say, U for ‘unknown’.\n",
            "Slide 18:\n",
            "Pros:\n",
            "Less possibilities with one extra category, resulting in low variance after one hot encoding — since it is categorical\n",
            "Negates the loss of data by adding an unique category\n",
            "Cons:\n",
            "Adds less variance\n",
            "Adds another feature to the model while encoding, which may result in poor performance \n",
            "\n",
            "Slide 19:\n",
            "Noisy Data\n",
            "Noise: random error or variance in a measured variable  \n",
            "Incorrect attribute values may due to\n",
            "faulty data collection instruments\n",
            "data entry problems\n",
            "data transmission problems\n",
            "technology limitation\n",
            "inconsistency in naming convention \n",
            "\n",
            "Other data problems which requires data cleaning\n",
            "duplicate records\n",
            "incomplete data\n",
            "inconsistent data\n",
            "Slide 20:\n",
            "How to Handle Noisy Data?\n",
            "Binning\n",
            "Binning methods smooth a sorted data value by consulting its “neighbor-hood,” that is, the values around it. \n",
            "first sort data and partition into (equal-frequency) bins\n",
            "then one can smooth by bin means,  smooth by bin median, smooth by bin boundaries, etc.\n",
            "Regression\n",
            "smooth by fitting the data into regression functions\n",
            "Clustering\n",
            "detect and remove outliers\n",
            "Semi-automated method: combined computer and human inspection\n",
            "detect suspicious values and check manually\n",
            "20\n",
            "Slide 21:\n",
            "Simple Discretization Methods: Binning\n",
            "Equal-width (distance) partitioning\n",
            "Divides the range into N intervals of equal size: uniform grid\n",
            "if A and B are the lowest and highest values of the attribute, the width of intervals will be: W = (B –A)/N.\n",
            "The most straightforward, but outliers may dominate presentation\n",
            "Skewed data is not handled well\n",
            "Equal-depth (frequency) partitioning\n",
            "Divides the range into N intervals, each containing approximately same number of samples\n",
            "Good data scaling\n",
            "Managing categorical attributes can be tricky\n",
            "Slide 22:\n",
            "Binning Methods for Data Smoothing\n",
            "Sorted data for price (in dollars): \n",
            "\t\t\t\t\t\t\t4, 8, 9, 15, 21, 21, 24, 25, 26, 28, 29, 34\n",
            "*  Partition into equal-frequency (equi-depth) bins:\n",
            "      - Bin 1: 4, 8, 9, 15\n",
            "      - Bin 2: 21, 21, 24, 25\n",
            "      - Bin 3: 26, 28, 29, 34\n",
            "\n",
            "Smoothing by bin means: each value in a bin is replaced by the mean value of the bin. \n",
            "\n",
            "      - Bin 1: 9, 9, 9, 9\n",
            "      - Bin 2: 23, 23, 23, 23\n",
            "      - Bin 3: 29, 29, 29, 29\n",
            "\n",
            "\n",
            "Slide 23:\n",
            "Binning Methods for Data Smoothing\n",
            "Sorted data for price (in dollars): \n",
            "\t\t\t\t\t\t\t4, 8, 9, 15, 21, 21, 24, 25, 26, 28, 29, 34\n",
            "*  Partition into equal-frequency (equi-depth) bins:\n",
            "      - Bin 1: 4, 8, 9, 15\n",
            "      - Bin 2: 21, 21, 24, 25\n",
            "      - Bin 3: 26, 28, 29, 34\n",
            "\n",
            "*  Smoothing by bin boundaries: Each bin value is then replaced by the closest boundary value. In general, the larger the width, the greater the effect of the smoothing.\n",
            "\n",
            "      - Bin 1: 4, 4, 4, 15\n",
            "      - Bin 2: 21, 21, 25, 25\n",
            "      - Bin 3: 26, 26, 26, 34\n",
            "Slide 24:\n",
            "Equal-width partitioning \n",
            "Slide 25:\n",
            "Regression\n",
            "25\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "x\n",
            "y\n",
            "y = x + 1\n",
            "\n",
            "\n",
            "\n",
            "X1\n",
            "Y1\n",
            "Y1’\n",
            "Data can be smoothed by fitting the data to a function, such as with regression.\n",
            "\n",
            "Linear regression (best line to fit\n",
            "\t\t       two variables)\n",
            "Multiple linear regression (more \n",
            "           than two variables), fit to a \n",
            "             multidimensional surface\n",
            "Linear regression – find the best line to fit two variables and use regression function to smooth data\n",
            "Slide 26:\n",
            "Cluster Analysis\n",
            "\n",
            "\n",
            "\n",
            "  detect and remove outliers, Where similar values are organized into groups or “clusters”\n",
            "\n",
            "Slide 27:\n",
            "\n",
            "Slide 28:\n",
            "Data Integration\n",
            "Data integration: \n",
            "Combines data from multiple sources into a coherent store\n",
            "\n",
            "Issues to be considered\n",
            "Schema integration: e.g., “cust-id” & “cust-no”\n",
            "Integrate metadata from different sources\n",
            "Entity identification problem: \n",
            "Identify real world entities from multiple data sources, \n",
            "\t\t\te.g., Bill Clinton = William Clinton\n",
            "Detecting and resolving data value conflicts\n",
            "For the same real world entity, attribute values from different sources are different\n",
            "Possible reasons: different representations, different scales, \n",
            "\t\t\t\te.g., metric vs. British units\n",
            "Slide 29:\n",
            "Handling Redundancy in Data Integration\n",
            "Redundant data occur often when integration of multiple databases is done.\n",
            "Object identification:  The same attribute or object may have different names in different databases (link adharcard and PAN card)\n",
            "Derivable data: One attribute may be a “derived” attribute in another table, e.g., annual revenue, age\n",
            "Redundant attributes can be detected by correlation analysis\n",
            "Careful integration of the data from multiple sources may help reduce/avoid redundancies and inconsistencies and improve mining speed and quality.\n",
            "Data value conflict detection and resolution\n",
            "Hotel chain, retail shops, schools, institutes\n",
            "Tuple duplication – increase redundancy and inconsistancy\n",
            "\n",
            "Slide 30:\n",
            "Data\n",
            "Slide 31:\n",
            "Correlation analysis of categorical (discrete) attributes using chi square.\n",
            "For given example expected frequency for the cell ( male , Fiction) is: \n",
            "Chi square computation is :\n",
            "Slide 32:\n",
            "Chi-square distribution table\n",
            "Slide 33:\n",
            "Goodness of fit test\n",
            "Stating the Hypothesis \n",
            "Null Hypothesis: There is no relationship between the two categorical variables. (They are independent.)\n",
            "Acceptable Hypothesis: There is a relationship between the two categorical variables. (They are not independent.)\n",
            "Find Degree of freedom = (R-1)*(C-1)\n",
            "For 1 degree of freedom, the chi square value needed to reject hypothesis at the 0.001 significance level is 10.828.\n",
            "Check chi-square(calculated) > chi-square(tabular)\n",
            "Yes – reject null hypothesis and accept alternate hypothesis\n",
            "\n",
            "Slide 34:\n",
            "Goodness of fit\n",
            "Our value is above this so we can reject the hypothesis that gender and prefered_reading are independent.\n",
            "Conclusion - Two attributes are (strongly) correlated for the given group of people.\n",
            "\n",
            "Slide 35:\n",
            "Example :1\n",
            "Slide 36:\n",
            "Finding the P-Value\n",
            "Technically, the p-value is the probability of observing χ2 as least as large as the one observed assuming that no relationship exists between the explanatory and response variable. Using statistical software, we find that the p-value for this test.\n",
            "Slide 37:\n",
            "Example 2:\n",
            "Slide 38:\n",
            "\n",
            "Slide 39:\n",
            "\n",
            "Slide 40:\n",
            "# Exercise :\n",
            "Slide 41:\n",
            "Correlation Analysis (Numerical Data)\n",
            "Correlation coefficient (also called Pearson’s product moment coefficient)\n",
            "\n",
            "\n",
            "\n",
            "Where;    n is the number of tuples \n",
            "\t\t\t\t\tare the respective means of A and B,  \n",
            "   \t\t\t  σA and σB are the respective standard deviation of A and B, \n",
            "   \t\t\t  Σ(AB) is the sum of the AB cross-product.\n",
            "\n",
            "If rA,B > 0, A and B are positively correlated (A’s values increase as B’s).  The higher, the stronger correlation.\n",
            "rA,B = 0: independent;  \n",
            "rA,B < 0: negatively correlated\n",
            "\n",
            "\n",
            "n\n",
            "n\n",
            "Slide 42:\n",
            "\n",
            "Slide 43:\n",
            "43\n",
            "Visually Evaluating Correlation\n",
            "Scatter plots showing the similarity from –1 to 1.\n",
            "Slide 44:\n",
            "\n",
            "Slide 45:\n",
            "\n",
            "Slide 46:\n",
            "\n",
            "Slide 47:\n",
            "Solution\n",
            "The answer r is: 2868 / 5413.27 = 0.529809\n",
            "Σx = 247\n",
            "Σy = 486\n",
            "Σxy = 20,485\n",
            "Σx2 = 11,409\n",
            "Σy2 = 40,022\n",
            "n is the sample size, in our case = 6\n",
            "The correlation coefficient =\n",
            "6(20,485) – (247 × 486) / [√[[6(11,409) – (2472)] × [6(40,022) – 4862]]]\n",
            "= 0.5298 \n",
            "The range of the correlation coefficient is from -1 to 1. Our result is 0.5298 or 52.98%, which means the variables have a moderate positive correlation.\n",
            "\n",
            "Slide 48:\n",
            "48\n",
            "Covariance (Numeric Data)\n",
            "Covariance is similar to correlation\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "where n is the number of tuples,      and      are the respective mean or expected values of A and B, σA and σB are the respective standard deviation of A and B.\n",
            "Positive covariance: If CovA,B > 0, then A and B both tend to be larger than their expected values.\n",
            "Negative covariance: If CovA,B < 0 then if A is larger than its expected value, B is likely to be smaller than its expected value.\n",
            "Independence: CovA,B = 0 but the converse is not true:\n",
            "Some pairs of random variables may have a covariance of 0 but are not independent. Only under some additional assumptions (e.g., the data follow multivariate normal distributions) does a covariance of 0 imply independence\n",
            "Correlation coefficient:\n",
            "Slide 49:\n",
            "Co-Variance: An Example\n",
            "\n",
            "It can be simplified in computation as\n",
            "\n",
            "Suppose two stocks A and B have the following values in one week:  (2, 5), (3, 8), (5, 10), (4, 11), (6, 14). \n",
            "Question:  If the stocks are affected by the same industry trends, will their prices rise or fall together?\n",
            "E(A) = (2 + 3 + 5 + 4 + 6)/ 5 = 20/5 = 4\n",
            "E(B) = (5 + 8 + 10 + 11 + 14) /5 = 48/5 = 9.6\n",
            "Cov(A,B) = (2×5+3×8+5×10+4×11+6×14)/5 − 4 × 9.6 = 4\n",
            "Thus, A and B rise together since Cov(A, B) > 0.\n",
            "Slide 50:\n",
            "\n",
            "Slide 51:\n",
            "Data Transformation\n",
            "Smoothing: remove noise from data using smoothing techniques\n",
            "Aggregation: summarization, data cube construction\n",
            "Generalization: concept hierarchy climbing\n",
            "Normalization: scaled to fall within a small, specified range\n",
            "min-max normalization\n",
            "z-score normalization\n",
            "normalization by decimal scaling\n",
            "Attribute/feature construction:\n",
            "New attributes constructed from the given ones\n",
            "51\n",
            "Slide 52:\n",
            "Data Transformation: Normalization\n",
            "Min-max normalization: For Linear Transformation; to [new_minA, new_maxA]\n",
            "\n",
            "\n",
            "\tEx.  Let income range $12,000 to $98,000 normalized to [0.0, 1.0].  Then $73,600 is mapped to  \n",
            "\n",
            "Z-score normalization (μ: mean, σ: standard deviation):\n",
            "Ex. Let μ (mean) = 54,000, \n",
            "\t\t     σ (std. dev)= 16,000.  Then\n",
            "\n",
            "Normalization by decimal scaling\n",
            "Where j is the smallest integer such that, Max(|ν’|) < 1\n",
            "Slide 53:\n",
            "\n",
            "Slide 54:\n",
            "\n",
            "Suppose that the minimum and maximum values for the feature income are $12,000 and $98,000, respectively.\n",
            "We would like to map income to the range [0.0,1.0]\n",
            "By min-max normalization, a value of $73,600 for income is transformed to:\n",
            "\n",
            "Slide 55:\n",
            "\n",
            "Slide 56:\n",
            "Example\n",
            "Suppose that the mean and standard deviation of the values for the feature income are $54,000 and $16,000, respectively. With z-score normalization, a value of $73,600 for income is transformed to\n",
            "Slide 57:\n",
            "\n",
            "Slide 58:\n",
            "Example\n",
            "Suppose that the recorded values of 𝐹range from −986to 917. \n",
            "The maximum absolute value of 𝐹is 986. \n",
            "To normalize by decimal scaling, we therefore divide each value by 1,000(i.e., 𝑗=3)\n",
            " so that −986 normalizes to −0.986 and 917 normalizes to 0.917\n"
          ]
        }
      ]
    }
  ]
}